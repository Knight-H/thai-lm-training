{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace: Summary of the Task:: MLM [link](https://huggingface.co/transformers/task_summary.html?highlight=fill%20mask#masked-language-modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSeniorProjectTokenizer(object):\n",
    "    def __init__(self, TOK_PATH = Path('./senior_proj_itos'), BOS='xxbos', EOS='xxeos', FLD = 'xxfld', UNK='xxunk', PAD='xxpad',\n",
    "                 TK_REP='xxrep', TK_WREP='xxwrep', TK_NUM='xxnum', TK_LAUGH='xxlaugh', n_cpus=1,\n",
    "                ):\n",
    "        from senior_project_util import ThaiTokenizer, pre_rules_th, post_rules_th\n",
    "        from fastai.text.transform import BaseTokenizer, Tokenizer, Vocab\n",
    "        from fastai.text.data import TokenizeProcessor, NumericalizeProcessor\n",
    "\n",
    "        with open(TOK_PATH/\"bert_itos_80k_cleaned.pkl\", 'rb') as f:\n",
    "            itos = pickle.load(f)\n",
    "            \n",
    "        self.vocab = Vocab(itos)\n",
    "        self.tokenizer = Tokenizer(tok_func = ThaiTokenizer, lang = 'th', \n",
    "                                   pre_rules = pre_rules_th, post_rules=post_rules_th, n_cpus=n_cpus)\n",
    "        \n",
    "        self.cls_token_id = self.vocab.stoi[BOS]\n",
    "        self.sep_token_id = self.vocab.stoi[EOS]\n",
    "        self.pad_token_id = self.vocab.stoi[PAD]\n",
    "        \n",
    "        self.mask_token = FLD  #SINCE THIS ONE IS NOT USED, and INSIDE SPECIAL TOKEN....\n",
    "        self._pad_token = PAD\n",
    "        \n",
    "        self.mask_token_id = self.vocab.stoi[self.mask_token]\n",
    "        \n",
    "#         tokenizer_processor = TokenizeProcessor(tokenizer=tt, chunksize=300000, mark_fields=False)\n",
    "#         numbericalize_processor = NumericalizeProcessor(vocab=vocab)\n",
    "        \n",
    "    def num_special_tokens_to_add(self, pair=False):\n",
    "        return 2\n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer._process_all_1([text])[0]\n",
    "#         return self.tokenizer.process_all([text])[0]\n",
    "    \n",
    "    def convert_tokens_to_ids(self, token_list):\n",
    "        #From https://huggingface.co/transformers/_modules/transformers/tokenization_utils_fast.html#PreTrainedTokenizerFast.convert_tokens_to_ids\n",
    "        if token_list is None:\n",
    "            return None\n",
    "\n",
    "        if isinstance(token_list, str):\n",
    "            return self.vocab.numericalize([token_list])[0]\n",
    "        \n",
    "        return self.vocab.numericalize(token_list)\n",
    "    \n",
    "    def build_inputs_with_special_tokens(self, token_list):\n",
    "        # From https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_bert.py#L235\n",
    "        return [self.cls_token_id] + token_list + [self.sep_token_id]\n",
    "    \n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0, token_ids_1 = None, already_has_special_tokens = False\n",
    "    ):\n",
    "        # From https://huggingface.co/transformers/_modules/transformers/tokenization_utils.html#PreTrainedTokenizer.get_special_tokens_mask\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` method.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0: list of ids (must not contain special tokens)\n",
    "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
    "                for sequence pairs\n",
    "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
    "                special tokens for the model\n",
    "\n",
    "        Returns:\n",
    "            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "        return [0] * ((len(token_ids_1) if token_ids_1 else 0) + len(token_ids_0))\n",
    "    \n",
    "    def __len__(self):\n",
    "        #https://huggingface.co/transformers/_modules/transformers/tokenization_utils_fast.html#PreTrainedTokenizerFast.__len__\n",
    "        return len(self.vocab.itos)\n",
    "    \n",
    "    def encode(self, text):\n",
    "        _input = self.tokenize(text)\n",
    "        _input = self.convert_tokens_to_ids(_input)\n",
    "        _input = self.build_inputs_with_special_tokens(_input)\n",
    "        _input = torch.tensor([_input])\n",
    "        return _input\n",
    "    def decode(self, numerical_token):\n",
    "        if isinstance(numerical_token, list):\n",
    "            return self.vocab.itos[numerical_token[0]]\n",
    "        return self.vocab.itos[numerical_token]\n",
    "    \n",
    "tokenizer = CustomSeniorProjectTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ใคร', 'เคย', 'มี', 'แฟน', 'ที่', 'กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'แล้ว', 'รู้สึก', 'เสีย', 'ความสุข', 'ไป', 'อย่าง', 'นึง', 'บ้าง', 'มั้ย', 'ครับ', ' ', 'ก่อนอื่น', 'ผม', 'ต้อง', 'บอก', 'ก่อน', 'เลย', 'ว่า', 'คนเรา', 'จะ', 'เลือก', 'กิน', 'อาหาร', 'แบบ', 'ไหน', 'ชอบ', 'แบบ', 'ไหน', 'เป็นเรื่อง', 'ของ', 'ความชอบ', 'ส่วนตัว', 'นะ', 'ครับ', 'ทุกคน', 'มี', 'สิทธิ', 'ใน', 'การเลือก', 'ของที่ชอบ', 'และ', 'ไม่ชอบ', 'อยู่แล้ว', ' ', 'แต่', 'ผม', 'รู้สึก', 'ว่า', 'ตอนนี้', 'ผม', 'กำลัง', 'ประสบปัญหา', 'ที่', 'ดูเหมือน', 'จะ', 'เล็ก', 'แต่', 'กลายเป็น', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', ' ', 'ผม', 'คบ', 'กับ', 'แฟน', 'มา', 'xxnum', ' ', 'ปี', 'แล้ว', 'ครับ', ' ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'กิน', 'อาหาร', 'ญี่ปุ่น', 'และ', 'ปลาดิบ', 'แต่', 'แฟน', 'ผม', 'ไม่', 'กิน', 'ปลาดิบ', 'เลย', ' ', 'ผม', 'อยากกิน', 'บุ', 'ฟเฟ่', 'เนื้อ', 'แต่', 'แฟน', 'ผม', 'ก็', 'ไม่', 'กิน', 'เนื้อ', ' ', 'เรา', 'เลย', 'ไม่ได้', 'เข้า', 'ทาน', 'ร้าน', 'บุ', 'ฟเฟ่', 'เนื้อ', 'และ', 'บุ', 'ฟเฟ่', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก', 'ลัว', 'แฟน', 'ผม', 'ทาน', 'ไม่', 'คุ้ม', ' ', 'และ', 'เรื่องใหญ่', 'เลย', 'คือ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'ทานอาหาร', 'รสจัด', 'และ', 'รส', 'เผ็ด', 'มาก', ' ', 'แต่', 'แฟน', 'ผม', 'ทาน', 'เผ็ด', 'ไม่ได้', 'เลยเวลา', 'เรา', 'ไป', 'กิน', 'ส้มตำ', 'กัน', 'ก็', 'จะ', 'สั่ง', ' ', 'ส้มตำ', 'ไม่', 'ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'ใส่', 'พริก', ' ', 'ลาบ', 'ไม่', 'ใส่', 'พริก', ' ', 'ร้าน', 'กับข้าว', 'อื่น ๆ', 'ก็', 'เช่นกัน', 'แฟน', 'ผม', 'จะ', 'ไม่ชอบ', 'กิน', 'ผัก', 'ไม่ค่อย', 'สั่ง', 'กับข้าว', 'ที่', 'เป็น', 'ผัก', 'แล้ว', 'ผม', 'ชอบ', 'ผักบุ้ง', 'ทอด', 'กรอบ', ' ', 'เห็ด', 'หอม', 'สด', 'ทอด', 'มาก', ' ', 'แต่', 'ก็', 'ไม่ได้', 'สั่ง', 'เพราะว่า', 'เธอ', 'ไม่', 'กิน', 'ถึง', 'เค้า', 'จะ', 'บอก', 'ให้', 'สั่ง', 'เลย', 'ๆ', 'ก็', 'เถอะ', 'แต่', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ', 'อยู่ดี', 'อ่ะ', 'ครับ', ' ', 'ผม', 'รู้สึก', 'กิน', 'อาหาร', 'ไม่', 'มีความสุข', 'เลย', 'ชีวิต', 'ผม', 'ขาด', 'รส', 'เผ็ด', 'ไป', 'เหมือน', 'จะ', 'ขาดใจ', 'เหมือน', 'มัน', 'ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'นึง', 'เลย', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'ถ้า', 'เรา', 'แต่งงาน', 'กัน', 'แล้ว', 'ผม', 'ก็', 'อาจ', 'จะต้อง', 'มีปัญหา', 'เรื่อง', 'นี้', 'มากขึ้น', ' ', 'พอ', 'ผม', 'เห็น', 'คู่', 'ที่', 'ชอบ', 'ทานอาหาร', 'เหมือน', 'ๆ', 'กัน', 'เห็น', 'เค้า', 'กิน', 'อาหาร', 'กัน', 'อย่างมีความสุข', 'แล้ว', 'ผม', 'รู้สึก', 'อิจฉา', 'มาก', 'ๆ', 'เลย', ' ', 'มี', 'ใคร', 'เคย', 'มีปัญหา', 'แบบผม', 'มั้ย', 'ครับ', 'แล้', 'วจะ', 'แก้ปัญหา', 'นี้', 'ยังไง', 'ดี', 'ครับ']\n"
     ]
    }
   ],
   "source": [
    "text = \"ใครเคยมีแฟนที่กินอาหารไม่ถูกปากกันแล้วรู้สึกเสียความสุขไปอย่างนึงบ้างมั้ยครับ  ก่อนอื่นผมต้องบอกก่อนเลยว่าคนเราจะเลือกกินอาหารแบบไหนชอบแบบไหนเป็นเรื่องของความชอบส่วนตัวนะครับทุกคนมีสิทธิในการเลือกของที่ชอบและไม่ชอบอยู่แล้ว แต่ผมรู้สึกว่าตอนนี้ผมกำลังประสบปัญหาที่ดูเหมือนจะเล็กแต่กลายเป็นว่ามันค่อนข้างใหญ่ ผมคบกับแฟนมา6ปีแล้วครับ ผมเป็นคนชอบกินอาหารญี่ปุ่นและปลาดิบแต่แฟนผมไม่กินปลาดิบเลย ผมอยากกินบุฟเฟ่เนื้อแต่แฟนผมก็ไม่กินเนื้อ เราเลยไม่ได้เข้าทานร้านบุฟเฟ่เนื้อและบุฟเฟ่อาหารญี่ปุ่นกันเพราะรู้สึกลัวแฟนผมทานไม่คุ้ม และเรื่องใหญ่เลยคือผมเป็นคนชอบทานอาหารรสจัดและรสเผ็ดมาก แต่แฟนผมทานเผ็ดไม่ได้เลยเวลาเราไปกินส้มตำกันก็จะสั่ง ส้มตำไม่ใส่พริก ต้มแซ่บไม่ใส่พริก ลาบไม่ใส่พริก ร้านกับข้าวอื่นๆก็เช่นกันแฟนผมจะไม่ชอบกินผักไม่ค่อยสั่งกับข้าวที่เป็นผักแล้วผมชอบผักบุ้งทอดกรอบ เห็ดหอมสดทอดมาก แต่ก็ไม่ได้สั่งเพราะว่าเธอไม่กินถึงเค้าจะบอกให้สั่งเลยๆก็เถอะแต่ผมก็ยังเกรงใจเธออยู่ดีอ่ะครับ ผมรู้สึกกินอาหารไม่มีความสุขเลยชีวิตผมขาดรสเผ็ดไปเหมือนจะขาดใจเหมือนมันทำให้ขาดความสุขไปอย่างนึงเลยอ่ะครับ ยิ่งถ้าเราแต่งงานกันแล้วผมก็อาจจะต้องมีปัญหาเรื่องนี้มากขึ้น พอผมเห็นคู่ที่ชอบทานอาหารเหมือนๆกันเห็นเค้ากินอาหารกันอย่างมีความสุขแล้วผมรู้สึกอิจฉามากๆเลย มีใครเคยมีปัญหาแบบผมมั้ยครับแล้วจะแก้ปัญหานี้ยังไงดีครับ\"\n",
    "value = tokenizer.tokenize(text)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/modeling_auto.py:765: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelWithLMHead.from_pretrained(\"./OriginalBert3_export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(80000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=80000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148153472"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomSeniorProjectTokenizer' object has no attribute 'batch_encode_plus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-caa835dc7c8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_and_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36m_parse_and_tokenize\u001b[0;34m(self, pad_to_max_length, add_special_tokens, *args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;31m# Parse arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         inputs = self.tokenizer.batch_encode_plus(\n\u001b[0m\u001b[1;32m    464\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomSeniorProjectTokenizer' object has no attribute 'batch_encode_plus'"
     ]
    }
   ],
   "source": [
    "nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomSeniorProjectTokenizer' object has no attribute 'batch_encode_plus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b7087a92fae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"วันนี้เป็นวันที่{nlp.tokenizer.mask_token}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_and_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36m_parse_and_tokenize\u001b[0;34m(self, pad_to_max_length, add_special_tokens, *args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;31m# Parse arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         inputs = self.tokenizer.batch_encode_plus(\n\u001b[0m\u001b[1;32m    464\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomSeniorProjectTokenizer' object has no attribute 'batch_encode_plus'"
     ]
    }
   ],
   "source": [
    "nlp(f\"วันนี้เป็นวันที่{nlp.tokenizer.mask_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Trainer [link](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L375)\n",
    "\n",
    "## DataCollator [link](https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py#L69)\n",
    "\n",
    "1. A torch.utils.data.dataloader.Dataloader is created from a Dataset (where the examples are tokenized with `tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))` and appended special tokens `<s>` and `</s>` through `tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + self.block_size])` [this is created where `block_size` is `self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)`] ) referenced [here](https://github.com/huggingface/transformers/blob/9022ef021a56db975d25c7108cbd19d0dd399174/src/transformers/trainer.py#L224).  \n",
    "This will return a \"List\" of ids. \n",
    "2. [_tensorize_batch()](https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py#L90) will attempt to convert from ```examples: List[torch.Tensor]) -> torch.Tensor:``` and also apply [self.tokenizer._pad_token()](https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py#L101) along the way for examples with irregular length\n",
    "3. ```if self.mlm: ``` , mask the tokens with [mask_tokens()](https://github.com/huggingface/transformers/blob/master/src/transformers/data/data_collator.py#L103) to Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxbos', 'xxunk', ' ', 'models', ' ', 'are', ' ', 'xxunk', ' ', 'than', ' ', 'the', ' ', 'models', ' ', 'they', ' ', 'mimic', '.', ' ', 'using', ' ', 'them', ' ', 'instead', ' ', 'of', ' ', 'the', ' ', 'large', ' ', 'versions', ' ', 'would', ' ', 'help', ' ', 'xxfld', ' ', 'our', ' ', 'carbon', ' ', 'footprint', '.', 'xxeos']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     0,     9, 50708,     9,  5400,     9,     0,     9, 15262,\n",
       "             9,   379,     9, 50708,     9, 11009,     9, 76615,    47,     9,\n",
       "         29179,     9, 17114,     9, 54855,     9,   882,     9,   379,     9,\n",
       "         14533,     9, 73189,     9, 15472,     9, 12358,     9,     4,     9,\n",
       "          6858,     9, 17364,     9, 77218,    47,     3]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input = tokenizer.encode(sequence)\n",
    "print(list(map(lambda x: tokenizer.decode([x]) , _input[0])))\n",
    "_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "         0.1500, 0.1500]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = _input.clone()\n",
    "# We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "probability_matrix = torch.full(labels.shape, 0.15)\n",
    "probability_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
       "         0.1500, 0.1500]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If there is special tokens, fill it with 0.0\n",
    "special_tokens_mask = [\n",
    "    tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "]\n",
    "print(special_tokens_mask)\n",
    "probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "probability_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IF there is padding mask, fill it with 0 too\n",
    "padding_mask = labels.eq(tokenizer.pad_token_id)\n",
    "probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False, False,  True, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False,  True, False,  True,  True, False,  True, False, False,  True,\n",
      "         False,  True, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  -100,  -100,  -100,     9,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,     9,  -100,     9, 54855,  -100,   882,  -100,  -100,     9,\n",
       "          -100,     9,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly select which one to mask based on the probabilities given ealier, and if the mask isn't chosen, fill in -100 for the value\n",
    "masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "print(masked_indices)\n",
    "labels[~masked_indices] = -100  # We only compute loss on masked \n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    4,     0,     9, 50708,     4,  5400,     9,     0,     9, 15262,\n",
      "             9,   379,     9, 50708,     9, 11009,     9, 76615,    47,     9,\n",
      "         29179,     9, 17114,     4,     4,     9,     4,     9,   379,     4,\n",
      "         14533,     4, 73189,     9, 15472,     9, 12358,     9,     4,     9,\n",
      "          6858,     9, 17364,     9, 77218,    47,     3]]) tensor([[    2,  -100,  -100,  -100,     9,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,     9,  -100,     9, 54855,  -100,   882,  -100,  -100,     9,\n",
      "          -100,     9,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "# 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "_input[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "# 10% of the time, we replace masked input tokens with random word\n",
    "indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "_input[indices_random] = random_words[indices_random]\n",
    "\n",
    "# The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "print(_input, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxfld', 'xxunk', ' ', 'models', 'xxfld', 'are', ' ', 'xxunk', ' ', 'than', ' ', 'the', ' ', 'models', ' ', 'they', ' ', 'mimic', '.', ' ', 'using', ' ', 'them', 'xxfld', 'xxfld', ' ', 'xxfld', ' ', 'the', 'xxfld', 'large', 'xxfld', 'versions', ' ', 'would', ' ', 'help', ' ', 'xxfld', ' ', 'our', ' ', 'carbon', ' ', 'footprint', '.', 'xxeos']\n"
     ]
    }
   ],
   "source": [
    "print(list(map(lambda x: tokenizer.decode([x]) , _input[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now I'm going to define `_mask_input()` \n",
    " 0.15 Probability: With 100% replace with mask, and 0% unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mask_input(sequence, tokenizer, mlm_probability=0.15, truncation=True):\n",
    "    # Tokenize the sequence with special tokens inserted\n",
    "    _input = tokenizer.encode(sequence)\n",
    "    \n",
    "    labels = _input.clone()\n",
    "    # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    \n",
    "    # If there is special tokens, fill it with 0.0\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    \n",
    "    # IF there is padding mask, fill it with 0 too\n",
    "    padding_mask = labels.eq(tokenizer.pad_token_id)\n",
    "    probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "    \n",
    "    # Randomly select which one to mask based on the probabilities given ealier, and if the mask isn't chosen, fill in -100 for the value\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked \n",
    "    \n",
    "    # 100% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced = masked_indices\n",
    "    _input[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "    \n",
    "    return _input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,     0,     9, 50708,     9,     4,     9,     4,     9,     4,\n",
      "             9,   379,     9, 50708,     4, 11009,     9, 76615,     4,     9,\n",
      "             4,     9, 17114,     9, 54855,     9,   882,     9,   379,     9,\n",
      "             4,     9, 73189,     9, 15472,     4, 12358,     9,  6858,     9,\n",
      "             4,     4,     4,    47,     3]])\n",
      "['xxbos', 'xxunk', ' ', 'models', ' ', 'xxfld', ' ', 'xxfld', ' ', 'xxfld', ' ', 'the', ' ', 'models', 'xxfld', 'they', ' ', 'mimic', 'xxfld', ' ', 'xxfld', ' ', 'them', ' ', 'instead', ' ', 'of', ' ', 'the', ' ', 'xxfld', ' ', 'versions', ' ', 'would', 'xxfld', 'help', ' ', 'our', ' ', 'xxfld', 'xxfld', 'xxfld', '.', 'xxeos']\n",
      "tensor([[ -100,  -100,  -100,  -100,  -100,  5400,  -100,     0,  -100, 15262,\n",
      "          -100,  -100,  -100,  -100,     9,  -100,  -100,  -100,    47,  -100,\n",
      "         29179,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         14533,  -100,  -100,  -100,  -100,     9,  -100,  -100,  -100,  -100,\n",
      "         17364,     9, 77218,  -100,  -100]])\n",
      "['-', '-', '-', '-', '-', 'are', '-', 'xxunk', '-', 'than', '-', '-', '-', '-', ' ', '-', '-', '-', '.', '-', 'using', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'large', '-', '-', '-', '-', ' ', '-', '-', '-', '-', 'carbon', ' ', 'footprint', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input, labels = _mask_input(f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help our carbon footprint.\", tokenizer)\n",
    "print(tokenized_input)\n",
    "print(list(map(lambda x: tokenizer.decode([x]) , tokenized_input[0])))\n",
    "print(labels)\n",
    "print(list(map(lambda x: tokenizer.decode([x]) if x != -100 else '-', labels[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now I'm going to define `_predict_masks()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict_masks(_input, labels, tokenizer, k=5, verbose=True, verbose_length=6):\n",
    "    mask_token_index = torch.where(_input == tokenizer.mask_token_id)[1]\n",
    "#     print(mask_token_index)\n",
    "    token_logits = model(_input)[0]\n",
    "#     print(token_logits)\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "#     print(mask_token_logits)\n",
    "    top_k_tokens = torch.topk(mask_token_logits, k, dim=1).indices.numpy()\n",
    "    _labels = labels[0, mask_token_index].numpy()\n",
    "    _labels = np.vstack(_labels) #vertical stack to be able to compare to top_5_tokens\n",
    "#     print(top_5_tokens)\n",
    "#     print(_labels)\n",
    "    \n",
    "    for index, mask_index in enumerate(mask_token_index):\n",
    "        begin_index = max(mask_index-verbose_length,0)\n",
    "        begin_index_offset = max(mask_index-verbose_length,0) - (mask_index-verbose_length) # Will be 0 if no offset\n",
    "        \n",
    "        input_verbose_sequence = _input[0, max(mask_index-verbose_length,0):mask_index+verbose_length].clone()\n",
    "        labeled_verbose_sequence = input_verbose_sequence.clone()\n",
    "        \n",
    "#         print(labeled_verbose_sequence)\n",
    "        \n",
    "        labeled_verbose_sequence[verbose_length-begin_index_offset] = labels[0, mask_index]\n",
    "        \n",
    "        print(\"Masked Input: \")\n",
    "        print(f\"\\t{list(map(lambda x: tokenizer.decode([x]) , input_verbose_sequence))}\")\n",
    "#         print(f\"\\t{tokenizer.decode(input_verbose_sequence)}\")\n",
    "        print(\"Labeled Input: \")\n",
    "        print(f\"\\t{list(map(lambda x: tokenizer.decode([x]) , labeled_verbose_sequence))}\")\n",
    "#         print(f\"\\t{tokenizer.decode(labeled_verbose_sequence)}\")\n",
    "        print(\"Predictions: \")\n",
    "        for prediction in top_k_tokens[index]:\n",
    "            prediction_verbose_sequence = input_verbose_sequence.clone()\n",
    "            prediction_verbose_sequence[verbose_length-begin_index_offset] = prediction\n",
    "            print(f\"\\t{list(map(lambda x: tokenizer.decode([x]) , prediction_verbose_sequence))}\")\n",
    "#             print(f\"\\t{tokenizer.decode(prediction_verbose_sequence)}\")\n",
    "        print(\"\")\n",
    "    \n",
    "    correct_labels = np.any(top_k_tokens == _labels, axis=1)\n",
    "    percent_correct = np.sum(correct_labels)/correct_labels.size\n",
    "    print(f\"Total Input Tokens: {_input.squeeze().shape}\")\n",
    "    print(f\"Total Correct for Top {k}: {np.sum(correct_labels)}\")\n",
    "    print(f\"Total Mask: {correct_labels.size}\")\n",
    "    print(f\"Percent Correct: {percent_correct*100:.2f}%\")\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,     0,     9, 50708,     9,     4,     9,     4,     9,     4,\n",
      "             9,   379,     9, 50708,     4, 11009,     9, 76615,     4,     9,\n",
      "             4,     9, 17114,     9, 54855,     9,   882,     9,   379,     9,\n",
      "             4,     9, 73189,     9, 15472,     4, 12358,     9,  6858,     9,\n",
      "             4,     4,     4,    47,     3]])\n",
      "tensor([    2,     0,     9, 50708,     9,     4,     9,     4,     9,     4,\n",
      "            9,   379,     9, 50708,     4, 11009,     9, 76615,     4,     9,\n",
      "            4,     9, 17114,     9, 54855,     9,   882,     9,   379,     9,\n",
      "            4,     9, 73189,     9, 15472,     4, 12358,     9,  6858,     9,\n",
      "            4,     4,     4,    47,     3])\n",
      "tensor([ 5,  7,  9, 14, 18, 20, 30, 35, 40, 41, 42])\n",
      "tensor([[[10.8335, -2.9528, 25.5903,  ..., -0.8487, -1.9001, -1.6345],\n",
      "         [22.6757, -0.4105,  5.9939,  ...,  1.3035, -0.7734,  0.5439],\n",
      "         [ 8.6228, -5.1675,  3.9214,  ..., -4.0427, -4.7083, -4.0134],\n",
      "         ...,\n",
      "         [11.1095,  0.7911,  0.5366,  ...,  1.5080,  0.4663,  1.8212],\n",
      "         [11.0044, -5.8397,  1.1338,  ..., -3.0428, -7.2902, -3.7938],\n",
      "         [10.5722, -4.5725, 11.7105,  ..., -1.9198, -4.9740, -4.4188]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[15.4429, -7.3978, -3.8571,  ..., -6.7987, -5.6180, -6.7776],\n",
      "        [24.3924,  0.6749,  4.2694,  ...,  1.2185,  1.2116,  1.7517],\n",
      "        [13.5905, -3.8879,  2.6273,  ..., -2.1330, -5.9468, -3.6192],\n",
      "        ...,\n",
      "        [10.2568, -6.9289, -0.5555,  ..., -4.6969, -7.3174, -6.0157],\n",
      "        [ 8.6884, -5.8589,  1.7074,  ..., -4.7035, -5.5833, -5.0317],\n",
      "        [12.0529, -4.1549, -8.8737,  ..., -2.4114, -2.2445, -3.1183]],\n",
      "       grad_fn=<IndexBackward>)\n",
      "[[ 5400  2086    83  7659     0]\n",
      " [    0 21589 66246 12252 37068]\n",
      " [15262  9836  3640 11668 32826]\n",
      " [    9  9509    47   105   540]\n",
      " ...\n",
      " [    9  9509    60    47    13]\n",
      " [ 6858   379  3518  3683 17395]\n",
      " [    9   540  9509   104   382]\n",
      " [17364 10825 34601 17689 30239]]\n",
      "[[ 5400]\n",
      " [    0]\n",
      " [15262]\n",
      " [    9]\n",
      " [   47]\n",
      " [29179]\n",
      " [14533]\n",
      " [    9]\n",
      " [17364]\n",
      " [    9]\n",
      " [77218]]\n",
      "tensor([    2,     0,     9, 50708,     9,     4,     9,     4,     9,     4,\n",
      "            9])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input)\n",
    "print(tokenized_input[0,0:200])\n",
    "mask_token_index = torch.where(tokenized_input == tokenizer.mask_token_id)[1]\n",
    "print(mask_token_index)\n",
    "token_logits = model(_input)[0]\n",
    "print(token_logits)\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "print(mask_token_logits)\n",
    "top_k_tokens = torch.topk(mask_token_logits, k=5, dim=1).indices.numpy()\n",
    "print(top_k_tokens)\n",
    "_labels = labels[0, mask_token_index].numpy()\n",
    "_labels = np.vstack(_labels)\n",
    "print(_labels)\n",
    "for index, mask_index in enumerate(mask_token_index):\n",
    "    input_verbose_sequence = tokenized_input[0, max(mask_index-6,0):mask_index+6].clone()\n",
    "    print(input_verbose_sequence)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Input: \n",
      "\t['xxbos', 'xxunk', ' ', 'models', ' ', 'xxfld', ' ', 'xxfld', ' ', 'xxfld', ' ']\n",
      "Labeled Input: \n",
      "\t['xxbos', 'xxunk', ' ', 'models', ' ', 'are', ' ', 'xxfld', ' ', 'xxfld', ' ']\n",
      "Predictions: \n",
      "\t['xxbos', 'xxunk', ' ', 'models', ' ', 'they', ' ', 'xxfld', ' ', 'xxfld', ' ']\n",
      "\t['xxbos', 'xxunk', ' ', 'models', ' ', 'and', ' ', 'xxfld', ' ', 'xxfld', ' ']\n",
      "\t['xxbos', 'xxunk', ' ', 'models', ' ', 'we', ' ', 'xxfld', ' ', 'xxfld', ' ']\n",
      "\t['xxbos', 'xxunk', ' ', 'models', ' ', 'xxunk', ' ', 'xxfld', ' ', 'xxfld', ' ']\n",
      "\t['xxbos', 'xxunk', ' ', 'models', ' ', 'you', ' ', 'xxfld', ' ', 'xxfld', ' ']\n",
      "\n",
      "Masked Input: \n",
      "\t['xxunk', ' ', 'models', ' ', 'xxfld', ' ', 'xxfld', ' ', 'xxfld', ' ', 'the', ' ']\n",
      "Labeled Input: \n",
      "\t['xxunk', ' ', 'models', ' ', 'xxfld', ' ', 'xxunk', ' ', 'xxfld', ' ', 'the', ' ']\n",
      "Predictions: \n",
      "\t['xxunk', ' ', 'models', ' ', 'xxfld', ' ', 'xxunk', ' ', 'xxfld', ' ', 'the', ' ']\n",
      "\t['xxunk', ' ', 'models', ' ', 'xxfld', ' ', 'and', ' ', 'xxfld', ' ', 'the', ' ']\n",
      "\t['xxunk', ' ', 'models', ' ', 'xxfld', ' ', 'such', ' ', 'xxfld', ' ', 'the', ' ']\n",
      "\t['xxunk', ' ', 'models', ' ', 'xxfld', ' ', 'is', ' ', 'xxfld', ' ', 'the', ' ']\n",
      "\t['xxunk', ' ', 'models', ' ', 'xxfld', ' ', 'in', ' ', 'xxfld', ' ', 'the', ' ']\n",
      "\n",
      "Masked Input: \n",
      "\t['models', ' ', 'xxfld', ' ', 'xxfld', ' ', 'xxfld', ' ', 'the', ' ', 'models', 'xxfld']\n",
      "Labeled Input: \n",
      "\t['models', ' ', 'xxfld', ' ', 'xxfld', ' ', 'than', ' ', 'the', ' ', 'models', 'xxfld']\n",
      "Predictions: \n",
      "\t['models', ' ', 'xxfld', ' ', 'xxfld', ' ', 'of', ' ', 'the', ' ', 'models', 'xxfld']\n",
      "\t['models', ' ', 'xxfld', ' ', 'xxfld', ' ', 'and', ' ', 'the', ' ', 'models', 'xxfld']\n",
      "\t['models', ' ', 'xxfld', ' ', 'xxfld', ' ', 'for', ' ', 'the', ' ', 'models', 'xxfld']\n",
      "\t['models', ' ', 'xxfld', ' ', 'xxfld', ' ', 'with', ' ', 'the', ' ', 'models', 'xxfld']\n",
      "\t['models', ' ', 'xxfld', ' ', 'xxfld', ' ', 'xxunk', ' ', 'the', ' ', 'models', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t[' ', 'xxfld', ' ', 'the', ' ', 'models', 'xxfld', 'they', ' ', 'mimic', 'xxfld', ' ']\n",
      "Labeled Input: \n",
      "\t[' ', 'xxfld', ' ', 'the', ' ', 'models', ' ', 'they', ' ', 'mimic', 'xxfld', ' ']\n",
      "Predictions: \n",
      "\t[' ', 'xxfld', ' ', 'the', ' ', 'models', ' ', 'they', ' ', 'mimic', 'xxfld', ' ']\n",
      "\t[' ', 'xxfld', ' ', 'the', ' ', 'models', '.', 'they', ' ', 'mimic', 'xxfld', ' ']\n",
      "\t[' ', 'xxfld', ' ', 'the', ' ', 'models', ',', 'they', ' ', 'mimic', 'xxfld', ' ']\n",
      "\t[' ', 'xxfld', ' ', 'the', ' ', 'models', '&', 'they', ' ', 'mimic', 'xxfld', ' ']\n",
      "\t[' ', 'xxfld', ' ', 'the', ' ', 'models', '..', 'they', ' ', 'mimic', 'xxfld', ' ']\n",
      "\n",
      "Masked Input: \n",
      "\t[' ', 'models', 'xxfld', 'they', ' ', 'mimic', 'xxfld', ' ', 'xxfld', ' ', 'them', ' ']\n",
      "Labeled Input: \n",
      "\t[' ', 'models', 'xxfld', 'they', ' ', 'mimic', '.', ' ', 'xxfld', ' ', 'them', ' ']\n",
      "Predictions: \n",
      "\t[' ', 'models', 'xxfld', 'they', ' ', 'mimic', '.', ' ', 'xxfld', ' ', 'them', ' ']\n",
      "\t[' ', 'models', 'xxfld', 'they', ' ', 'mimic', ',', ' ', 'xxfld', ' ', 'them', ' ']\n",
      "\t[' ', 'models', 'xxfld', 'they', ' ', 'mimic', 'xxunk', ' ', 'xxfld', ' ', 'them', ' ']\n",
      "\t[' ', 'models', 'xxfld', 'they', ' ', 'mimic', \"'\", ' ', 'xxfld', ' ', 'them', ' ']\n",
      "\t[' ', 'models', 'xxfld', 'they', ' ', 'mimic', '_', ' ', 'xxfld', ' ', 'them', ' ']\n",
      "\n",
      "Masked Input: \n",
      "\t['xxfld', 'they', ' ', 'mimic', 'xxfld', ' ', 'xxfld', ' ', 'them', ' ', 'instead', ' ']\n",
      "Labeled Input: \n",
      "\t['xxfld', 'they', ' ', 'mimic', 'xxfld', ' ', 'using', ' ', 'them', ' ', 'instead', ' ']\n",
      "Predictions: \n",
      "\t['xxfld', 'they', ' ', 'mimic', 'xxfld', ' ', 'xxunk', ' ', 'them', ' ', 'instead', ' ']\n",
      "\t['xxfld', 'they', ' ', 'mimic', 'xxfld', ' ', 'help', ' ', 'them', ' ', 'instead', ' ']\n",
      "\t['xxfld', 'they', ' ', 'mimic', 'xxfld', ' ', 'find', ' ', 'them', ' ', 'instead', ' ']\n",
      "\t['xxfld', 'they', ' ', 'mimic', 'xxfld', ' ', 'with', ' ', 'them', ' ', 'instead', ' ']\n",
      "\t['xxfld', 'they', ' ', 'mimic', 'xxfld', ' ', 'after', ' ', 'them', ' ', 'instead', ' ']\n",
      "\n",
      "Masked Input: \n",
      "\t['instead', ' ', 'of', ' ', 'the', ' ', 'xxfld', ' ', 'versions', ' ', 'would', 'xxfld']\n",
      "Labeled Input: \n",
      "\t['instead', ' ', 'of', ' ', 'the', ' ', 'large', ' ', 'versions', ' ', 'would', 'xxfld']\n",
      "Predictions: \n",
      "\t['instead', ' ', 'of', ' ', 'the', ' ', 'xxunk', ' ', 'versions', ' ', 'would', 'xxfld']\n",
      "\t['instead', ' ', 'of', ' ', 'the', ' ', 'media', ' ', 'versions', ' ', 'would', 'xxfld']\n",
      "\t['instead', ' ', 'of', ' ', 'the', ' ', 'public', ' ', 'versions', ' ', 'would', 'xxfld']\n",
      "\t['instead', ' ', 'of', ' ', 'the', ' ', 'social', ' ', 'versions', ' ', 'would', 'xxfld']\n",
      "\t['instead', ' ', 'of', ' ', 'the', ' ', 'xxnum', ' ', 'versions', ' ', 'would', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t[' ', 'xxfld', ' ', 'versions', ' ', 'would', 'xxfld', 'help', ' ', 'our', ' ', 'xxfld']\n",
      "Labeled Input: \n",
      "\t[' ', 'xxfld', ' ', 'versions', ' ', 'would', ' ', 'help', ' ', 'our', ' ', 'xxfld']\n",
      "Predictions: \n",
      "\t[' ', 'xxfld', ' ', 'versions', ' ', 'would', ' ', 'help', ' ', 'our', ' ', 'xxfld']\n",
      "\t[' ', 'xxfld', ' ', 'versions', ' ', 'would', '.', 'help', ' ', 'our', ' ', 'xxfld']\n",
      "\t[' ', 'xxfld', ' ', 'versions', ' ', 'would', ',', 'help', ' ', 'our', ' ', 'xxfld']\n",
      "\t[' ', 'xxfld', ' ', 'versions', ' ', 'would', '*', 'help', ' ', 'our', ' ', 'xxfld']\n",
      "\t[' ', 'xxfld', ' ', 'versions', ' ', 'would', 'xxunk', 'help', ' ', 'our', ' ', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t['would', 'xxfld', 'help', ' ', 'our', ' ', 'xxfld', 'xxfld', 'xxfld', '.', 'xxeos']\n",
      "Labeled Input: \n",
      "\t['would', 'xxfld', 'help', ' ', 'our', ' ', 'carbon', 'xxfld', 'xxfld', '.', 'xxeos']\n",
      "Predictions: \n",
      "\t['would', 'xxfld', 'help', ' ', 'our', ' ', 'xxunk', 'xxfld', 'xxfld', '.', 'xxeos']\n",
      "\t['would', 'xxfld', 'help', ' ', 'our', ' ', 'own', 'xxfld', 'xxfld', '.', 'xxeos']\n",
      "\t['would', 'xxfld', 'help', ' ', 'our', ' ', 'document', 'xxfld', 'xxfld', '.', 'xxeos']\n",
      "\t['would', 'xxfld', 'help', ' ', 'our', ' ', 'data', 'xxfld', 'xxfld', '.', 'xxeos']\n",
      "\t['would', 'xxfld', 'help', ' ', 'our', ' ', 'media', 'xxfld', 'xxfld', '.', 'xxeos']\n",
      "\n",
      "Masked Input: \n",
      "\t['xxfld', 'help', ' ', 'our', ' ', 'xxfld', 'xxfld', 'xxfld', '.', 'xxeos']\n",
      "Labeled Input: \n",
      "\t['xxfld', 'help', ' ', 'our', ' ', 'xxfld', ' ', 'xxfld', '.', 'xxeos']\n",
      "Predictions: \n",
      "\t['xxfld', 'help', ' ', 'our', ' ', 'xxfld', ' ', 'xxfld', '.', 'xxeos']\n",
      "\t['xxfld', 'help', ' ', 'our', ' ', 'xxfld', '.', 'xxfld', '.', 'xxeos']\n",
      "\t['xxfld', 'help', ' ', 'our', ' ', 'xxfld', ',', 'xxfld', '.', 'xxeos']\n",
      "\t['xxfld', 'help', ' ', 'our', ' ', 'xxfld', '..', 'xxfld', '.', 'xxeos']\n",
      "\t['xxfld', 'help', ' ', 'our', ' ', 'xxfld', 'xxunk', 'xxfld', '.', 'xxeos']\n",
      "\n",
      "Masked Input: \n",
      "\t['help', ' ', 'our', ' ', 'xxfld', 'xxfld', 'xxfld', '.', 'xxeos']\n",
      "Labeled Input: \n",
      "\t['help', ' ', 'our', ' ', 'xxfld', 'xxfld', 'footprint', '.', 'xxeos']\n",
      "Predictions: \n",
      "\t['help', ' ', 'our', ' ', 'xxfld', 'xxfld', 'info', '.', 'xxeos']\n",
      "\t['help', ' ', 'our', ' ', 'xxfld', 'xxfld', 'xxunk', '.', 'xxeos']\n",
      "\t['help', ' ', 'our', ' ', 'xxfld', 'xxfld', '.', '.', 'xxeos']\n",
      "\t['help', ' ', 'our', ' ', 'xxfld', 'xxfld', 'administrator', '.', 'xxeos']\n",
      "\t['help', ' ', 'our', ' ', 'xxfld', 'xxfld', 'files', '.', 'xxeos']\n",
      "\n",
      "Total Input Tokens: torch.Size([45])\n",
      "Total Correct for Top 5: 5\n",
      "Total Mask: 11\n",
      "Percent Correct: 45.45%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45454545454545453"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_predict_masks(tokenized_input, labels, tokenizer , k=5, verbose=True, verbose_length=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Mask Filling on New Unseen Pantip Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tokenize on Pantip Sample ใครเคยมีแฟนที่กินอาหารไม่ถูกปากกันแล้วรู้สึกเสียความสุขไปอย่างนึงบ้างมั้ยครับ\n",
    "https://pantip.com/topic/40006922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Input: \n",
      "\t['แฟน', 'ที่', 'กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'xxfld', 'รู้สึก', 'xxfld', 'ความสุข', 'ไป', 'อย่าง']\n",
      "Labeled Input: \n",
      "\t['แฟน', 'ที่', 'กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'แล้ว', 'รู้สึก', 'xxfld', 'ความสุข', 'ไป', 'อย่าง']\n",
      "Predictions: \n",
      "\t['แฟน', 'ที่', 'กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'แล้ว', 'รู้สึก', 'xxfld', 'ความสุข', 'ไป', 'อย่าง']\n",
      "\t['แฟน', 'ที่', 'กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'และ', 'รู้สึก', 'xxfld', 'ความสุข', 'ไป', 'อย่าง']\n",
      "\t['แฟน', 'ที่', 'กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', ' ', 'รู้สึก', 'xxfld', 'ความสุข', 'ไป', 'อย่าง']\n",
      "\t['แฟน', 'ที่', 'กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'หรือ', 'รู้สึก', 'xxfld', 'ความสุข', 'ไป', 'อย่าง']\n",
      "\t['แฟน', 'ที่', 'กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'จน', 'รู้สึก', 'xxfld', 'ความสุข', 'ไป', 'อย่าง']\n",
      "\n",
      "Masked Input: \n",
      "\t['กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'xxfld', 'รู้สึก', 'xxfld', 'ความสุข', 'ไป', 'อย่าง', 'นึง', 'บ้าง']\n",
      "Labeled Input: \n",
      "\t['กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'xxfld', 'รู้สึก', 'เสีย', 'ความสุข', 'ไป', 'อย่าง', 'นึง', 'บ้าง']\n",
      "Predictions: \n",
      "\t['กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'xxfld', 'รู้สึก', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'นึง', 'บ้าง']\n",
      "\t['กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'xxfld', 'รู้สึก', 'เสีย', 'ความสุข', 'ไป', 'อย่าง', 'นึง', 'บ้าง']\n",
      "\t['กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'xxfld', 'รู้สึก', 'สูญเสีย', 'ความสุข', 'ไป', 'อย่าง', 'นึง', 'บ้าง']\n",
      "\t['กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'xxfld', 'รู้สึก', 'โหยหา', 'ความสุข', 'ไป', 'อย่าง', 'นึง', 'บ้าง']\n",
      "\t['กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'xxfld', 'รู้สึก', 'เสียดาย', 'ความสุข', 'ไป', 'อย่าง', 'นึง', 'บ้าง']\n",
      "\n",
      "Masked Input: \n",
      "\t['มั้ย', 'ครับ', ' ', 'ก่อนอื่น', 'ผม', 'ต้อง', 'xxfld', 'ก่อน', 'เลย', 'ว่า', 'คนเรา', 'จะ']\n",
      "Labeled Input: \n",
      "\t['มั้ย', 'ครับ', ' ', 'ก่อนอื่น', 'ผม', 'ต้อง', 'บอก', 'ก่อน', 'เลย', 'ว่า', 'คนเรา', 'จะ']\n",
      "Predictions: \n",
      "\t['มั้ย', 'ครับ', ' ', 'ก่อนอื่น', 'ผม', 'ต้อง', 'บอก', 'ก่อน', 'เลย', 'ว่า', 'คนเรา', 'จะ']\n",
      "\t['มั้ย', 'ครับ', ' ', 'ก่อนอื่น', 'ผม', 'ต้อง', 'ออกตัว', 'ก่อน', 'เลย', 'ว่า', 'คนเรา', 'จะ']\n",
      "\t['มั้ย', 'ครับ', ' ', 'ก่อนอื่น', 'ผม', 'ต้อง', 'เกริ่น', 'ก่อน', 'เลย', 'ว่า', 'คนเรา', 'จะ']\n",
      "\t['มั้ย', 'ครับ', ' ', 'ก่อนอื่น', 'ผม', 'ต้อง', 'แนะนำตัว', 'ก่อน', 'เลย', 'ว่า', 'คนเรา', 'จะ']\n",
      "\t['มั้ย', 'ครับ', ' ', 'ก่อนอื่น', 'ผม', 'ต้อง', 'เล่า', 'ก่อน', 'เลย', 'ว่า', 'คนเรา', 'จะ']\n",
      "\n",
      "Masked Input: \n",
      "\t['แบบ', 'ไหน', 'ชอบ', 'แบบ', 'ไหน', 'เป็นเรื่อง', 'xxfld', 'ความชอบ', 'ส่วนตัว', 'นะ', 'ครับ', 'ทุกคน']\n",
      "Labeled Input: \n",
      "\t['แบบ', 'ไหน', 'ชอบ', 'แบบ', 'ไหน', 'เป็นเรื่อง', 'ของ', 'ความชอบ', 'ส่วนตัว', 'นะ', 'ครับ', 'ทุกคน']\n",
      "Predictions: \n",
      "\t['แบบ', 'ไหน', 'ชอบ', 'แบบ', 'ไหน', 'เป็นเรื่อง', 'ของ', 'ความชอบ', 'ส่วนตัว', 'นะ', 'ครับ', 'ทุกคน']\n",
      "\t['แบบ', 'ไหน', 'ชอบ', 'แบบ', 'ไหน', 'เป็นเรื่อง', 'ที่', 'ความชอบ', 'ส่วนตัว', 'นะ', 'ครับ', 'ทุกคน']\n",
      "\t['แบบ', 'ไหน', 'ชอบ', 'แบบ', 'ไหน', 'เป็นเรื่อง', 'ความชอบ', 'ความชอบ', 'ส่วนตัว', 'นะ', 'ครับ', 'ทุกคน']\n",
      "\t['แบบ', 'ไหน', 'ชอบ', 'แบบ', 'ไหน', 'เป็นเรื่อง', 'ที่มี', 'ความชอบ', 'ส่วนตัว', 'นะ', 'ครับ', 'ทุกคน']\n",
      "\t['แบบ', 'ไหน', 'ชอบ', 'แบบ', 'ไหน', 'เป็นเรื่อง', 'หนึ่ง', 'ความชอบ', 'ส่วนตัว', 'นะ', 'ครับ', 'ทุกคน']\n",
      "\n",
      "Masked Input: \n",
      "\t['ทุกคน', 'มี', 'สิทธิ', 'ใน', 'การเลือก', 'ของที่ชอบ', 'xxfld', 'ไม่ชอบ', 'อยู่แล้ว', ' ', 'แต่', 'ผม']\n",
      "Labeled Input: \n",
      "\t['ทุกคน', 'มี', 'สิทธิ', 'ใน', 'การเลือก', 'ของที่ชอบ', 'และ', 'ไม่ชอบ', 'อยู่แล้ว', ' ', 'แต่', 'ผม']\n",
      "Predictions: \n",
      "\t['ทุกคน', 'มี', 'สิทธิ', 'ใน', 'การเลือก', 'ของที่ชอบ', 'และ', 'ไม่ชอบ', 'อยู่แล้ว', ' ', 'แต่', 'ผม']\n",
      "\t['ทุกคน', 'มี', 'สิทธิ', 'ใน', 'การเลือก', 'ของที่ชอบ', 'ผม', 'ไม่ชอบ', 'อยู่แล้ว', ' ', 'แต่', 'ผม']\n",
      "\t['ทุกคน', 'มี', 'สิทธิ', 'ใน', 'การเลือก', 'ของที่ชอบ', 'ที่', 'ไม่ชอบ', 'อยู่แล้ว', ' ', 'แต่', 'ผม']\n",
      "\t['ทุกคน', 'มี', 'สิทธิ', 'ใน', 'การเลือก', 'ของที่ชอบ', 'ก็', 'ไม่ชอบ', 'อยู่แล้ว', ' ', 'แต่', 'ผม']\n",
      "\t['ทุกคน', 'มี', 'สิทธิ', 'ใน', 'การเลือก', 'ของที่ชอบ', 'หรือ', 'ไม่ชอบ', 'อยู่แล้ว', ' ', 'แต่', 'ผม']\n",
      "\n",
      "Masked Input: \n",
      "\t['อยู่แล้ว', ' ', 'แต่', 'ผม', 'รู้สึก', 'ว่า', 'xxfld', 'ผม', 'กำลัง', 'ประสบปัญหา', 'ที่', 'ดูเหมือน']\n",
      "Labeled Input: \n",
      "\t['อยู่แล้ว', ' ', 'แต่', 'ผม', 'รู้สึก', 'ว่า', 'ตอนนี้', 'ผม', 'กำลัง', 'ประสบปัญหา', 'ที่', 'ดูเหมือน']\n",
      "Predictions: \n",
      "\t['อยู่แล้ว', ' ', 'แต่', 'ผม', 'รู้สึก', 'ว่า', 'แฟน', 'ผม', 'กำลัง', 'ประสบปัญหา', 'ที่', 'ดูเหมือน']\n",
      "\t['อยู่แล้ว', ' ', 'แต่', 'ผม', 'รู้สึก', 'ว่า', 'ตอนนี้', 'ผม', 'กำลัง', 'ประสบปัญหา', 'ที่', 'ดูเหมือน']\n",
      "\t['อยู่แล้ว', ' ', 'แต่', 'ผม', 'รู้สึก', 'ว่า', 'ช่วงนี้', 'ผม', 'กำลัง', 'ประสบปัญหา', 'ที่', 'ดูเหมือน']\n",
      "\t['อยู่แล้ว', ' ', 'แต่', 'ผม', 'รู้สึก', 'ว่า', 'ตัว', 'ผม', 'กำลัง', 'ประสบปัญหา', 'ที่', 'ดูเหมือน']\n",
      "\t['อยู่แล้ว', ' ', 'แต่', 'ผม', 'รู้สึก', 'ว่า', 'ตอนนั้น', 'ผม', 'กำลัง', 'ประสบปัญหา', 'ที่', 'ดูเหมือน']\n",
      "\n",
      "Masked Input: \n",
      "\t['ประสบปัญหา', 'ที่', 'ดูเหมือน', 'จะ', 'เล็ก', 'แต่', 'xxfld', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', 'xxfld']\n",
      "Labeled Input: \n",
      "\t['ประสบปัญหา', 'ที่', 'ดูเหมือน', 'จะ', 'เล็ก', 'แต่', 'กลายเป็น', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', 'xxfld']\n",
      "Predictions: \n",
      "\t['ประสบปัญหา', 'ที่', 'ดูเหมือน', 'จะ', 'เล็ก', 'แต่', 'ยอมรับ', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', 'xxfld']\n",
      "\t['ประสบปัญหา', 'ที่', 'ดูเหมือน', 'จะ', 'เล็ก', 'แต่', 'ผม', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', 'xxfld']\n",
      "\t['ประสบปัญหา', 'ที่', 'ดูเหมือน', 'จะ', 'เล็ก', 'แต่', 'รู้สึก', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', 'xxfld']\n",
      "\t['ประสบปัญหา', 'ที่', 'ดูเหมือน', 'จะ', 'เล็ก', 'แต่', 'แน่นอน', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', 'xxfld']\n",
      "\t['ประสบปัญหา', 'ที่', 'ดูเหมือน', 'จะ', 'เล็ก', 'แต่', 'กลายเป็น', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t['แต่', 'xxfld', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', 'xxfld', 'ผม', 'คบ', 'กับ', 'แฟน', 'มา']\n",
      "Labeled Input: \n",
      "\t['แต่', 'xxfld', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', ' ', 'ผม', 'คบ', 'กับ', 'แฟน', 'มา']\n",
      "Predictions: \n",
      "\t['แต่', 'xxfld', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', ' ', 'ผม', 'คบ', 'กับ', 'แฟน', 'มา']\n",
      "\t['แต่', 'xxfld', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', 'เพราะ', 'ผม', 'คบ', 'กับ', 'แฟน', 'มา']\n",
      "\t['แต่', 'xxfld', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', 'มาก', 'ผม', 'คบ', 'กับ', 'แฟน', 'มา']\n",
      "\t['แต่', 'xxfld', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', 'สำหรับ', 'ผม', 'คบ', 'กับ', 'แฟน', 'มา']\n",
      "\t['แต่', 'xxfld', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', 'จน', 'ผม', 'คบ', 'กับ', 'แฟน', 'มา']\n",
      "\n",
      "Masked Input: \n",
      "\t['มา', 'xxnum', ' ', 'ปี', 'แล้ว', 'ครับ', 'xxfld', 'ผม', 'เป็น', 'คน', 'ชอบ', 'กิน']\n",
      "Labeled Input: \n",
      "\t['มา', 'xxnum', ' ', 'ปี', 'แล้ว', 'ครับ', ' ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'กิน']\n",
      "Predictions: \n",
      "\t['มา', 'xxnum', ' ', 'ปี', 'แล้ว', 'ครับ', ' ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'กิน']\n",
      "\t['มา', 'xxnum', ' ', 'ปี', 'แล้ว', 'ครับ', 'แฟน', 'ผม', 'เป็น', 'คน', 'ชอบ', 'กิน']\n",
      "\t['มา', 'xxnum', ' ', 'ปี', 'แล้ว', 'ครับ', 'แต่', 'ผม', 'เป็น', 'คน', 'ชอบ', 'กิน']\n",
      "\t['มา', 'xxnum', ' ', 'ปี', 'แล้ว', 'ครับ', 'คือ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'กิน']\n",
      "\t['มา', 'xxnum', ' ', 'ปี', 'แล้ว', 'ครับ', 'และ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'กิน']\n",
      "\n",
      "Masked Input: \n",
      "\t['ผม', 'เป็น', 'คน', 'ชอบ', 'กิน', 'อาหาร', 'xxfld', 'และ', 'ปลาดิบ', 'แต่', 'แฟน', 'ผม']\n",
      "Labeled Input: \n",
      "\t['ผม', 'เป็น', 'คน', 'ชอบ', 'กิน', 'อาหาร', 'ญี่ปุ่น', 'และ', 'ปลาดิบ', 'แต่', 'แฟน', 'ผม']\n",
      "Predictions: \n",
      "\t['ผม', 'เป็น', 'คน', 'ชอบ', 'กิน', 'อาหาร', 'ญี่ปุ่น', 'และ', 'ปลาดิบ', 'แต่', 'แฟน', 'ผม']\n",
      "\t['ผม', 'เป็น', 'คน', 'ชอบ', 'กิน', 'อาหาร', 'ปลาดิบ', 'และ', 'ปลาดิบ', 'แต่', 'แฟน', 'ผม']\n",
      "\t['ผม', 'เป็น', 'คน', 'ชอบ', 'กิน', 'อาหาร', 'เกาหลี', 'และ', 'ปลาดิบ', 'แต่', 'แฟน', 'ผม']\n",
      "\t['ผม', 'เป็น', 'คน', 'ชอบ', 'กิน', 'อาหาร', 'ไทย', 'และ', 'ปลาดิบ', 'แต่', 'แฟน', 'ผม']\n",
      "\t['ผม', 'เป็น', 'คน', 'ชอบ', 'กิน', 'อาหาร', 'รสจัด', 'และ', 'ปลาดิบ', 'แต่', 'แฟน', 'ผม']\n",
      "\n",
      "Masked Input: \n",
      "\t[' ', 'เรา', 'เลย', 'ไม่ได้', 'เข้า', 'ทาน', 'xxfld', 'บุ', 'xxunk', 'เนื้อ', 'และ', 'บุ']\n",
      "Labeled Input: \n",
      "\t[' ', 'เรา', 'เลย', 'ไม่ได้', 'เข้า', 'ทาน', 'ร้าน', 'บุ', 'xxunk', 'เนื้อ', 'และ', 'บุ']\n",
      "Predictions: \n",
      "\t[' ', 'เรา', 'เลย', 'ไม่ได้', 'เข้า', 'ทาน', 'ทั้ง', 'บุ', 'xxunk', 'เนื้อ', 'และ', 'บุ']\n",
      "\t[' ', 'เรา', 'เลย', 'ไม่ได้', 'เข้า', 'ทาน', 'กับ', 'บุ', 'xxunk', 'เนื้อ', 'และ', 'บุ']\n",
      "\t[' ', 'เรา', 'เลย', 'ไม่ได้', 'เข้า', 'ทาน', ' ', 'บุ', 'xxunk', 'เนื้อ', 'และ', 'บุ']\n",
      "\t[' ', 'เรา', 'เลย', 'ไม่ได้', 'เข้า', 'ทาน', 'อาหาร', 'บุ', 'xxunk', 'เนื้อ', 'และ', 'บุ']\n",
      "\t[' ', 'เรา', 'เลย', 'ไม่ได้', 'เข้า', 'ทาน', 'แต่', 'บุ', 'xxunk', 'เนื้อ', 'และ', 'บุ']\n",
      "\n",
      "Masked Input: \n",
      "\t['xxfld', 'บุ', 'xxunk', 'เนื้อ', 'และ', 'บุ', 'xxfld', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก']\n",
      "Labeled Input: \n",
      "\t['xxfld', 'บุ', 'xxunk', 'เนื้อ', 'และ', 'บุ', 'xxunk', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก']\n",
      "Predictions: \n",
      "\t['xxfld', 'บุ', 'xxunk', 'เนื้อ', 'และ', 'บุ', 'xxunk', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก']\n",
      "\t['xxfld', 'บุ', 'xxunk', 'เนื้อ', 'และ', 'บุ', 'ง', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก']\n",
      "\t['xxfld', 'บุ', 'xxunk', 'เนื้อ', 'และ', 'บุ', 'ย่าง', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก']\n",
      "\t['xxfld', 'บุ', 'xxunk', 'เนื้อ', 'และ', 'บุ', 'น', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก']\n",
      "\t['xxfld', 'บุ', 'xxunk', 'เนื้อ', 'และ', 'บุ', 'ล', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก']\n",
      "\n",
      "Masked Input: \n",
      "\t['xxfld', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก', 'xxfld', 'แฟน', 'ผม', 'xxfld', 'ไม่', 'xxfld']\n",
      "Labeled Input: \n",
      "\t['xxfld', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก', 'ลัว', 'แฟน', 'ผม', 'xxfld', 'ไม่', 'xxfld']\n",
      "Predictions: \n",
      "\t['xxfld', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก', 'ว่า', 'แฟน', 'ผม', 'xxfld', 'ไม่', 'xxfld']\n",
      "\t['xxfld', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก', 'เหมือน', 'แฟน', 'ผม', 'xxfld', 'ไม่', 'xxfld']\n",
      "\t['xxfld', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก', 'เหมือนว่า', 'แฟน', 'ผม', 'xxfld', 'ไม่', 'xxfld']\n",
      "\t['xxfld', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก', 'ยังไง', 'แฟน', 'ผม', 'xxfld', 'ไม่', 'xxfld']\n",
      "\t['xxfld', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก', 'อิจฉา', 'แฟน', 'ผม', 'xxfld', 'ไม่', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t['กัน', 'เพราะ', 'รู้สึก', 'xxfld', 'แฟน', 'ผม', 'xxfld', 'ไม่', 'xxfld', ' ', 'และ', 'xxfld']\n",
      "Labeled Input: \n",
      "\t['กัน', 'เพราะ', 'รู้สึก', 'xxfld', 'แฟน', 'ผม', 'ทาน', 'ไม่', 'xxfld', ' ', 'และ', 'xxfld']\n",
      "Predictions: \n",
      "\t['กัน', 'เพราะ', 'รู้สึก', 'xxfld', 'แฟน', 'ผม', 'จะ', 'ไม่', 'xxfld', ' ', 'และ', 'xxfld']\n",
      "\t['กัน', 'เพราะ', 'รู้สึก', 'xxfld', 'แฟน', 'ผม', 'กิน', 'ไม่', 'xxfld', ' ', 'และ', 'xxfld']\n",
      "\t['กัน', 'เพราะ', 'รู้สึก', 'xxfld', 'แฟน', 'ผม', 'ยัง', 'ไม่', 'xxfld', ' ', 'และ', 'xxfld']\n",
      "\t['กัน', 'เพราะ', 'รู้สึก', 'xxfld', 'แฟน', 'ผม', 'ก็', 'ไม่', 'xxfld', ' ', 'และ', 'xxfld']\n",
      "\t['กัน', 'เพราะ', 'รู้สึก', 'xxfld', 'แฟน', 'ผม', 'ดู', 'ไม่', 'xxfld', ' ', 'และ', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t['รู้สึก', 'xxfld', 'แฟน', 'ผม', 'xxfld', 'ไม่', 'xxfld', ' ', 'และ', 'xxfld', 'เลย', 'คือ']\n",
      "Labeled Input: \n",
      "\t['รู้สึก', 'xxfld', 'แฟน', 'ผม', 'xxfld', 'ไม่', 'คุ้ม', ' ', 'และ', 'xxfld', 'เลย', 'คือ']\n",
      "Predictions: \n",
      "\t['รู้สึก', 'xxfld', 'แฟน', 'ผม', 'xxfld', 'ไม่', 'กิน', ' ', 'และ', 'xxfld', 'เลย', 'คือ']\n",
      "\t['รู้สึก', 'xxfld', 'แฟน', 'ผม', 'xxfld', 'ไม่', 'มีความสุข', ' ', 'และ', 'xxfld', 'เลย', 'คือ']\n",
      "\t['รู้สึก', 'xxfld', 'แฟน', 'ผม', 'xxfld', 'ไม่', 'โอเค', ' ', 'และ', 'xxfld', 'เลย', 'คือ']\n",
      "\t['รู้สึก', 'xxfld', 'แฟน', 'ผม', 'xxfld', 'ไม่', 'เป็น', ' ', 'และ', 'xxfld', 'เลย', 'คือ']\n",
      "\t['รู้สึก', 'xxfld', 'แฟน', 'ผม', 'xxfld', 'ไม่', 'ออก', ' ', 'และ', 'xxfld', 'เลย', 'คือ']\n",
      "\n",
      "Masked Input: \n",
      "\t['ผม', 'xxfld', 'ไม่', 'xxfld', ' ', 'และ', 'xxfld', 'เลย', 'คือ', 'ผม', 'เป็น', 'คน']\n",
      "Labeled Input: \n",
      "\t['ผม', 'xxfld', 'ไม่', 'xxfld', ' ', 'และ', 'เรื่องใหญ่', 'เลย', 'คือ', 'ผม', 'เป็น', 'คน']\n",
      "Predictions: \n",
      "\t['ผม', 'xxfld', 'ไม่', 'xxfld', ' ', 'และ', 'ที่สำคัญ', 'เลย', 'คือ', 'ผม', 'เป็น', 'คน']\n",
      "\t['ผม', 'xxfld', 'ไม่', 'xxfld', ' ', 'และ', 'เริ่ม', 'เลย', 'คือ', 'ผม', 'เป็น', 'คน']\n",
      "\t['ผม', 'xxfld', 'ไม่', 'xxfld', ' ', 'และ', 'ข้อเสีย', 'เลย', 'คือ', 'ผม', 'เป็น', 'คน']\n",
      "\t['ผม', 'xxfld', 'ไม่', 'xxfld', ' ', 'และ', 'ประเด็น', 'เลย', 'คือ', 'ผม', 'เป็น', 'คน']\n",
      "\t['ผม', 'xxfld', 'ไม่', 'xxfld', ' ', 'และ', 'เหตุผล', 'เลย', 'คือ', 'ผม', 'เป็น', 'คน']\n",
      "\n",
      "Masked Input: \n",
      "\t['เลย', 'คือ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'xxfld', 'รสจัด', 'และ', 'รส', 'เผ็ด', 'มาก']\n",
      "Labeled Input: \n",
      "\t['เลย', 'คือ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'ทานอาหาร', 'รสจัด', 'และ', 'รส', 'เผ็ด', 'มาก']\n",
      "Predictions: \n",
      "\t['เลย', 'คือ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'ทานอาหาร', 'รสจัด', 'และ', 'รส', 'เผ็ด', 'มาก']\n",
      "\t['เลย', 'คือ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'อาหาร', 'รสจัด', 'และ', 'รส', 'เผ็ด', 'มาก']\n",
      "\t['เลย', 'คือ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'กิน', 'รสจัด', 'และ', 'รส', 'เผ็ด', 'มาก']\n",
      "\t['เลย', 'คือ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'ทาน', 'รสจัด', 'และ', 'รส', 'เผ็ด', 'มาก']\n",
      "\t['เลย', 'คือ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'ทำอาหาร', 'รสจัด', 'และ', 'รส', 'เผ็ด', 'มาก']\n",
      "\n",
      "Masked Input: \n",
      "\t['และ', 'รส', 'เผ็ด', 'มาก', ' ', 'แต่', 'xxfld', 'ผม', 'ทาน', 'เผ็ด', 'ไม่ได้', 'เลยเวลา']\n",
      "Labeled Input: \n",
      "\t['และ', 'รส', 'เผ็ด', 'มาก', ' ', 'แต่', 'แฟน', 'ผม', 'ทาน', 'เผ็ด', 'ไม่ได้', 'เลยเวลา']\n",
      "Predictions: \n",
      "\t['และ', 'รส', 'เผ็ด', 'มาก', ' ', 'แต่', 'แฟน', 'ผม', 'ทาน', 'เผ็ด', 'ไม่ได้', 'เลยเวลา']\n",
      "\t['และ', 'รส', 'เผ็ด', 'มาก', ' ', 'แต่', 'เวลา', 'ผม', 'ทาน', 'เผ็ด', 'ไม่ได้', 'เลยเวลา']\n",
      "\t['และ', 'รส', 'เผ็ด', 'มาก', ' ', 'แต่', 'ถ้า', 'ผม', 'ทาน', 'เผ็ด', 'ไม่ได้', 'เลยเวลา']\n",
      "\t['และ', 'รส', 'เผ็ด', 'มาก', ' ', 'แต่', 'พอ', 'ผม', 'ทาน', 'เผ็ด', 'ไม่ได้', 'เลยเวลา']\n",
      "\t['และ', 'รส', 'เผ็ด', 'มาก', ' ', 'แต่', 'ทุกครั้งที่', 'ผม', 'ทาน', 'เผ็ด', 'ไม่ได้', 'เลยเวลา']\n",
      "\n",
      "Masked Input: \n",
      "\t['กัน', 'ก็', 'จะ', 'สั่ง', ' ', 'ส้มตำ', 'xxfld', 'ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ']\n",
      "Labeled Input: \n",
      "\t['กัน', 'ก็', 'จะ', 'สั่ง', ' ', 'ส้มตำ', 'ไม่', 'ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ']\n",
      "Predictions: \n",
      "\t['กัน', 'ก็', 'จะ', 'สั่ง', ' ', 'ส้มตำ', 'ไม่', 'ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ']\n",
      "\t['กัน', 'ก็', 'จะ', 'สั่ง', ' ', 'ส้มตำ', 'ที่', 'ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ']\n",
      "\t['กัน', 'ก็', 'จะ', 'สั่ง', ' ', 'ส้มตำ', 'แบบ', 'ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ']\n",
      "\t['กัน', 'ก็', 'จะ', 'สั่ง', ' ', 'ส้มตำ', 'ไม่ต้อง', 'ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ']\n",
      "\t['กัน', 'ก็', 'จะ', 'สั่ง', ' ', 'ส้มตำ', 'แต่', 'ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ']\n",
      "\n",
      "Masked Input: \n",
      "\t['ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'xxfld', 'xxfld', ' ', 'ลาบ', 'ไม่', 'ใส่']\n",
      "Labeled Input: \n",
      "\t['ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'ใส่', 'xxfld', ' ', 'ลาบ', 'ไม่', 'ใส่']\n",
      "Predictions: \n",
      "\t['ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'ใส่', 'xxfld', ' ', 'ลาบ', 'ไม่', 'ใส่']\n",
      "\t['ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'กิน', 'xxfld', ' ', 'ลาบ', 'ไม่', 'ใส่']\n",
      "\t['ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'เผ็ด', 'xxfld', ' ', 'ลาบ', 'ไม่', 'ใส่']\n",
      "\t['ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'ก็', 'xxfld', ' ', 'ลาบ', 'ไม่', 'ใส่']\n",
      "\t['ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'ใช้', 'xxfld', ' ', 'ลาบ', 'ไม่', 'ใส่']\n",
      "\n",
      "Masked Input: \n",
      "\t['พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'xxfld', 'xxfld', ' ', 'ลาบ', 'ไม่', 'ใส่', 'พริก']\n",
      "Labeled Input: \n",
      "\t['พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'xxfld', 'พริก', ' ', 'ลาบ', 'ไม่', 'ใส่', 'พริก']\n",
      "Predictions: \n",
      "\t['พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'xxfld', 'พริก', ' ', 'ลาบ', 'ไม่', 'ใส่', 'พริก']\n",
      "\t['พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'xxfld', 'เผ็ด', ' ', 'ลาบ', 'ไม่', 'ใส่', 'พริก']\n",
      "\t['พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'xxfld', 'ผัก', ' ', 'ลาบ', 'ไม่', 'ใส่', 'พริก']\n",
      "\t['พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'xxfld', 'เค็ม', ' ', 'ลาบ', 'ไม่', 'ใส่', 'พริก']\n",
      "\t['พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'xxfld', 'หอย', ' ', 'ลาบ', 'ไม่', 'ใส่', 'พริก']\n",
      "\n",
      "Masked Input: \n",
      "\t['แฟน', 'ผม', 'จะ', 'ไม่ชอบ', 'กิน', 'ผัก', 'xxfld', 'สั่ง', 'กับข้าว', 'ที่', 'เป็น', 'ผัก']\n",
      "Labeled Input: \n",
      "\t['แฟน', 'ผม', 'จะ', 'ไม่ชอบ', 'กิน', 'ผัก', 'ไม่ค่อย', 'สั่ง', 'กับข้าว', 'ที่', 'เป็น', 'ผัก']\n",
      "Predictions: \n",
      "\t['แฟน', 'ผม', 'จะ', 'ไม่ชอบ', 'กิน', 'ผัก', ' ', 'สั่ง', 'กับข้าว', 'ที่', 'เป็น', 'ผัก']\n",
      "\t['แฟน', 'ผม', 'จะ', 'ไม่ชอบ', 'กิน', 'ผัก', 'เวลา', 'สั่ง', 'กับข้าว', 'ที่', 'เป็น', 'ผัก']\n",
      "\t['แฟน', 'ผม', 'จะ', 'ไม่ชอบ', 'กิน', 'ผัก', 'ผม', 'สั่ง', 'กับข้าว', 'ที่', 'เป็น', 'ผัก']\n",
      "\t['แฟน', 'ผม', 'จะ', 'ไม่ชอบ', 'กิน', 'ผัก', 'และ', 'สั่ง', 'กับข้าว', 'ที่', 'เป็น', 'ผัก']\n",
      "\t['แฟน', 'ผม', 'จะ', 'ไม่ชอบ', 'กิน', 'ผัก', 'จะ', 'สั่ง', 'กับข้าว', 'ที่', 'เป็น', 'ผัก']\n",
      "\n",
      "Masked Input: \n",
      "\t['ผัก', 'แล้ว', 'ผม', 'ชอบ', 'ผักบุ้ง', 'ทอด', 'xxfld', ' ', 'เห็ด', 'หอม', 'xxfld', 'ทอด']\n",
      "Labeled Input: \n",
      "\t['ผัก', 'แล้ว', 'ผม', 'ชอบ', 'ผักบุ้ง', 'ทอด', 'กรอบ', ' ', 'เห็ด', 'หอม', 'xxfld', 'ทอด']\n",
      "Predictions: \n",
      "\t['ผัก', 'แล้ว', 'ผม', 'ชอบ', 'ผักบุ้ง', 'ทอด', 'มาก', ' ', 'เห็ด', 'หอม', 'xxfld', 'ทอด']\n",
      "\t['ผัก', 'แล้ว', 'ผม', 'ชอบ', 'ผักบุ้ง', 'ทอด', 'ๆ', ' ', 'เห็ด', 'หอม', 'xxfld', 'ทอด']\n",
      "\t['ผัก', 'แล้ว', 'ผม', 'ชอบ', 'ผักบุ้ง', 'ทอด', 'เลย', ' ', 'เห็ด', 'หอม', 'xxfld', 'ทอด']\n",
      "\t['ผัก', 'แล้ว', 'ผม', 'ชอบ', 'ผักบุ้ง', 'ทอด', 'ด้วย', ' ', 'เห็ด', 'หอม', 'xxfld', 'ทอด']\n",
      "\t['ผัก', 'แล้ว', 'ผม', 'ชอบ', 'ผักบุ้ง', 'ทอด', 'ครับ', ' ', 'เห็ด', 'หอม', 'xxfld', 'ทอด']\n",
      "\n",
      "Masked Input: \n",
      "\t['ผักบุ้ง', 'ทอด', 'xxfld', ' ', 'เห็ด', 'หอม', 'xxfld', 'ทอด', 'มาก', 'xxfld', 'แต่', 'ก็']\n",
      "Labeled Input: \n",
      "\t['ผักบุ้ง', 'ทอด', 'xxfld', ' ', 'เห็ด', 'หอม', 'สด', 'ทอด', 'มาก', 'xxfld', 'แต่', 'ก็']\n",
      "Predictions: \n",
      "\t['ผักบุ้ง', 'ทอด', 'xxfld', ' ', 'เห็ด', 'หอม', ' ', 'ทอด', 'มาก', 'xxfld', 'แต่', 'ก็']\n",
      "\t['ผักบุ้ง', 'ทอด', 'xxfld', ' ', 'เห็ด', 'หอม', 'ๆ', 'ทอด', 'มาก', 'xxfld', 'แต่', 'ก็']\n",
      "\t['ผักบุ้ง', 'ทอด', 'xxfld', ' ', 'เห็ด', 'หอม', 'และ', 'ทอด', 'มาก', 'xxfld', 'แต่', 'ก็']\n",
      "\t['ผักบุ้ง', 'ทอด', 'xxfld', ' ', 'เห็ด', 'หอม', 'ผัก', 'ทอด', 'มาก', 'xxfld', 'แต่', 'ก็']\n",
      "\t['ผักบุ้ง', 'ทอด', 'xxfld', ' ', 'เห็ด', 'หอม', 'ชอบ', 'ทอด', 'มาก', 'xxfld', 'แต่', 'ก็']\n",
      "\n",
      "Masked Input: \n",
      "\t[' ', 'เห็ด', 'หอม', 'xxfld', 'ทอด', 'มาก', 'xxfld', 'แต่', 'ก็', 'ไม่ได้', 'xxfld', 'เพราะว่า']\n",
      "Labeled Input: \n",
      "\t[' ', 'เห็ด', 'หอม', 'xxfld', 'ทอด', 'มาก', ' ', 'แต่', 'ก็', 'ไม่ได้', 'xxfld', 'เพราะว่า']\n",
      "Predictions: \n",
      "\t[' ', 'เห็ด', 'หอม', 'xxfld', 'ทอด', 'มาก', ' ', 'แต่', 'ก็', 'ไม่ได้', 'xxfld', 'เพราะว่า']\n",
      "\t[' ', 'เห็ด', 'หอม', 'xxfld', 'ทอด', 'มาก', 'ๆ', 'แต่', 'ก็', 'ไม่ได้', 'xxfld', 'เพราะว่า']\n",
      "\t[' ', 'เห็ด', 'หอม', 'xxfld', 'ทอด', 'มาก', 'ครับ', 'แต่', 'ก็', 'ไม่ได้', 'xxfld', 'เพราะว่า']\n",
      "\t[' ', 'เห็ด', 'หอม', 'xxfld', 'ทอด', 'มาก', 'เลย', 'แต่', 'ก็', 'ไม่ได้', 'xxfld', 'เพราะว่า']\n",
      "\t[' ', 'เห็ด', 'หอม', 'xxfld', 'ทอด', 'มาก', 'นะ', 'แต่', 'ก็', 'ไม่ได้', 'xxfld', 'เพราะว่า']\n",
      "\n",
      "Masked Input: \n",
      "\t['ทอด', 'มาก', 'xxfld', 'แต่', 'ก็', 'ไม่ได้', 'xxfld', 'เพราะว่า', 'เธอ', 'ไม่', 'กิน', 'ถึง']\n",
      "Labeled Input: \n",
      "\t['ทอด', 'มาก', 'xxfld', 'แต่', 'ก็', 'ไม่ได้', 'สั่ง', 'เพราะว่า', 'เธอ', 'ไม่', 'กิน', 'ถึง']\n",
      "Predictions: \n",
      "\t['ทอด', 'มาก', 'xxfld', 'แต่', 'ก็', 'ไม่ได้', 'สั่ง', 'เพราะว่า', 'เธอ', 'ไม่', 'กิน', 'ถึง']\n",
      "\t['ทอด', 'มาก', 'xxfld', 'แต่', 'ก็', 'ไม่ได้', 'กิน', 'เพราะว่า', 'เธอ', 'ไม่', 'กิน', 'ถึง']\n",
      "\t['ทอด', 'มาก', 'xxfld', 'แต่', 'ก็', 'ไม่ได้', 'อยากกิน', 'เพราะว่า', 'เธอ', 'ไม่', 'กิน', 'ถึง']\n",
      "\t['ทอด', 'มาก', 'xxfld', 'แต่', 'ก็', 'ไม่ได้', 'ชอบ', 'เพราะว่า', 'เธอ', 'ไม่', 'กิน', 'ถึง']\n",
      "\t['ทอด', 'มาก', 'xxfld', 'แต่', 'ก็', 'ไม่ได้', 'โกรธ', 'เพราะว่า', 'เธอ', 'ไม่', 'กิน', 'ถึง']\n",
      "\n",
      "Masked Input: \n",
      "\t['จะ', 'บอก', 'ให้', 'สั่ง', 'เลย', 'ๆ', 'xxfld', 'เถอะ', 'xxfld', 'ผม', 'ก็', 'ยัง']\n",
      "Labeled Input: \n",
      "\t['จะ', 'บอก', 'ให้', 'สั่ง', 'เลย', 'ๆ', 'ก็', 'เถอะ', 'xxfld', 'ผม', 'ก็', 'ยัง']\n",
      "Predictions: \n",
      "\t['จะ', 'บอก', 'ให้', 'สั่ง', 'เลย', 'ๆ', 'ก็', 'เถอะ', 'xxfld', 'ผม', 'ก็', 'ยัง']\n",
      "\t['จะ', 'บอก', 'ให้', 'สั่ง', 'เลย', 'ๆ', 'ไป', 'เถอะ', 'xxfld', 'ผม', 'ก็', 'ยัง']\n",
      "\t['จะ', 'บอก', 'ให้', 'สั่ง', 'เลย', 'ๆ', 'ก็ได้', 'เถอะ', 'xxfld', 'ผม', 'ก็', 'ยัง']\n",
      "\t['จะ', 'บอก', 'ให้', 'สั่ง', 'เลย', 'ๆ', 'หน่อย', 'เถอะ', 'xxfld', 'ผม', 'ก็', 'ยัง']\n",
      "\t['จะ', 'บอก', 'ให้', 'สั่ง', 'เลย', 'ๆ', 'บ้าง', 'เถอะ', 'xxfld', 'ผม', 'ก็', 'ยัง']\n",
      "\n",
      "Masked Input: \n",
      "\t['ให้', 'สั่ง', 'เลย', 'ๆ', 'xxfld', 'เถอะ', 'xxfld', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ']\n",
      "Labeled Input: \n",
      "\t['ให้', 'สั่ง', 'เลย', 'ๆ', 'xxfld', 'เถอะ', 'แต่', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ']\n",
      "Predictions: \n",
      "\t['ให้', 'สั่ง', 'เลย', 'ๆ', 'xxfld', 'เถอะ', 'แต่', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ']\n",
      "\t['ให้', 'สั่ง', 'เลย', 'ๆ', 'xxfld', 'เถอะ', 'และ', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ']\n",
      "\t['ให้', 'สั่ง', 'เลย', 'ๆ', 'xxfld', 'เถอะ', 'ตอนนี้', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ']\n",
      "\t['ให้', 'สั่ง', 'เลย', 'ๆ', 'xxfld', 'เถอะ', 'บางที', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ']\n",
      "\t['ให้', 'สั่ง', 'เลย', 'ๆ', 'xxfld', 'เถอะ', 'เพราะ', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ']\n",
      "\n",
      "Masked Input: \n",
      "\t['xxfld', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ผม', 'รู้สึก']\n",
      "Labeled Input: \n",
      "\t['xxfld', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ', 'อยู่ดี', 'อ่ะ', 'ครับ', ' ', 'ผม', 'รู้สึก']\n",
      "Predictions: \n",
      "\t['xxfld', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ', 'อยู่', 'อ่ะ', 'ครับ', ' ', 'ผม', 'รู้สึก']\n",
      "\t['xxfld', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ', 'อยู่ดี', 'อ่ะ', 'ครับ', ' ', 'ผม', 'รู้สึก']\n",
      "\t['xxfld', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ', 'เลย', 'อ่ะ', 'ครับ', ' ', 'ผม', 'รู้สึก']\n",
      "\t['xxfld', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ', 'อีก', 'อ่ะ', 'ครับ', ' ', 'ผม', 'รู้สึก']\n",
      "\t['xxfld', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ', 'ด้วย', 'อ่ะ', 'ครับ', ' ', 'ผม', 'รู้สึก']\n",
      "\n",
      "Masked Input: \n",
      "\t['ผม', 'รู้สึก', 'กิน', 'อาหาร', 'ไม่', 'มีความสุข', 'xxfld', 'ชีวิต', 'ผม', 'xxfld', 'รส', 'เผ็ด']\n",
      "Labeled Input: \n",
      "\t['ผม', 'รู้สึก', 'กิน', 'อาหาร', 'ไม่', 'มีความสุข', 'เลย', 'ชีวิต', 'ผม', 'xxfld', 'รส', 'เผ็ด']\n",
      "Predictions: \n",
      "\t['ผม', 'รู้สึก', 'กิน', 'อาหาร', 'ไม่', 'มีความสุข', ' ', 'ชีวิต', 'ผม', 'xxfld', 'รส', 'เผ็ด']\n",
      "\t['ผม', 'รู้สึก', 'กิน', 'อาหาร', 'ไม่', 'มีความสุข', 'เลย', 'ชีวิต', 'ผม', 'xxfld', 'รส', 'เผ็ด']\n",
      "\t['ผม', 'รู้สึก', 'กิน', 'อาหาร', 'ไม่', 'มีความสุข', 'แต่', 'ชีวิต', 'ผม', 'xxfld', 'รส', 'เผ็ด']\n",
      "\t['ผม', 'รู้สึก', 'กิน', 'อาหาร', 'ไม่', 'มีความสุข', 'เหมือน', 'ชีวิต', 'ผม', 'xxfld', 'รส', 'เผ็ด']\n",
      "\t['ผม', 'รู้สึก', 'กิน', 'อาหาร', 'ไม่', 'มีความสุข', 'ใน', 'ชีวิต', 'ผม', 'xxfld', 'รส', 'เผ็ด']\n",
      "\n",
      "Masked Input: \n",
      "\t['อาหาร', 'ไม่', 'มีความสุข', 'xxfld', 'ชีวิต', 'ผม', 'xxfld', 'รส', 'เผ็ด', 'ไป', 'xxfld', 'จะ']\n",
      "Labeled Input: \n",
      "\t['อาหาร', 'ไม่', 'มีความสุข', 'xxfld', 'ชีวิต', 'ผม', 'ขาด', 'รส', 'เผ็ด', 'ไป', 'xxfld', 'จะ']\n",
      "Predictions: \n",
      "\t['อาหาร', 'ไม่', 'มีความสุข', 'xxfld', 'ชีวิต', 'ผม', 'กิน', 'รส', 'เผ็ด', 'ไป', 'xxfld', 'จะ']\n",
      "\t['อาหาร', 'ไม่', 'มีความสุข', 'xxfld', 'ชีวิต', 'ผม', 'ขาด', 'รส', 'เผ็ด', 'ไป', 'xxfld', 'จะ']\n",
      "\t['อาหาร', 'ไม่', 'มีความสุข', 'xxfld', 'ชีวิต', 'ผม', 'เสีย', 'รส', 'เผ็ด', 'ไป', 'xxfld', 'จะ']\n",
      "\t['อาหาร', 'ไม่', 'มีความสุข', 'xxfld', 'ชีวิต', 'ผม', 'ตัด', 'รส', 'เผ็ด', 'ไป', 'xxfld', 'จะ']\n",
      "\t['อาหาร', 'ไม่', 'มีความสุข', 'xxfld', 'ชีวิต', 'ผม', 'ใส่', 'รส', 'เผ็ด', 'ไป', 'xxfld', 'จะ']\n",
      "\n",
      "Masked Input: \n",
      "\t['ชีวิต', 'ผม', 'xxfld', 'รส', 'เผ็ด', 'ไป', 'xxfld', 'จะ', 'ขาดใจ', 'เหมือน', 'มัน', 'ทำให้']\n",
      "Labeled Input: \n",
      "\t['ชีวิต', 'ผม', 'xxfld', 'รส', 'เผ็ด', 'ไป', 'เหมือน', 'จะ', 'ขาดใจ', 'เหมือน', 'มัน', 'ทำให้']\n",
      "Predictions: \n",
      "\t['ชีวิต', 'ผม', 'xxfld', 'รส', 'เผ็ด', 'ไป', 'เหมือน', 'จะ', 'ขาดใจ', 'เหมือน', 'มัน', 'ทำให้']\n",
      "\t['ชีวิต', 'ผม', 'xxfld', 'รส', 'เผ็ด', 'ไป', 'ก็', 'จะ', 'ขาดใจ', 'เหมือน', 'มัน', 'ทำให้']\n",
      "\t['ชีวิต', 'ผม', 'xxfld', 'รส', 'เผ็ด', 'ไป', 'มัน', 'จะ', 'ขาดใจ', 'เหมือน', 'มัน', 'ทำให้']\n",
      "\t['ชีวิต', 'ผม', 'xxfld', 'รส', 'เผ็ด', 'ไป', 'จน', 'จะ', 'ขาดใจ', 'เหมือน', 'มัน', 'ทำให้']\n",
      "\t['ชีวิต', 'ผม', 'xxfld', 'รส', 'เผ็ด', 'ไป', 'กลัว', 'จะ', 'ขาดใจ', 'เหมือน', 'มัน', 'ทำให้']\n",
      "\n",
      "Masked Input: \n",
      "\t['มัน', 'ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'xxfld', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง']\n",
      "Labeled Input: \n",
      "\t['มัน', 'ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'นึง', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง']\n",
      "Predictions: \n",
      "\t['มัน', 'ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'นึง', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง']\n",
      "\t['มัน', 'ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'ที่', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง']\n",
      "\t['มัน', 'ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'หนึ่ง', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง']\n",
      "\t['มัน', 'ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'ละ', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง']\n",
      "\t['มัน', 'ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'อื่น', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง']\n",
      "\n",
      "Masked Input: \n",
      "\t['ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'xxfld', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'xxfld']\n",
      "Labeled Input: \n",
      "\t['ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'xxfld', 'เลย', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'xxfld']\n",
      "Predictions: \n",
      "\t['ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'xxfld', 'เลย', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'xxfld']\n",
      "\t['ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'xxfld', 'บ้าง', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'xxfld']\n",
      "\t['ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'xxfld', 'แล้ว', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'xxfld']\n",
      "\t['ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'xxfld', 'จริงๆ', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'xxfld']\n",
      "\t['ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'xxfld', 'ด้วย', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t['xxfld', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'xxfld', 'เรา', 'แต่งงาน', 'กัน', 'แล้ว', 'ผม']\n",
      "Labeled Input: \n",
      "\t['xxfld', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'ถ้า', 'เรา', 'แต่งงาน', 'กัน', 'แล้ว', 'ผม']\n",
      "Predictions: \n",
      "\t['xxfld', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'ถ้า', 'เรา', 'แต่งงาน', 'กัน', 'แล้ว', 'ผม']\n",
      "\t['xxfld', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'เวลา', 'เรา', 'แต่งงาน', 'กัน', 'แล้ว', 'ผม']\n",
      "\t['xxfld', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'ตอนนี้', 'เรา', 'แต่งงาน', 'กัน', 'แล้ว', 'ผม']\n",
      "\t['xxfld', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'ตอนที่', 'เรา', 'แต่งงาน', 'กัน', 'แล้ว', 'ผม']\n",
      "\t['xxfld', 'xxfld', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'เมื่อ', 'เรา', 'แต่งงาน', 'กัน', 'แล้ว', 'ผม']\n",
      "\n",
      "Masked Input: \n",
      "\t[' ', 'พอ', 'ผม', 'เห็น', 'คู่', 'ที่', 'xxfld', 'ทานอาหาร', 'เหมือน', 'ๆ', 'กัน', 'เห็น']\n",
      "Labeled Input: \n",
      "\t[' ', 'พอ', 'ผม', 'เห็น', 'คู่', 'ที่', 'ชอบ', 'ทานอาหาร', 'เหมือน', 'ๆ', 'กัน', 'เห็น']\n",
      "Predictions: \n",
      "\t[' ', 'พอ', 'ผม', 'เห็น', 'คู่', 'ที่', 'เค้า', 'ทานอาหาร', 'เหมือน', 'ๆ', 'กัน', 'เห็น']\n",
      "\t[' ', 'พอ', 'ผม', 'เห็น', 'คู่', 'ที่', 'เรา', 'ทานอาหาร', 'เหมือน', 'ๆ', 'กัน', 'เห็น']\n",
      "\t[' ', 'พอ', 'ผม', 'เห็น', 'คู่', 'ที่', 'ไป', 'ทานอาหาร', 'เหมือน', 'ๆ', 'กัน', 'เห็น']\n",
      "\t[' ', 'พอ', 'ผม', 'เห็น', 'คู่', 'ที่', 'ชอบ', 'ทานอาหาร', 'เหมือน', 'ๆ', 'กัน', 'เห็น']\n",
      "\t[' ', 'พอ', 'ผม', 'เห็น', 'คู่', 'ที่', 'ได้', 'ทานอาหาร', 'เหมือน', 'ๆ', 'กัน', 'เห็น']\n",
      "\n",
      "Masked Input: \n",
      "\t['เลย', ' ', 'มี', 'ใคร', 'เคย', 'มีปัญหา', 'xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld']\n",
      "Labeled Input: \n",
      "\t['เลย', ' ', 'มี', 'ใคร', 'เคย', 'มีปัญหา', 'แบบผม', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld']\n",
      "Predictions: \n",
      "\t['เลย', ' ', 'มี', 'ใคร', 'เคย', 'มีปัญหา', 'แบบผม', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld']\n",
      "\t['เลย', ' ', 'มี', 'ใคร', 'เคย', 'มีปัญหา', 'แบบนี้', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld']\n",
      "\t['เลย', ' ', 'มี', 'ใคร', 'เคย', 'มีปัญหา', 'บ้าง', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld']\n",
      "\t['เลย', ' ', 'มี', 'ใคร', 'เคย', 'มีปัญหา', 'นี้', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld']\n",
      "\t['เลย', ' ', 'มี', 'ใคร', 'เคย', 'มีปัญหา', 'กัน', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t['ใคร', 'เคย', 'มีปัญหา', 'xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'xxfld']\n",
      "Labeled Input: \n",
      "\t['ใคร', 'เคย', 'มีปัญหา', 'xxfld', 'มั้ย', 'ครับ', 'แล้', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'xxfld']\n",
      "Predictions: \n",
      "\t['ใคร', 'เคย', 'มีปัญหา', 'xxfld', 'มั้ย', 'ครับ', 'แล้', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'xxfld']\n",
      "\t['ใคร', 'เคย', 'มีปัญหา', 'xxfld', 'มั้ย', 'ครับ', ' ', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'xxfld']\n",
      "\t['ใคร', 'เคย', 'มีปัญหา', 'xxfld', 'มั้ย', 'ครับ', 'ข่า', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'xxfld']\n",
      "\t['ใคร', 'เคย', 'มีปัญหา', 'xxfld', 'มั้ย', 'ครับ', 'ผิ', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'xxfld']\n",
      "\t['ใคร', 'เคย', 'มีปัญหา', 'xxfld', 'มั้ย', 'ครับ', 'ถึ', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t['มีปัญหา', 'xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'xxfld', 'ครับ', 'xxeos']\n",
      "Labeled Input: \n",
      "\t['มีปัญหา', 'xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'แก้ปัญหา', 'xxfld', 'ยังไง', 'xxfld', 'ครับ', 'xxeos']\n",
      "Predictions: \n",
      "\t['มีปัญหา', 'xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'มี', 'xxfld', 'ยังไง', 'xxfld', 'ครับ', 'xxeos']\n",
      "\t['มีปัญหา', 'xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'แก้ไข', 'xxfld', 'ยังไง', 'xxfld', 'ครับ', 'xxeos']\n",
      "\t['มีปัญหา', 'xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'จัดการ', 'xxfld', 'ยังไง', 'xxfld', 'ครับ', 'xxeos']\n",
      "\t['มีปัญหา', 'xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'ทำ', 'xxfld', 'ยังไง', 'xxfld', 'ครับ', 'xxeos']\n",
      "\t['มีปัญหา', 'xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'แก้', 'xxfld', 'ยังไง', 'xxfld', 'ครับ', 'xxeos']\n",
      "\n",
      "Masked Input: \n",
      "\t['xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'xxfld', 'ครับ', 'xxeos']\n",
      "Labeled Input: \n",
      "\t['xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld', 'นี้', 'ยังไง', 'xxfld', 'ครับ', 'xxeos']\n",
      "Predictions: \n",
      "\t['xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld', 'กัน', 'ยังไง', 'xxfld', 'ครับ', 'xxeos']\n",
      "\t['xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld', 'ความรู้สึก', 'ยังไง', 'xxfld', 'ครับ', 'xxeos']\n",
      "\t['xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld', 'ผม', 'ยังไง', 'xxfld', 'ครับ', 'xxeos']\n",
      "\t['xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld', 'เป็น', 'ยังไง', 'xxfld', 'ครับ', 'xxeos']\n",
      "\t['xxfld', 'มั้ย', 'ครับ', 'xxfld', 'วจะ', 'xxfld', 'รู้สึก', 'ยังไง', 'xxfld', 'ครับ', 'xxeos']\n",
      "\n",
      "Masked Input: \n",
      "\t['ครับ', 'xxfld', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'xxfld', 'ครับ', 'xxeos']\n",
      "Labeled Input: \n",
      "\t['ครับ', 'xxfld', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'ดี', 'ครับ', 'xxeos']\n",
      "Predictions: \n",
      "\t['ครับ', 'xxfld', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'อ่ะ', 'ครับ', 'xxeos']\n",
      "\t['ครับ', 'xxfld', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'บ้าง', 'ครับ', 'xxeos']\n",
      "\t['ครับ', 'xxfld', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'ดี', 'ครับ', 'xxeos']\n",
      "\t['ครับ', 'xxfld', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'มั้ย', 'ครับ', 'xxeos']\n",
      "\t['ครับ', 'xxfld', 'วจะ', 'xxfld', 'xxfld', 'ยังไง', 'หรือ', 'ครับ', 'xxeos']\n",
      "\n",
      "Total Input Tokens: torch.Size([325])\n",
      "Total Correct for Top 5: 31\n",
      "Total Mask: 41\n",
      "Percent Correct: 75.61%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7560975609756098"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"ใครเคยมีแฟนที่กินอาหารไม่ถูกปากกันแล้วรู้สึกเสียความสุขไปอย่างนึงบ้างมั้ยครับ  ก่อนอื่นผมต้องบอกก่อนเลยว่าคนเราจะเลือกกินอาหารแบบไหนชอบแบบไหนเป็นเรื่องของความชอบส่วนตัวนะครับทุกคนมีสิทธิในการเลือกของที่ชอบและไม่ชอบอยู่แล้ว แต่ผมรู้สึกว่าตอนนี้ผมกำลังประสบปัญหาที่ดูเหมือนจะเล็กแต่กลายเป็นว่ามันค่อนข้างใหญ่ ผมคบกับแฟนมา6ปีแล้วครับ ผมเป็นคนชอบกินอาหารญี่ปุ่นและปลาดิบแต่แฟนผมไม่กินปลาดิบเลย ผมอยากกินบุฟเฟ่เนื้อแต่แฟนผมก็ไม่กินเนื้อ เราเลยไม่ได้เข้าทานร้านบุฟเฟ่เนื้อและบุฟเฟ่อาหารญี่ปุ่นกันเพราะรู้สึกลัวแฟนผมทานไม่คุ้ม และเรื่องใหญ่เลยคือผมเป็นคนชอบทานอาหารรสจัดและรสเผ็ดมาก แต่แฟนผมทานเผ็ดไม่ได้เลยเวลาเราไปกินส้มตำกันก็จะสั่ง ส้มตำไม่ใส่พริก ต้มแซ่บไม่ใส่พริก ลาบไม่ใส่พริก ร้านกับข้าวอื่นๆก็เช่นกันแฟนผมจะไม่ชอบกินผักไม่ค่อยสั่งกับข้าวที่เป็นผักแล้วผมชอบผักบุ้งทอดกรอบ เห็ดหอมสดทอดมาก แต่ก็ไม่ได้สั่งเพราะว่าเธอไม่กินถึงเค้าจะบอกให้สั่งเลยๆก็เถอะแต่ผมก็ยังเกรงใจเธออยู่ดีอ่ะครับ ผมรู้สึกกินอาหารไม่มีความสุขเลยชีวิตผมขาดรสเผ็ดไปเหมือนจะขาดใจเหมือนมันทำให้ขาดความสุขไปอย่างนึงเลยอ่ะครับ ยิ่งถ้าเราแต่งงานกันแล้วผมก็อาจจะต้องมีปัญหาเรื่องนี้มากขึ้น พอผมเห็นคู่ที่ชอบทานอาหารเหมือนๆกันเห็นเค้ากินอาหารกันอย่างมีความสุขแล้วผมรู้สึกอิจฉามากๆเลย มีใครเคยมีปัญหาแบบผมมั้ยครับแล้วจะแก้ปัญหานี้ยังไงดีครับ\"\n",
    "tokenized_input, labels = _mask_input(text, tokenizer, mlm_probability=0.1)\n",
    "_predict_masks(tokenized_input, labels, tokenizer , k=5, verbose=True, verbose_length=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tokenize on Pantip Sample อาการแบบนี้คือไรกัน?\n",
    "https://pantip.com/topic/40009518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Input: \n",
      "\t['xxbos', 'อาการ', 'แบบนี้', 'คือ', 'ไร', 'กัน', 'xxfld', ' ', 'เขา', 'คุยกับ', 'เรา', 'มา']\n",
      "Labeled Input: \n",
      "\t['xxbos', 'อาการ', 'แบบนี้', 'คือ', 'ไร', 'กัน', '?', ' ', 'เขา', 'คุยกับ', 'เรา', 'มา']\n",
      "Predictions: \n",
      "\t['xxbos', 'อาการ', 'แบบนี้', 'คือ', 'ไร', 'กัน', 'แน่', ' ', 'เขา', 'คุยกับ', 'เรา', 'มา']\n",
      "\t['xxbos', 'อาการ', 'แบบนี้', 'คือ', 'ไร', 'กัน', '?', ' ', 'เขา', 'คุยกับ', 'เรา', 'มา']\n",
      "\t['xxbos', 'อาการ', 'แบบนี้', 'คือ', 'ไร', 'กัน', 'หรือ', ' ', 'เขา', 'คุยกับ', 'เรา', 'มา']\n",
      "\t['xxbos', 'อาการ', 'แบบนี้', 'คือ', 'ไร', 'กัน', 'เนี่ย', ' ', 'เขา', 'คุยกับ', 'เรา', 'มา']\n",
      "\t['xxbos', 'อาการ', 'แบบนี้', 'คือ', 'ไร', 'กัน', '??', ' ', 'เขา', 'คุยกับ', 'เรา', 'มา']\n",
      "\n",
      "Masked Input: \n",
      "\t['เขา', 'คุยกับ', 'เรา', 'มา', 'xxnum', ' ', 'xxfld', ' ', 'เรา', 'ตามจีบ', 'เขา', 'นะคะ']\n",
      "Labeled Input: \n",
      "\t['เขา', 'คุยกับ', 'เรา', 'มา', 'xxnum', ' ', 'เดือน', ' ', 'เรา', 'ตามจีบ', 'เขา', 'นะคะ']\n",
      "Predictions: \n",
      "\t['เขา', 'คุยกับ', 'เรา', 'มา', 'xxnum', ' ', 'เดือน', ' ', 'เรา', 'ตามจีบ', 'เขา', 'นะคะ']\n",
      "\t['เขา', 'คุยกับ', 'เรา', 'มา', 'xxnum', ' ', 'วัน', ' ', 'เรา', 'ตามจีบ', 'เขา', 'นะคะ']\n",
      "\t['เขา', 'คุยกับ', 'เรา', 'มา', 'xxnum', ' ', 'ปี', ' ', 'เรา', 'ตามจีบ', 'เขา', 'นะคะ']\n",
      "\t['เขา', 'คุยกับ', 'เรา', 'มา', 'xxnum', ' ', 'ปีก', ' ', 'เรา', 'ตามจีบ', 'เขา', 'นะคะ']\n",
      "\t['เขา', 'คุยกับ', 'เรา', 'มา', 'xxnum', ' ', 'ชั่วโมง', ' ', 'เรา', 'ตามจีบ', 'เขา', 'นะคะ']\n",
      "\n",
      "Masked Input: \n",
      "\t['นน', ' ', 'เพื่อ', 'เขา', 'ได้', ' ', 'xxfld', 'จะ', 'ยอม', 'เป็น', 'แฟน', 'เรา']\n",
      "Labeled Input: \n",
      "\t['นน', ' ', 'เพื่อ', 'เขา', 'ได้', ' ', 'เขา', 'จะ', 'ยอม', 'เป็น', 'แฟน', 'เรา']\n",
      "Predictions: \n",
      "\t['นน', ' ', 'เพื่อ', 'เขา', 'ได้', ' ', 'เขา', 'จะ', 'ยอม', 'เป็น', 'แฟน', 'เรา']\n",
      "\t['นน', ' ', 'เพื่อ', 'เขา', 'ได้', ' ', 'เรา', 'จะ', 'ยอม', 'เป็น', 'แฟน', 'เรา']\n",
      "\t['นน', ' ', 'เพื่อ', 'เขา', 'ได้', ' ', 'ก็', 'จะ', 'ยอม', 'เป็น', 'แฟน', 'เรา']\n",
      "\t['นน', ' ', 'เพื่อ', 'เขา', 'ได้', ' ', 'เพื่อน', 'จะ', 'ยอม', 'เป็น', 'แฟน', 'เรา']\n",
      "\t['นน', ' ', 'เพื่อ', 'เขา', 'ได้', ' ', 'ถึง', 'จะ', 'ยอม', 'เป็น', 'แฟน', 'เรา']\n",
      "\n",
      "Masked Input: \n",
      "\t['ค่ะ', 'พอ', 'ไป', 'รู้', 'ว่า', 'เขา', 'xxfld', 'เพื่อน', ' ', 'เพื่อน', 'เขา', 'ถาม']\n",
      "Labeled Input: \n",
      "\t['ค่ะ', 'พอ', 'ไป', 'รู้', 'ว่า', 'เขา', 'คุยกับ', 'เพื่อน', ' ', 'เพื่อน', 'เขา', 'ถาม']\n",
      "Predictions: \n",
      "\t['ค่ะ', 'พอ', 'ไป', 'รู้', 'ว่า', 'เขา', 'มี', 'เพื่อน', ' ', 'เพื่อน', 'เขา', 'ถาม']\n",
      "\t['ค่ะ', 'พอ', 'ไป', 'รู้', 'ว่า', 'เขา', 'ชอบ', 'เพื่อน', ' ', 'เพื่อน', 'เขา', 'ถาม']\n",
      "\t['ค่ะ', 'พอ', 'ไป', 'รู้', 'ว่า', 'เขา', 'คุยกับ', 'เพื่อน', ' ', 'เพื่อน', 'เขา', 'ถาม']\n",
      "\t['ค่ะ', 'พอ', 'ไป', 'รู้', 'ว่า', 'เขา', 'รัก', 'เพื่อน', ' ', 'เพื่อน', 'เขา', 'ถาม']\n",
      "\t['ค่ะ', 'พอ', 'ไป', 'รู้', 'ว่า', 'เขา', 'เป็น', 'เพื่อน', ' ', 'เพื่อน', 'เขา', 'ถาม']\n",
      "\n",
      "Masked Input: \n",
      "\t['เขา', 'ตอบ', 'เพื่อน', 'ว่า', 'เขา', 'ว่า', 'xxfld', 'ควร', 'อยู่', 'คนเดียว', ' ', 'ยัง']\n",
      "Labeled Input: \n",
      "\t['เขา', 'ตอบ', 'เพื่อน', 'ว่า', 'เขา', 'ว่า', 'เขา', 'ควร', 'อยู่', 'คนเดียว', ' ', 'ยัง']\n",
      "Predictions: \n",
      "\t['เขา', 'ตอบ', 'เพื่อน', 'ว่า', 'เขา', 'ว่า', 'เรา', 'ควร', 'อยู่', 'คนเดียว', ' ', 'ยัง']\n",
      "\t['เขา', 'ตอบ', 'เพื่อน', 'ว่า', 'เขา', 'ว่า', 'ไม่', 'ควร', 'อยู่', 'คนเดียว', ' ', 'ยัง']\n",
      "\t['เขา', 'ตอบ', 'เพื่อน', 'ว่า', 'เขา', 'ว่า', 'เขา', 'ควร', 'อยู่', 'คนเดียว', ' ', 'ยัง']\n",
      "\t['เขา', 'ตอบ', 'เพื่อน', 'ว่า', 'เขา', 'ว่า', 'ทำไม', 'ควร', 'อยู่', 'คนเดียว', ' ', 'ยัง']\n",
      "\t['เขา', 'ตอบ', 'เพื่อน', 'ว่า', 'เขา', 'ว่า', 'ยัง', 'ควร', 'อยู่', 'คนเดียว', ' ', 'ยัง']\n",
      "\n",
      "Masked Input: \n",
      "\t['อยู่', 'คนเดียว', ' ', 'ยัง', 'ไม่', 'พร้อม', 'xxfld', 'รัก', 'ใคร', ' ', 'จน', 'xxfld']\n",
      "Labeled Input: \n",
      "\t['อยู่', 'คนเดียว', ' ', 'ยัง', 'ไม่', 'พร้อม', 'จะ', 'รัก', 'ใคร', ' ', 'จน', 'xxfld']\n",
      "Predictions: \n",
      "\t['อยู่', 'คนเดียว', ' ', 'ยัง', 'ไม่', 'พร้อม', 'จะ', 'รัก', 'ใคร', ' ', 'จน', 'xxfld']\n",
      "\t['อยู่', 'คนเดียว', ' ', 'ยัง', 'ไม่', 'พร้อม', 'ที่จะ', 'รัก', 'ใคร', ' ', 'จน', 'xxfld']\n",
      "\t['อยู่', 'คนเดียว', ' ', 'ยัง', 'ไม่', 'พร้อม', 'ว่าจะ', 'รัก', 'ใคร', ' ', 'จน', 'xxfld']\n",
      "\t['อยู่', 'คนเดียว', ' ', 'ยัง', 'ไม่', 'พร้อม', ' ', 'รัก', 'ใคร', ' ', 'จน', 'xxfld']\n",
      "\t['อยู่', 'คนเดียว', ' ', 'ยัง', 'ไม่', 'พร้อม', 'ไป', 'รัก', 'ใคร', ' ', 'จน', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t['พร้อม', 'xxfld', 'รัก', 'ใคร', ' ', 'จน', 'xxfld', 'เรา', 'เริ่ม', 'รู้สึก', 'ว่า', 'ทำไม']\n",
      "Labeled Input: \n",
      "\t['พร้อม', 'xxfld', 'รัก', 'ใคร', ' ', 'จน', 'ตอนนี้', 'เรา', 'เริ่ม', 'รู้สึก', 'ว่า', 'ทำไม']\n",
      "Predictions: \n",
      "\t['พร้อม', 'xxfld', 'รัก', 'ใคร', ' ', 'จน', 'ตอนนี้', 'เรา', 'เริ่ม', 'รู้สึก', 'ว่า', 'ทำไม']\n",
      "\t['พร้อม', 'xxfld', 'รัก', 'ใคร', ' ', 'จน', 'วันนี้', 'เรา', 'เริ่ม', 'รู้สึก', 'ว่า', 'ทำไม']\n",
      "\t['พร้อม', 'xxfld', 'รัก', 'ใคร', ' ', 'จน', 'ทำให้', 'เรา', 'เริ่ม', 'รู้สึก', 'ว่า', 'ทำไม']\n",
      "\t['พร้อม', 'xxfld', 'รัก', 'ใคร', ' ', 'จน', 'วันหนึ่ง', 'เรา', 'เริ่ม', 'รู้สึก', 'ว่า', 'ทำไม']\n",
      "\t['พร้อม', 'xxfld', 'รัก', 'ใคร', ' ', 'จน', 'ตอนนั้น', 'เรา', 'เริ่ม', 'รู้สึก', 'ว่า', 'ทำไม']\n",
      "\n",
      "Masked Input: \n",
      "\t['ทัก', 'ไป', 'ครึ่ง', 'วัน', ' ', 'ปกติ', 'xxfld', 'จะ', 'มี', 'การ', 'มอ', 'นิ่ง']\n",
      "Labeled Input: \n",
      "\t['ทัก', 'ไป', 'ครึ่ง', 'วัน', ' ', 'ปกติ', 'เรา', 'จะ', 'มี', 'การ', 'มอ', 'นิ่ง']\n",
      "Predictions: \n",
      "\t['ทัก', 'ไป', 'ครึ่ง', 'วัน', ' ', 'ปกติ', 'เรา', 'จะ', 'มี', 'การ', 'มอ', 'นิ่ง']\n",
      "\t['ทัก', 'ไป', 'ครึ่ง', 'วัน', ' ', 'ปกติ', 'เขา', 'จะ', 'มี', 'การ', 'มอ', 'นิ่ง']\n",
      "\t['ทัก', 'ไป', 'ครึ่ง', 'วัน', ' ', 'ปกติ', 'ก็', 'จะ', 'มี', 'การ', 'มอ', 'นิ่ง']\n",
      "\t['ทัก', 'ไป', 'ครึ่ง', 'วัน', ' ', 'ปกติ', 'มัน', 'จะ', 'มี', 'การ', 'มอ', 'นิ่ง']\n",
      "\t['ทัก', 'ไป', 'ครึ่ง', 'วัน', ' ', 'ปกติ', 'ๆ', 'จะ', 'มี', 'การ', 'มอ', 'นิ่ง']\n",
      "\n",
      "Masked Input: \n",
      "\t['จะ', 'มี', 'การ', 'มอ', 'นิ่ง', 'กัน', 'xxfld', 'ค่ะ', 'พอ', 'เรา', 'ไม่', 'xxfld']\n",
      "Labeled Input: \n",
      "\t['จะ', 'มี', 'การ', 'มอ', 'นิ่ง', 'กัน', 'ตอนเช้า', 'ค่ะ', 'พอ', 'เรา', 'ไม่', 'xxfld']\n",
      "Predictions: \n",
      "\t['จะ', 'มี', 'การ', 'มอ', 'นิ่ง', 'กัน', 'ทุกวัน', 'ค่ะ', 'พอ', 'เรา', 'ไม่', 'xxfld']\n",
      "\t['จะ', 'มี', 'การ', 'มอ', 'นิ่ง', 'กัน', 'ตลอด', 'ค่ะ', 'พอ', 'เรา', 'ไม่', 'xxfld']\n",
      "\t['จะ', 'มี', 'การ', 'มอ', 'นิ่ง', 'กัน', 'ทั้งวัน', 'ค่ะ', 'พอ', 'เรา', 'ไม่', 'xxfld']\n",
      "\t['จะ', 'มี', 'การ', 'มอ', 'นิ่ง', 'กัน', 'บ้าง', 'ค่ะ', 'พอ', 'เรา', 'ไม่', 'xxfld']\n",
      "\t['จะ', 'มี', 'การ', 'มอ', 'นิ่ง', 'กัน', 'เลย', 'ค่ะ', 'พอ', 'เรา', 'ไม่', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t['กัน', 'xxfld', 'ค่ะ', 'พอ', 'เรา', 'ไม่', 'xxfld', 'ไป', ' ', 'เขา', 'ทำงาน', 'xxfld']\n",
      "Labeled Input: \n",
      "\t['กัน', 'xxfld', 'ค่ะ', 'พอ', 'เรา', 'ไม่', 'ทัก', 'ไป', ' ', 'เขา', 'ทำงาน', 'xxfld']\n",
      "Predictions: \n",
      "\t['กัน', 'xxfld', 'ค่ะ', 'พอ', 'เรา', 'ไม่', 'ทัก', 'ไป', ' ', 'เขา', 'ทำงาน', 'xxfld']\n",
      "\t['กัน', 'xxfld', 'ค่ะ', 'พอ', 'เรา', 'ไม่', 'โทร', 'ไป', ' ', 'เขา', 'ทำงาน', 'xxfld']\n",
      "\t['กัน', 'xxfld', 'ค่ะ', 'พอ', 'เรา', 'ไม่', 'ตอบ', 'ไป', ' ', 'เขา', 'ทำงาน', 'xxfld']\n",
      "\t['กัน', 'xxfld', 'ค่ะ', 'พอ', 'เรา', 'ไม่', 'อยาก', 'ไป', ' ', 'เขา', 'ทำงาน', 'xxfld']\n",
      "\t['กัน', 'xxfld', 'ค่ะ', 'พอ', 'เรา', 'ไม่', 'ไลน์', 'ไป', ' ', 'เขา', 'ทำงาน', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t['ไม่', 'xxfld', 'ไป', ' ', 'เขา', 'ทำงาน', 'xxfld', ' ', 'ถึง', 'เวลา', 'พัก', 'ของเขา']\n",
      "Labeled Input: \n",
      "\t['ไม่', 'xxfld', 'ไป', ' ', 'เขา', 'ทำงาน', 'เสร็จ', ' ', 'ถึง', 'เวลา', 'พัก', 'ของเขา']\n",
      "Predictions: \n",
      "\t['ไม่', 'xxfld', 'ไป', ' ', 'เขา', 'ทำงาน', 'กลางคืน', ' ', 'ถึง', 'เวลา', 'พัก', 'ของเขา']\n",
      "\t['ไม่', 'xxfld', 'ไป', ' ', 'เขา', 'ทำงาน', 'เสร็จ', ' ', 'ถึง', 'เวลา', 'พัก', 'ของเขา']\n",
      "\t['ไม่', 'xxfld', 'ไป', ' ', 'เขา', 'ทำงาน', 'ดึก', ' ', 'ถึง', 'เวลา', 'พัก', 'ของเขา']\n",
      "\t['ไม่', 'xxfld', 'ไป', ' ', 'เขา', 'ทำงาน', 'ต่างจังหวัด', ' ', 'ถึง', 'เวลา', 'พัก', 'ของเขา']\n",
      "\t['ไม่', 'xxfld', 'ไป', ' ', 'เขา', 'ทำงาน', 'อยู่', ' ', 'ถึง', 'เวลา', 'พัก', 'ของเขา']\n",
      "\n",
      "Masked Input: \n",
      "\t['เขา', 'ก็', 'ทัก', 'มา', 'ว่า', 'กินข้าว', 'xxfld', ' ', 'เรา', 'ก็', 'ยิ่ง', ' ']\n",
      "Labeled Input: \n",
      "\t['เขา', 'ก็', 'ทัก', 'มา', 'ว่า', 'กินข้าว', 'กัน', ' ', 'เรา', 'ก็', 'ยิ่ง', ' ']\n",
      "Predictions: \n",
      "\t['เขา', 'ก็', 'ทัก', 'มา', 'ว่า', 'กินข้าว', 'ยัง', ' ', 'เรา', 'ก็', 'ยิ่ง', ' ']\n",
      "\t['เขา', 'ก็', 'ทัก', 'มา', 'ว่า', 'กินข้าว', 'หรือ', ' ', 'เรา', 'ก็', 'ยิ่ง', ' ']\n",
      "\t['เขา', 'ก็', 'ทัก', 'มา', 'ว่า', 'กินข้าว', 'มั้ย', ' ', 'เรา', 'ก็', 'ยิ่ง', ' ']\n",
      "\t['เขา', 'ก็', 'ทัก', 'มา', 'ว่า', 'กินข้าว', 'หรือยัง', ' ', 'เรา', 'ก็', 'ยิ่ง', ' ']\n",
      "\t['เขา', 'ก็', 'ทัก', 'มา', 'ว่า', 'กินข้าว', 'กัน', ' ', 'เรา', 'ก็', 'ยิ่ง', ' ']\n",
      "\n",
      "Masked Input: \n",
      "\t['ก็', 'เลย', 'เล่า', 'ให้', 'เขา', 'ฟัง', 'xxfld', 'ให้', 'ไลน์', 'ไป', ' ', 'ให้']\n",
      "Labeled Input: \n",
      "\t['ก็', 'เลย', 'เล่า', 'ให้', 'เขา', 'ฟัง', 'ว่า', 'ให้', 'ไลน์', 'ไป', ' ', 'ให้']\n",
      "Predictions: \n",
      "\t['ก็', 'เลย', 'เล่า', 'ให้', 'เขา', 'ฟัง', 'ว่า', 'ให้', 'ไลน์', 'ไป', ' ', 'ให้']\n",
      "\t['ก็', 'เลย', 'เล่า', 'ให้', 'เขา', 'ฟัง', ' ', 'ให้', 'ไลน์', 'ไป', ' ', 'ให้']\n",
      "\t['ก็', 'เลย', 'เล่า', 'ให้', 'เขา', 'ฟัง', 'แล้ว', 'ให้', 'ไลน์', 'ไป', ' ', 'ให้']\n",
      "\t['ก็', 'เลย', 'เล่า', 'ให้', 'เขา', 'ฟัง', 'เขา', 'ให้', 'ไลน์', 'ไป', ' ', 'ให้']\n",
      "\t['ก็', 'เลย', 'เล่า', 'ให้', 'เขา', 'ฟัง', 'ว่าจะ', 'ให้', 'ไลน์', 'ไป', ' ', 'ให้']\n",
      "\n",
      "Masked Input: \n",
      "\t['ไลน์', 'ไป', ' ', 'ให้', 'เขา', 'ไป', 'xxfld', 'อยู่', 'แต่', 'xxunk', 'นั่งรถ', 'xxfld']\n",
      "Labeled Input: \n",
      "\t['ไลน์', 'ไป', ' ', 'ให้', 'เขา', 'ไป', 'ส่ง', 'อยู่', 'แต่', 'xxunk', 'นั่งรถ', 'xxfld']\n",
      "Predictions: \n",
      "\t['ไลน์', 'ไป', ' ', 'ให้', 'เขา', 'ไป', 'ส่ง', 'อยู่', 'แต่', 'xxunk', 'นั่งรถ', 'xxfld']\n",
      "\t['ไลน์', 'ไป', ' ', 'ให้', 'เขา', 'ไป', 'รับ', 'อยู่', 'แต่', 'xxunk', 'นั่งรถ', 'xxfld']\n",
      "\t['ไลน์', 'ไป', ' ', 'ให้', 'เขา', 'ไป', 'ทำงาน', 'อยู่', 'แต่', 'xxunk', 'นั่งรถ', 'xxfld']\n",
      "\t['ไลน์', 'ไป', ' ', 'ให้', 'เขา', 'ไป', 'เรียน', 'อยู่', 'แต่', 'xxunk', 'นั่งรถ', 'xxfld']\n",
      "\t['ไลน์', 'ไป', ' ', 'ให้', 'เขา', 'ไป', 'ไหน', 'อยู่', 'แต่', 'xxunk', 'นั่งรถ', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t['ไป', 'xxfld', 'อยู่', 'แต่', 'xxunk', 'นั่งรถ', 'xxfld', 'เดียวกัน', ' ', 'เรา', 'ก็', 'ขับ']\n",
      "Labeled Input: \n",
      "\t['ไป', 'xxfld', 'อยู่', 'แต่', 'xxunk', 'นั่งรถ', 'คัน', 'เดียวกัน', ' ', 'เรา', 'ก็', 'ขับ']\n",
      "Predictions: \n",
      "\t['ไป', 'xxfld', 'อยู่', 'แต่', 'xxunk', 'นั่งรถ', 'คัน', 'เดียวกัน', ' ', 'เรา', 'ก็', 'ขับ']\n",
      "\t['ไป', 'xxfld', 'อยู่', 'แต่', 'xxunk', 'นั่งรถ', 'สาย', 'เดียวกัน', ' ', 'เรา', 'ก็', 'ขับ']\n",
      "\t['ไป', 'xxfld', 'อยู่', 'แต่', 'xxunk', 'นั่งรถ', 'ทาง', 'เดียวกัน', ' ', 'เรา', 'ก็', 'ขับ']\n",
      "\t['ไป', 'xxfld', 'อยู่', 'แต่', 'xxunk', 'นั่งรถ', 'ที่', 'เดียวกัน', ' ', 'เรา', 'ก็', 'ขับ']\n",
      "\t['ไป', 'xxfld', 'อยู่', 'แต่', 'xxunk', 'นั่งรถ', 'มอ', 'เดียวกัน', ' ', 'เรา', 'ก็', 'ขับ']\n",
      "\n",
      "Masked Input: \n",
      "\t['ก็', 'ขับ', 'ของเรา', 'คน', 'ที่มา', 'จีบ', 'xxfld', 'เขา', 'xxfld', 'ขับ', 'xxfld', 'ของเขา']\n",
      "Labeled Input: \n",
      "\t['ก็', 'ขับ', 'ของเรา', 'คน', 'ที่มา', 'จีบ', 'เรา', 'เขา', 'xxfld', 'ขับ', 'xxfld', 'ของเขา']\n",
      "Predictions: \n",
      "\t['ก็', 'ขับ', 'ของเรา', 'คน', 'ที่มา', 'จีบ', ' ', 'เขา', 'xxfld', 'ขับ', 'xxfld', 'ของเขา']\n",
      "\t['ก็', 'ขับ', 'ของเรา', 'คน', 'ที่มา', 'จีบ', 'เรา', 'เขา', 'xxfld', 'ขับ', 'xxfld', 'ของเขา']\n",
      "\t['ก็', 'ขับ', 'ของเรา', 'คน', 'ที่มา', 'จีบ', 'เขา', 'เขา', 'xxfld', 'ขับ', 'xxfld', 'ของเขา']\n",
      "\t['ก็', 'ขับ', 'ของเรา', 'คน', 'ที่มา', 'จีบ', 'ค่ะ', 'เขา', 'xxfld', 'ขับ', 'xxfld', 'ของเขา']\n",
      "\t['ก็', 'ขับ', 'ของเรา', 'คน', 'ที่มา', 'จีบ', 'ของเรา', 'เขา', 'xxfld', 'ขับ', 'xxfld', 'ของเขา']\n",
      "\n",
      "Masked Input: \n",
      "\t['ของเรา', 'คน', 'ที่มา', 'จีบ', 'xxfld', 'เขา', 'xxfld', 'ขับ', 'xxfld', 'ของเขา', 'แค่', 'มา']\n",
      "Labeled Input: \n",
      "\t['ของเรา', 'คน', 'ที่มา', 'จีบ', 'xxfld', 'เขา', 'ก็', 'ขับ', 'xxfld', 'ของเขา', 'แค่', 'มา']\n",
      "Predictions: \n",
      "\t['ของเรา', 'คน', 'ที่มา', 'จีบ', 'xxfld', 'เขา', 'ก็', 'ขับ', 'xxfld', 'ของเขา', 'แค่', 'มา']\n",
      "\t['ของเรา', 'คน', 'ที่มา', 'จีบ', 'xxfld', 'เขา', 'จะ', 'ขับ', 'xxfld', 'ของเขา', 'แค่', 'มา']\n",
      "\t['ของเรา', 'คน', 'ที่มา', 'จีบ', 'xxfld', 'เขา', 'ไม่ได้', 'ขับ', 'xxfld', 'ของเขา', 'แค่', 'มา']\n",
      "\t['ของเรา', 'คน', 'ที่มา', 'จีบ', 'xxfld', 'เขา', 'บอก', 'ขับ', 'xxfld', 'ของเขา', 'แค่', 'มา']\n",
      "\t['ของเรา', 'คน', 'ที่มา', 'จีบ', 'xxfld', 'เขา', 'ไม่', 'ขับ', 'xxfld', 'ของเขา', 'แค่', 'มา']\n",
      "\n",
      "Masked Input: \n",
      "\t['ที่มา', 'จีบ', 'xxfld', 'เขา', 'xxfld', 'ขับ', 'xxfld', 'ของเขา', 'แค่', 'มา', 'ส่ง', 'เฉยๆ']\n",
      "Labeled Input: \n",
      "\t['ที่มา', 'จีบ', 'xxfld', 'เขา', 'xxfld', 'ขับ', 'คัน', 'ของเขา', 'แค่', 'มา', 'ส่ง', 'เฉยๆ']\n",
      "Predictions: \n",
      "\t['ที่มา', 'จีบ', 'xxfld', 'เขา', 'xxfld', 'ขับ', 'รถ', 'ของเขา', 'แค่', 'มา', 'ส่ง', 'เฉยๆ']\n",
      "\t['ที่มา', 'จีบ', 'xxfld', 'เขา', 'xxfld', 'ขับ', 'มอเตอร์ไซค์', 'ของเขา', 'แค่', 'มา', 'ส่ง', 'เฉยๆ']\n",
      "\t['ที่มา', 'จีบ', 'xxfld', 'เขา', 'xxfld', 'ขับ', 'รถยนต์', 'ของเขา', 'แค่', 'มา', 'ส่ง', 'เฉยๆ']\n",
      "\t['ที่มา', 'จีบ', 'xxfld', 'เขา', 'xxfld', 'ขับ', 'วิน', 'ของเขา', 'แค่', 'มา', 'ส่ง', 'เฉยๆ']\n",
      "\t['ที่มา', 'จีบ', 'xxfld', 'เขา', 'xxfld', 'ขับ', 'รถเก๋ง', 'ของเขา', 'แค่', 'มา', 'ส่ง', 'เฉยๆ']\n",
      "\n",
      "Masked Input: \n",
      "\t['เช้า', 'มา', 'เขา', 'ทัก', 'มา', 'มอ', 'xxfld', ' ', 'ก็', 'ถาม', 'เรา', 'เรื่อง']\n",
      "Labeled Input: \n",
      "\t['เช้า', 'มา', 'เขา', 'ทัก', 'มา', 'มอ', 'นิ่ง', ' ', 'ก็', 'ถาม', 'เรา', 'เรื่อง']\n",
      "Predictions: \n",
      "\t['เช้า', 'มา', 'เขา', 'ทัก', 'มา', 'มอ', 'นิ่ง', ' ', 'ก็', 'ถาม', 'เรา', 'เรื่อง']\n",
      "\t['เช้า', 'มา', 'เขา', 'ทัก', 'มา', 'มอ', 'เงียบ', ' ', 'ก็', 'ถาม', 'เรา', 'เรื่อง']\n",
      "\t['เช้า', 'มา', 'เขา', 'ทัก', 'มา', 'มอ', 'ว่าง', ' ', 'ก็', 'ถาม', 'เรา', 'เรื่อง']\n",
      "\t['เช้า', 'มา', 'เขา', 'ทัก', 'มา', 'มอ', 'กว้าง', ' ', 'ก็', 'ถาม', 'เรา', 'เรื่อง']\n",
      "\t['เช้า', 'มา', 'เขา', 'ทัก', 'มา', 'มอ', 'ดึก', ' ', 'ก็', 'ถาม', 'เรา', 'เรื่อง']\n",
      "\n",
      "Masked Input: \n",
      "\t[' ', 'เรา', 'ทำงาน', 'ที่', 'กลับ', 'ดึก', 'xxfld', 'ๆ', 'ไม่ได้', 'ทัก', 'ไป', 'บอก']\n",
      "Labeled Input: \n",
      "\t[' ', 'เรา', 'ทำงาน', 'ที่', 'กลับ', 'ดึก', 'มาก', 'ๆ', 'ไม่ได้', 'ทัก', 'ไป', 'บอก']\n",
      "Predictions: \n",
      "\t[' ', 'เรา', 'ทำงาน', 'ที่', 'กลับ', 'ดึก', 'มาก', 'ๆ', 'ไม่ได้', 'ทัก', 'ไป', 'บอก']\n",
      "\t[' ', 'เรา', 'ทำงาน', 'ที่', 'กลับ', 'ดึก', 'กว่า', 'ๆ', 'ไม่ได้', 'ทัก', 'ไป', 'บอก']\n",
      "\t[' ', 'เรา', 'ทำงาน', 'ที่', 'กลับ', 'ดึก', 'สุด', 'ๆ', 'ไม่ได้', 'ทัก', 'ไป', 'บอก']\n",
      "\t[' ', 'เรา', 'ทำงาน', 'ที่', 'กลับ', 'ดึก', 'จัง', 'ๆ', 'ไม่ได้', 'ทัก', 'ไป', 'บอก']\n",
      "\t[' ', 'เรา', 'ทำงาน', 'ที่', 'กลับ', 'ดึก', ' ', 'ๆ', 'ไม่ได้', 'ทัก', 'ไป', 'บอก']\n",
      "\n",
      "Masked Input: \n",
      "\t['แล้ว', 'นะ', 'เงียบ', 'ไป', 'ทั้งคืน', 'เลย', 'xxfld', 'เขา', 'ก็', 'ถาม', 'เรื่อง', 'เมื่อคืน']\n",
      "Labeled Input: \n",
      "\t['แล้ว', 'นะ', 'เงียบ', 'ไป', 'ทั้งคืน', 'เลย', ' ', 'เขา', 'ก็', 'ถาม', 'เรื่อง', 'เมื่อคืน']\n",
      "Predictions: \n",
      "\t['แล้ว', 'นะ', 'เงียบ', 'ไป', 'ทั้งคืน', 'เลย', ' ', 'เขา', 'ก็', 'ถาม', 'เรื่อง', 'เมื่อคืน']\n",
      "\t['แล้ว', 'นะ', 'เงียบ', 'ไป', 'ทั้งคืน', 'เลย', 'ค่ะ', 'เขา', 'ก็', 'ถาม', 'เรื่อง', 'เมื่อคืน']\n",
      "\t['แล้ว', 'นะ', 'เงียบ', 'ไป', 'ทั้งคืน', 'เลย', 'นะคะ', 'เขา', 'ก็', 'ถาม', 'เรื่อง', 'เมื่อคืน']\n",
      "\t['แล้ว', 'นะ', 'เงียบ', 'ไป', 'ทั้งคืน', 'เลย', 'นะ', 'เขา', 'ก็', 'ถาม', 'เรื่อง', 'เมื่อคืน']\n",
      "\t['แล้ว', 'นะ', 'เงียบ', 'ไป', 'ทั้งคืน', 'เลย', 'คะ', 'เขา', 'ก็', 'ถาม', 'เรื่อง', 'เมื่อคืน']\n",
      "\n",
      "Masked Input: \n",
      "\t['ไป', 'ส่ง', 'ที่บ้าน', 'เป็นไง', 'บ้าง', ' ', 'xxfld', 'แต่', 'เรื่อง', 'ของ', 'ผู้ชาย', 'xxfld']\n",
      "Labeled Input: \n",
      "\t['ไป', 'ส่ง', 'ที่บ้าน', 'เป็นไง', 'บ้าง', ' ', 'ถาม', 'แต่', 'เรื่อง', 'ของ', 'ผู้ชาย', 'xxfld']\n",
      "Predictions: \n",
      "\t['ไป', 'ส่ง', 'ที่บ้าน', 'เป็นไง', 'บ้าง', ' ', 'มี', 'แต่', 'เรื่อง', 'ของ', 'ผู้ชาย', 'xxfld']\n",
      "\t['ไป', 'ส่ง', 'ที่บ้าน', 'เป็นไง', 'บ้าง', ' ', 'ถาม', 'แต่', 'เรื่อง', 'ของ', 'ผู้ชาย', 'xxfld']\n",
      "\t['ไป', 'ส่ง', 'ที่บ้าน', 'เป็นไง', 'บ้าง', ' ', 'เจอ', 'แต่', 'เรื่อง', 'ของ', 'ผู้ชาย', 'xxfld']\n",
      "\t['ไป', 'ส่ง', 'ที่บ้าน', 'เป็นไง', 'บ้าง', ' ', 'รู้', 'แต่', 'เรื่อง', 'ของ', 'ผู้ชาย', 'xxfld']\n",
      "\t['ไป', 'ส่ง', 'ที่บ้าน', 'เป็นไง', 'บ้าง', ' ', 'เหลือ', 'แต่', 'เรื่อง', 'ของ', 'ผู้ชาย', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t[' ', 'xxfld', 'แต่', 'เรื่อง', 'ของ', 'ผู้ชาย', 'xxfld', 'จีบ', 'เรา', 'ทั้งวัน', ' ', 'จน']\n",
      "Labeled Input: \n",
      "\t[' ', 'xxfld', 'แต่', 'เรื่อง', 'ของ', 'ผู้ชาย', 'ที่มา', 'จีบ', 'เรา', 'ทั้งวัน', ' ', 'จน']\n",
      "Predictions: \n",
      "\t[' ', 'xxfld', 'แต่', 'เรื่อง', 'ของ', 'ผู้ชาย', 'ที่', 'จีบ', 'เรา', 'ทั้งวัน', ' ', 'จน']\n",
      "\t[' ', 'xxfld', 'แต่', 'เรื่อง', 'ของ', 'ผู้ชาย', 'ที่มา', 'จีบ', 'เรา', 'ทั้งวัน', ' ', 'จน']\n",
      "\t[' ', 'xxfld', 'แต่', 'เรื่อง', 'ของ', 'ผู้ชาย', 'มา', 'จีบ', 'เรา', 'ทั้งวัน', ' ', 'จน']\n",
      "\t[' ', 'xxfld', 'แต่', 'เรื่อง', 'ของ', 'ผู้ชาย', ' ', 'จีบ', 'เรา', 'ทั้งวัน', ' ', 'จน']\n",
      "\t[' ', 'xxfld', 'แต่', 'เรื่อง', 'ของ', 'ผู้ชาย', 'เขา', 'จีบ', 'เรา', 'ทั้งวัน', ' ', 'จน']\n",
      "\n",
      "Masked Input: \n",
      "\t['ทั้งวัน', ' ', 'จน', 'เรา', 'เปลี่ยน', 'เรื่อง', 'xxfld', 'ยัง', 'กลับมา', 'ถาม', 'xxfld', 'กรอบ']\n",
      "Labeled Input: \n",
      "\t['ทั้งวัน', ' ', 'จน', 'เรา', 'เปลี่ยน', 'เรื่อง', 'ก็', 'ยัง', 'กลับมา', 'ถาม', 'xxfld', 'กรอบ']\n",
      "Predictions: \n",
      "\t['ทั้งวัน', ' ', 'จน', 'เรา', 'เปลี่ยน', 'เรื่อง', 'เขา', 'ยัง', 'กลับมา', 'ถาม', 'xxfld', 'กรอบ']\n",
      "\t['ทั้งวัน', ' ', 'จน', 'เรา', 'เปลี่ยน', 'เรื่อง', 'ก็', 'ยัง', 'กลับมา', 'ถาม', 'xxfld', 'กรอบ']\n",
      "\t['ทั้งวัน', ' ', 'จน', 'เรา', 'เปลี่ยน', 'เรื่อง', ' ', 'ยัง', 'กลับมา', 'ถาม', 'xxfld', 'กรอบ']\n",
      "\t['ทั้งวัน', ' ', 'จน', 'เรา', 'เปลี่ยน', 'เรื่อง', 'แล้ว', 'ยัง', 'กลับมา', 'ถาม', 'xxfld', 'กรอบ']\n",
      "\t['ทั้งวัน', ' ', 'จน', 'เรา', 'เปลี่ยน', 'เรื่อง', 'แล้วก็', 'ยัง', 'กลับมา', 'ถาม', 'xxfld', 'กรอบ']\n",
      "\n",
      "Masked Input: \n",
      "\t['เปลี่ยน', 'เรื่อง', 'xxfld', 'ยัง', 'กลับมา', 'ถาม', 'xxfld', 'กรอบ', ' ', 'xxfld', 'อาการ', 'แบบนี้']\n",
      "Labeled Input: \n",
      "\t['เปลี่ยน', 'เรื่อง', 'xxfld', 'ยัง', 'กลับมา', 'ถาม', 'อี', 'กรอบ', ' ', 'xxfld', 'อาการ', 'แบบนี้']\n",
      "Predictions: \n",
      "\t['เปลี่ยน', 'เรื่อง', 'xxfld', 'ยัง', 'กลับมา', 'ถาม', 'อี', 'กรอบ', ' ', 'xxfld', 'อาการ', 'แบบนี้']\n",
      "\t['เปลี่ยน', 'เรื่อง', 'xxfld', 'ยัง', 'กลับมา', 'ถาม', 'ทุ', 'กรอบ', ' ', 'xxfld', 'อาการ', 'แบบนี้']\n",
      "\t['เปลี่ยน', 'เรื่อง', 'xxfld', 'ยัง', 'กลับมา', 'ถาม', 'บอ', 'กรอบ', ' ', 'xxfld', 'อาการ', 'แบบนี้']\n",
      "\t['เปลี่ยน', 'เรื่อง', 'xxfld', 'ยัง', 'กลับมา', 'ถาม', 'จา', 'กรอบ', ' ', 'xxfld', 'อาการ', 'แบบนี้']\n",
      "\t['เปลี่ยน', 'เรื่อง', 'xxfld', 'ยัง', 'กลับมา', 'ถาม', 'ออ', 'กรอบ', ' ', 'xxfld', 'อาการ', 'แบบนี้']\n",
      "\n",
      "Masked Input: \n",
      "\t['ยัง', 'กลับมา', 'ถาม', 'xxfld', 'กรอบ', ' ', 'xxfld', 'อาการ', 'แบบนี้', 'คือ', 'อะไร', 'คะ']\n",
      "Labeled Input: \n",
      "\t['ยัง', 'กลับมา', 'ถาม', 'xxfld', 'กรอบ', ' ', 'ไอ', 'อาการ', 'แบบนี้', 'คือ', 'อะไร', 'คะ']\n",
      "Predictions: \n",
      "\t['ยัง', 'กลับมา', 'ถาม', 'xxfld', 'กรอบ', ' ', 'ว่า', 'อาการ', 'แบบนี้', 'คือ', 'อะไร', 'คะ']\n",
      "\t['ยัง', 'กลับมา', 'ถาม', 'xxfld', 'กรอบ', ' ', 'แล้ว', 'อาการ', 'แบบนี้', 'คือ', 'อะไร', 'คะ']\n",
      "\t['ยัง', 'กลับมา', 'ถาม', 'xxfld', 'กรอบ', ' ', 'คือ', 'อาการ', 'แบบนี้', 'คือ', 'อะไร', 'คะ']\n",
      "\t['ยัง', 'กลับมา', 'ถาม', 'xxfld', 'กรอบ', ' ', 'อาการ', 'อาการ', 'แบบนี้', 'คือ', 'อะไร', 'คะ']\n",
      "\t['ยัง', 'กลับมา', 'ถาม', 'xxfld', 'กรอบ', ' ', 'แบบนี้', 'อาการ', 'แบบนี้', 'คือ', 'อะไร', 'คะ']\n",
      "\n",
      "Masked Input: \n",
      "\t['พอ', 'เรา', 'มี', 'คน', 'เข้ามา', 'ทำไม', 'xxfld', 'ถึง', 'มีอาการ', 'xxfld', ' ', 'มา']\n",
      "Labeled Input: \n",
      "\t['พอ', 'เรา', 'มี', 'คน', 'เข้ามา', 'ทำไม', 'เขา', 'ถึง', 'มีอาการ', 'xxfld', ' ', 'มา']\n",
      "Predictions: \n",
      "\t['พอ', 'เรา', 'มี', 'คน', 'เข้ามา', 'ทำไม', 'เขา', 'ถึง', 'มีอาการ', 'xxfld', ' ', 'มา']\n",
      "\t['พอ', 'เรา', 'มี', 'คน', 'เข้ามา', 'ทำไม', 'เรา', 'ถึง', 'มีอาการ', 'xxfld', ' ', 'มา']\n",
      "\t['พอ', 'เรา', 'มี', 'คน', 'เข้ามา', 'ทำไม', ' ', 'ถึง', 'มีอาการ', 'xxfld', ' ', 'มา']\n",
      "\t['พอ', 'เรา', 'มี', 'คน', 'เข้ามา', 'ทำไม', 'มัน', 'ถึง', 'มีอาการ', 'xxfld', ' ', 'มา']\n",
      "\t['พอ', 'เรา', 'มี', 'คน', 'เข้ามา', 'ทำไม', 'เค้า', 'ถึง', 'มีอาการ', 'xxfld', ' ', 'มา']\n",
      "\n",
      "Masked Input: \n",
      "\t['คน', 'เข้ามา', 'ทำไม', 'xxfld', 'ถึง', 'มีอาการ', 'xxfld', ' ', 'มา', 'ถาม', 'แบบนี้', 'ซ้ำๆ']\n",
      "Labeled Input: \n",
      "\t['คน', 'เข้ามา', 'ทำไม', 'xxfld', 'ถึง', 'มีอาการ', 'แบบนี้', ' ', 'มา', 'ถาม', 'แบบนี้', 'ซ้ำๆ']\n",
      "Predictions: \n",
      "\t['คน', 'เข้ามา', 'ทำไม', 'xxfld', 'ถึง', 'มีอาการ', 'แบบนี้', ' ', 'มา', 'ถาม', 'แบบนี้', 'ซ้ำๆ']\n",
      "\t['คน', 'เข้ามา', 'ทำไม', 'xxfld', 'ถึง', 'มีอาการ', 'แบบ', ' ', 'มา', 'ถาม', 'แบบนี้', 'ซ้ำๆ']\n",
      "\t['คน', 'เข้ามา', 'ทำไม', 'xxfld', 'ถึง', 'มีอาการ', 'อะไร', ' ', 'มา', 'ถาม', 'แบบนี้', 'ซ้ำๆ']\n",
      "\t['คน', 'เข้ามา', 'ทำไม', 'xxfld', 'ถึง', 'มีอาการ', 'แปลก', ' ', 'มา', 'ถาม', 'แบบนี้', 'ซ้ำๆ']\n",
      "\t['คน', 'เข้ามา', 'ทำไม', 'xxfld', 'ถึง', 'มีอาการ', 'นี้', ' ', 'มา', 'ถาม', 'แบบนี้', 'ซ้ำๆ']\n",
      "\n",
      "Masked Input: \n",
      "\t['ไม่', 'อยาก', 'คิด', 'อะไร', 'ไป', 'เอง', 'xxfld', 'xxfld', 'พอ', 'xxfld', 'ตอบ', 'ได้']\n",
      "Labeled Input: \n",
      "\t['ไม่', 'อยาก', 'คิด', 'อะไร', 'ไป', 'เอง', ' ', 'xxfld', 'พอ', 'xxfld', 'ตอบ', 'ได้']\n",
      "Predictions: \n",
      "\t['ไม่', 'อยาก', 'คิด', 'อะไร', 'ไป', 'เอง', ' ', 'xxfld', 'พอ', 'xxfld', 'ตอบ', 'ได้']\n",
      "\t['ไม่', 'อยาก', 'คิด', 'อะไร', 'ไป', 'เอง', 'เลย', 'xxfld', 'พอ', 'xxfld', 'ตอบ', 'ได้']\n",
      "\t['ไม่', 'อยาก', 'คิด', 'อะไร', 'ไป', 'เอง', 'บ้าง', 'xxfld', 'พอ', 'xxfld', 'ตอบ', 'ได้']\n",
      "\t['ไม่', 'อยาก', 'คิด', 'อะไร', 'ไป', 'เอง', 'หรือ', 'xxfld', 'พอ', 'xxfld', 'ตอบ', 'ได้']\n",
      "\t['ไม่', 'อยาก', 'คิด', 'อะไร', 'ไป', 'เอง', '?', 'xxfld', 'พอ', 'xxfld', 'ตอบ', 'ได้']\n",
      "\n",
      "Masked Input: \n",
      "\t['อยาก', 'คิด', 'อะไร', 'ไป', 'เอง', 'xxfld', 'xxfld', 'พอ', 'xxfld', 'ตอบ', 'ได้', 'บ้าง']\n",
      "Labeled Input: \n",
      "\t['อยาก', 'คิด', 'อะไร', 'ไป', 'เอง', 'xxfld', 'ใคร', 'พอ', 'xxfld', 'ตอบ', 'ได้', 'บ้าง']\n",
      "Predictions: \n",
      "\t['อยาก', 'คิด', 'อะไร', 'ไป', 'เอง', 'xxfld', 'ใคร', 'พอ', 'xxfld', 'ตอบ', 'ได้', 'บ้าง']\n",
      "\t['อยาก', 'คิด', 'อะไร', 'ไป', 'เอง', 'xxfld', ' ', 'พอ', 'xxfld', 'ตอบ', 'ได้', 'บ้าง']\n",
      "\t['อยาก', 'คิด', 'อะไร', 'ไป', 'เอง', 'xxfld', 'เรา', 'พอ', 'xxfld', 'ตอบ', 'ได้', 'บ้าง']\n",
      "\t['อยาก', 'คิด', 'อะไร', 'ไป', 'เอง', 'xxfld', 'เพื่อน', 'พอ', 'xxfld', 'ตอบ', 'ได้', 'บ้าง']\n",
      "\t['อยาก', 'คิด', 'อะไร', 'ไป', 'เอง', 'xxfld', 'ก็', 'พอ', 'xxfld', 'ตอบ', 'ได้', 'บ้าง']\n",
      "\n",
      "Masked Input: \n",
      "\t['อะไร', 'ไป', 'เอง', 'xxfld', 'xxfld', 'พอ', 'xxfld', 'ตอบ', 'ได้', 'บ้าง', 'คะ', 'xxfld']\n",
      "Labeled Input: \n",
      "\t['อะไร', 'ไป', 'เอง', 'xxfld', 'xxfld', 'พอ', 'จะ', 'ตอบ', 'ได้', 'บ้าง', 'คะ', 'xxfld']\n",
      "Predictions: \n",
      "\t['อะไร', 'ไป', 'เอง', 'xxfld', 'xxfld', 'พอ', 'จะ', 'ตอบ', 'ได้', 'บ้าง', 'คะ', 'xxfld']\n",
      "\t['อะไร', 'ไป', 'เอง', 'xxfld', 'xxfld', 'พอ', 'ช่วย', 'ตอบ', 'ได้', 'บ้าง', 'คะ', 'xxfld']\n",
      "\t['อะไร', 'ไป', 'เอง', 'xxfld', 'xxfld', 'พอ', 'ใคร', 'ตอบ', 'ได้', 'บ้าง', 'คะ', 'xxfld']\n",
      "\t['อะไร', 'ไป', 'เอง', 'xxfld', 'xxfld', 'พอ', 'เรา', 'ตอบ', 'ได้', 'บ้าง', 'คะ', 'xxfld']\n",
      "\t['อะไร', 'ไป', 'เอง', 'xxfld', 'xxfld', 'พอ', 'เขา', 'ตอบ', 'ได้', 'บ้าง', 'คะ', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t['พอ', 'xxfld', 'ตอบ', 'ได้', 'บ้าง', 'คะ', 'xxfld', 'ไอ', 'แบบนี้', 'มัน', 'xxfld', 'อะไร']\n",
      "Labeled Input: \n",
      "\t['พอ', 'xxfld', 'ตอบ', 'ได้', 'บ้าง', 'คะ', 'ว่า', 'ไอ', 'แบบนี้', 'มัน', 'xxfld', 'อะไร']\n",
      "Predictions: \n",
      "\t['พอ', 'xxfld', 'ตอบ', 'ได้', 'บ้าง', 'คะ', 'ว่า', 'ไอ', 'แบบนี้', 'มัน', 'xxfld', 'อะไร']\n",
      "\t['พอ', 'xxfld', 'ตอบ', 'ได้', 'บ้าง', 'คะ', 'แล้ว', 'ไอ', 'แบบนี้', 'มัน', 'xxfld', 'อะไร']\n",
      "\t['พอ', 'xxfld', 'ตอบ', 'ได้', 'บ้าง', 'คะ', 'แต่', 'ไอ', 'แบบนี้', 'มัน', 'xxfld', 'อะไร']\n",
      "\t['พอ', 'xxfld', 'ตอบ', 'ได้', 'บ้าง', 'คะ', 'คือ', 'ไอ', 'แบบนี้', 'มัน', 'xxfld', 'อะไร']\n",
      "\t['พอ', 'xxfld', 'ตอบ', 'ได้', 'บ้าง', 'คะ', 'ทำไม', 'ไอ', 'แบบนี้', 'มัน', 'xxfld', 'อะไร']\n",
      "\n",
      "Masked Input: \n",
      "\t['บ้าง', 'คะ', 'xxfld', 'ไอ', 'แบบนี้', 'มัน', 'xxfld', 'อะไร', ' ', 'รู้สึก', 'อะไร', 'อยู่']\n",
      "Labeled Input: \n",
      "\t['บ้าง', 'คะ', 'xxfld', 'ไอ', 'แบบนี้', 'มัน', 'คือ', 'อะไร', ' ', 'รู้สึก', 'อะไร', 'อยู่']\n",
      "Predictions: \n",
      "\t['บ้าง', 'คะ', 'xxfld', 'ไอ', 'แบบนี้', 'มัน', 'คือ', 'อะไร', ' ', 'รู้สึก', 'อะไร', 'อยู่']\n",
      "\t['บ้าง', 'คะ', 'xxfld', 'ไอ', 'แบบนี้', 'มัน', 'เกิด', 'อะไร', ' ', 'รู้สึก', 'อะไร', 'อยู่']\n",
      "\t['บ้าง', 'คะ', 'xxfld', 'ไอ', 'แบบนี้', 'มัน', 'เรียกว่า', 'อะไร', ' ', 'รู้สึก', 'อะไร', 'อยู่']\n",
      "\t['บ้าง', 'คะ', 'xxfld', 'ไอ', 'แบบนี้', 'มัน', 'หมายความว่า', 'อะไร', ' ', 'รู้สึก', 'อะไร', 'อยู่']\n",
      "\t['บ้าง', 'คะ', 'xxfld', 'ไอ', 'แบบนี้', 'มัน', 'หมายถึง', 'อะไร', ' ', 'รู้สึก', 'อะไร', 'อยู่']\n",
      "\n",
      "Total Input Tokens: torch.Size([394])\n",
      "Total Correct for Top 5: 30\n",
      "Total Mask: 33\n",
      "Percent Correct: 90.91%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"อาการแบบนี้คือไรกัน?  เขาคุยกับเรามา 5-6 เดือน เราตามจีบเขานะคะ ก็คุยกันมา ในระยะเวลาเขาบอกว่า ถ้าเราลด นน เพื่อเขาได้ เขาจะยอมเป็นแฟนเรา ตรรกะโง่มากนะคะ แต่ถามว่าทำมั้ย ทำค่ะ พอไปรู้ว่าเขาคุยกับเพื่อน เพื่อนเขาถามว่า รู้สึกยังไงกับเรา เขาตอบเพื่อนว่า เขาว่าเขาควรอยู่คนเดียว ยังไม่พร้อมจะรักใคร จนตอนนี้เราเริ่มรู้สึกว่า ทำไมเราต้องทำขนาดนั้น ถ้าเขาจะรัก รักที่เป็นตัวเราไม่ได้หรอ หลังๆเลยเริ่มสนใจเขาน้อยลง แต่ยังคุยกันเหมือนเดิม เราลองแกล้งเงียบไป ไม่ทักไปครึ่งวัน ปกติเราจะมีการมอนิ่งกันตอนเช้าค่ะ พอเราไม่ทักไป เขาทำงานเสร็จ ถึงเวลาพักของเขา เขาก็ทักมาว่า กินข้าวกัน เราก็ยิ่ง งง ก็คิดว่า เขาอาจจะชินหับการคุยกับเราทุกวันเฉยๆ นี่เลยไม่ได้สนใจในส่วนนั้น เราก็ตอบตามปกติ จนเมื่อคืนมีคนมาทักเราจีบเรา จะไปส่งเราที่บ้าน เราก็เลยเล่าให้เขาฟังว่า ให้ไลน์ไป ให้เขาไปส่งอยู่แต่ไมไ่ด้นั่งรถคันเดียวกัน เราก็ขับของเรา คนที่มาจีบเราเขาก็ขับคันของเขาแค่มาส่งเฉยๆ พอเช้ามาเขาทักมามอนิ่ง ก็ถามเราเรื่องเมื่อคืน เราทำงานที่กลับดึกมากๆไม่ได้ทักไปบอกเขาไว้ว่า ถึงบ้านแล้วนะ เงียบไปทั้งคืนเลย เขาก็ถามเรื่องเมื่อคืนว่า หนุ่มไปส่งที่บ้านเป็นไงบ้าง ถามแต่เรื่องของผู้ชายที่มาจีบเราทั้งวัน จนเราเปลี่ยนเรื่องก็ยังกลับมาถามอีกรอบ ไออาการแบบนี้คืออะไรคะ ? ไหนเขาบอกอยากอยู่คนเดียว แต่พอเรามีคนเข้ามา ทำไมเขาถึงมีอาการแบบนี้ มาถามแบบนี้ซ้ำๆ คืออะไรกัน เราไม่อยากคิดอะไรไปเอง ใครพอจะตอบได้บ้างคะ ว่า ไอแบบนี้มันคืออะไร รู้สึกอะไรอยู่\"\n",
    "tokenized_input, labels = _mask_input(text, tokenizer, mlm_probability=0.1)\n",
    "_predict_masks(tokenized_input, labels, tokenizer , k=5, verbose=True, verbose_length=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tokenize on Pantip Sample ร่างจำแลง (Literature)\n",
    "https://pantip.com/topic/40039192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Input: \n",
      "\t['xxfld', 'ก้าวแรก', 'ของ', 'xxfld', 'ที่', 'สัมผัส']\n",
      "Labeled Input: \n",
      "\t['xxbos', 'ก้าวแรก', 'ของ', 'xxfld', 'ที่', 'สัมผัส']\n",
      "Predictions: \n",
      "\t['xxbos', 'ก้าวแรก', 'ของ', 'xxfld', 'ที่', 'สัมผัส']\n",
      "\t['เหมือนกับ', 'ก้าวแรก', 'ของ', 'xxfld', 'ที่', 'สัมผัส']\n",
      "\t['พร้อมกับ', 'ก้าวแรก', 'ของ', 'xxfld', 'ที่', 'สัมผัส']\n",
      "\t['เช่นเดียวกับ', 'ก้าวแรก', 'ของ', 'xxfld', 'ที่', 'สัมผัส']\n",
      "\t['ดังเช่น', 'ก้าวแรก', 'ของ', 'xxfld', 'ที่', 'สัมผัส']\n",
      "\n",
      "Masked Input: \n",
      "\t['xxfld', 'ก้าวแรก', 'ของ', 'xxfld', 'ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน']\n",
      "Labeled Input: \n",
      "\t['xxfld', 'ก้าวแรก', 'ของ', 'ผม', 'ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน']\n",
      "Predictions: \n",
      "\t['xxfld', 'ก้าวแรก', 'ของ', 'ร่างกาย', 'ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน']\n",
      "\t['xxfld', 'ก้าวแรก', 'ของ', 'ผม', 'ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน']\n",
      "\t['xxfld', 'ก้าวแรก', 'ของ', 'ผู้', 'ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน']\n",
      "\t['xxfld', 'ก้าวแรก', 'ของ', 'ชีวิต', 'ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน']\n",
      "\t['xxfld', 'ก้าวแรก', 'ของ', 'คน', 'ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน']\n",
      "\n",
      "Masked Input: \n",
      "\t['xxfld', 'ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน', 'xxfld', 'xxfld', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ']\n",
      "Labeled Input: \n",
      "\t['xxfld', 'ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน', 'xxunk', 'xxfld', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ']\n",
      "Predictions: \n",
      "\t['xxfld', 'ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน', 'โสต', 'xxfld', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ']\n",
      "\t['xxfld', 'ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน', 'ห้อง', 'xxfld', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ']\n",
      "\t['xxfld', 'ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน', 'กาย', 'xxfld', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ']\n",
      "\t['xxfld', 'ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน', 'ภวังค์', 'xxfld', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ']\n",
      "\t['xxfld', 'ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน', 'ทุก', 'xxfld', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ']\n",
      "\n",
      "Masked Input: \n",
      "\t['ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน', 'xxfld', 'xxfld', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ', 'รวมถึง']\n",
      "Labeled Input: \n",
      "\t['ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน', 'xxfld', 'แห่ง', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ', 'รวมถึง']\n",
      "Predictions: \n",
      "\t['ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน', 'xxfld', 'แห่ง', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ', 'รวมถึง']\n",
      "\t['ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน', 'xxfld', 'แล้ว', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ', 'รวมถึง']\n",
      "\t['ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน', 'xxfld', 'นั้น', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ', 'รวมถึง']\n",
      "\t['ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน', 'xxfld', ' ', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ', 'รวมถึง']\n",
      "\t['ที่', 'สัมผัส', 'ฝ่าเท้า', 'ลง', 'ภายใน', 'xxfld', 'เย็น', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ', 'รวมถึง']\n",
      "\n",
      "Masked Input: \n",
      "\t['xxfld', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ', 'รวมถึง', 'xxfld', 'ประดัง', 'ประเด', 'เข้า', 'ใน', 'โสต']\n",
      "Labeled Input: \n",
      "\t['xxfld', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ', 'รวมถึง', 'ภาพเคลื่อนไหว', 'ประดัง', 'ประเด', 'เข้า', 'ใน', 'โสต']\n",
      "Predictions: \n",
      "\t['xxfld', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ', 'รวมถึง', 'เสียง', 'ประดัง', 'ประเด', 'เข้า', 'ใน', 'โสต']\n",
      "\t['xxfld', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ', 'รวมถึง', 'เสียงหัวเราะ', 'ประดัง', 'ประเด', 'เข้า', 'ใน', 'โสต']\n",
      "\t['xxfld', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ', 'รวมถึง', 'เสียงร้อง', 'ประดัง', 'ประเด', 'เข้า', 'ใน', 'โสต']\n",
      "\t['xxfld', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ', 'รวมถึง', 'เสียงดนตรี', 'ประดัง', 'ประเด', 'เข้า', 'ใน', 'โสต']\n",
      "\t['xxfld', 'นั้น', ' ', 'เสียง', 'ต่าง ๆ', 'รวมถึง', 'เสียงเพลง', 'ประดัง', 'ประเด', 'เข้า', 'ใน', 'โสต']\n",
      "\n",
      "Masked Input: \n",
      "\t['ประเด', 'เข้า', 'ใน', 'โสต', 'อย่าง', 'พร้อมเพรียง', 'xxfld', 'สร้าง', 'ความ', 'อึกทึก', 'มึนตึง', 'ไป']\n",
      "Labeled Input: \n",
      "\t['ประเด', 'เข้า', 'ใน', 'โสต', 'อย่าง', 'พร้อมเพรียง', ' ', 'สร้าง', 'ความ', 'อึกทึก', 'มึนตึง', 'ไป']\n",
      "Predictions: \n",
      "\t['ประเด', 'เข้า', 'ใน', 'โสต', 'อย่าง', 'พร้อมเพรียง', ' ', 'สร้าง', 'ความ', 'อึกทึก', 'มึนตึง', 'ไป']\n",
      "\t['ประเด', 'เข้า', 'ใน', 'โสต', 'อย่าง', 'พร้อมเพรียง', 'จน', 'สร้าง', 'ความ', 'อึกทึก', 'มึนตึง', 'ไป']\n",
      "\t['ประเด', 'เข้า', 'ใน', 'โสต', 'อย่าง', 'พร้อมเพรียง', 'และ', 'สร้าง', 'ความ', 'อึกทึก', 'มึนตึง', 'ไป']\n",
      "\t['ประเด', 'เข้า', 'ใน', 'โสต', 'อย่าง', 'พร้อมเพรียง', 'ที่', 'สร้าง', 'ความ', 'อึกทึก', 'มึนตึง', 'ไป']\n",
      "\t['ประเด', 'เข้า', 'ใน', 'โสต', 'อย่าง', 'พร้อมเพรียง', 'ก็', 'สร้าง', 'ความ', 'อึกทึก', 'มึนตึง', 'ไป']\n",
      "\n",
      "Masked Input: \n",
      "\t['หมด', ' ', '\\n', ' ', 'แสง', 'สี', 'xxfld', 'เพดาน', 'ที่', 'กะพริบ', 'กวัดแกว่ง', 'ไป']\n",
      "Labeled Input: \n",
      "\t['หมด', ' ', '\\n', ' ', 'แสง', 'สี', 'บน', 'เพดาน', 'ที่', 'กะพริบ', 'กวัดแกว่ง', 'ไป']\n",
      "Predictions: \n",
      "\t['หมด', ' ', '\\n', ' ', 'แสง', 'สี', 'บน', 'เพดาน', 'ที่', 'กะพริบ', 'กวัดแกว่ง', 'ไป']\n",
      "\t['หมด', ' ', '\\n', ' ', 'แสง', 'สี', 'ใน', 'เพดาน', 'ที่', 'กะพริบ', 'กวัดแกว่ง', 'ไป']\n",
      "\t['หมด', ' ', '\\n', ' ', 'แสง', 'สี', 'จาก', 'เพดาน', 'ที่', 'กะพริบ', 'กวัดแกว่ง', 'ไป']\n",
      "\t['หมด', ' ', '\\n', ' ', 'แสง', 'สี', 'ของ', 'เพดาน', 'ที่', 'กะพริบ', 'กวัดแกว่ง', 'ไป']\n",
      "\t['หมด', ' ', '\\n', ' ', 'แสง', 'สี', 'ตาม', 'เพดาน', 'ที่', 'กะพริบ', 'กวัดแกว่ง', 'ไป']\n",
      "\n",
      "Masked Input: \n",
      "\t['คน', 'ที่', 'ไม่', 'เคยชิน', 'อย่าง', 'ผม', 'xxfld', 'เพิ่ง', 'ย่างกรายเข้ามา', 'เป็น', 'หน', 'แรก']\n",
      "Labeled Input: \n",
      "\t['คน', 'ที่', 'ไม่', 'เคยชิน', 'อย่าง', 'ผม', 'ที่', 'เพิ่ง', 'ย่างกรายเข้ามา', 'เป็น', 'หน', 'แรก']\n",
      "Predictions: \n",
      "\t['คน', 'ที่', 'ไม่', 'เคยชิน', 'อย่าง', 'ผม', 'ก็', 'เพิ่ง', 'ย่างกรายเข้ามา', 'เป็น', 'หน', 'แรก']\n",
      "\t['คน', 'ที่', 'ไม่', 'เคยชิน', 'อย่าง', 'ผม', 'ที่', 'เพิ่ง', 'ย่างกรายเข้ามา', 'เป็น', 'หน', 'แรก']\n",
      "\t['คน', 'ที่', 'ไม่', 'เคยชิน', 'อย่าง', 'ผม', 'นั้น', 'เพิ่ง', 'ย่างกรายเข้ามา', 'เป็น', 'หน', 'แรก']\n",
      "\t['คน', 'ที่', 'ไม่', 'เคยชิน', 'อย่าง', 'ผม', 'นี่', 'เพิ่ง', 'ย่างกรายเข้ามา', 'เป็น', 'หน', 'แรก']\n",
      "\t['คน', 'ที่', 'ไม่', 'เคยชิน', 'อย่าง', 'ผม', ' ', 'เพิ่ง', 'ย่างกรายเข้ามา', 'เป็น', 'หน', 'แรก']\n",
      "\n",
      "Masked Input: \n",
      "\t['เป็น', 'หน', 'แรก', ' ', '\\n', ' ', 'xxfld', 'ก็', 'กระหึ่ม', 'กระแทกกระทั้น', 'เร้าใจ', ' ']\n",
      "Labeled Input: \n",
      "\t['เป็น', 'หน', 'แรก', ' ', '\\n', ' ', 'เสียงเพลง', 'ก็', 'กระหึ่ม', 'กระแทกกระทั้น', 'เร้าใจ', ' ']\n",
      "Predictions: \n",
      "\t['เป็น', 'หน', 'แรก', ' ', '\\n', ' ', 'เสียงเพลง', 'ก็', 'กระหึ่ม', 'กระแทกกระทั้น', 'เร้าใจ', ' ']\n",
      "\t['เป็น', 'หน', 'แรก', ' ', '\\n', ' ', 'เสียง', 'ก็', 'กระหึ่ม', 'กระแทกกระทั้น', 'เร้าใจ', ' ']\n",
      "\t['เป็น', 'หน', 'แรก', ' ', '\\n', ' ', 'เสียงพูด', 'ก็', 'กระหึ่ม', 'กระแทกกระทั้น', 'เร้าใจ', ' ']\n",
      "\t['เป็น', 'หน', 'แรก', ' ', '\\n', ' ', 'บางครั้ง', 'ก็', 'กระหึ่ม', 'กระแทกกระทั้น', 'เร้าใจ', ' ']\n",
      "\t['เป็น', 'หน', 'แรก', ' ', '\\n', ' ', 'ความเงียบ', 'ก็', 'กระหึ่ม', 'กระแทกกระทั้น', 'เร้าใจ', ' ']\n",
      "\n",
      "Masked Input: \n",
      "\t['ทุก', 'อณู', 'ส่วน', 'ของ', 'ร่างกาย', 'อยาก', 'xxfld', 'เด้า', 'ดิ้น', 'วาดลวดลาย', 'จน', 'ทรุด']\n",
      "Labeled Input: \n",
      "\t['ทุก', 'อณู', 'ส่วน', 'ของ', 'ร่างกาย', 'อยาก', 'ขยับ', 'เด้า', 'ดิ้น', 'วาดลวดลาย', 'จน', 'ทรุด']\n",
      "Predictions: \n",
      "\t['ทุก', 'อณู', 'ส่วน', 'ของ', 'ร่างกาย', 'อยาก', 'จะ', 'เด้า', 'ดิ้น', 'วาดลวดลาย', 'จน', 'ทรุด']\n",
      "\t['ทุก', 'อณู', 'ส่วน', 'ของ', 'ร่างกาย', 'อยาก', 'ให้', 'เด้า', 'ดิ้น', 'วาดลวดลาย', 'จน', 'ทรุด']\n",
      "\t['ทุก', 'อณู', 'ส่วน', 'ของ', 'ร่างกาย', 'อยาก', 'เห็น', 'เด้า', 'ดิ้น', 'วาดลวดลาย', 'จน', 'ทรุด']\n",
      "\t['ทุก', 'อณู', 'ส่วน', 'ของ', 'ร่างกาย', 'อยาก', 'เต้น', 'เด้า', 'ดิ้น', 'วาดลวดลาย', 'จน', 'ทรุด']\n",
      "\t['ทุก', 'อณู', 'ส่วน', 'ของ', 'ร่างกาย', 'อยาก', 'นอน', 'เด้า', 'ดิ้น', 'วาดลวดลาย', 'จน', 'ทรุด']\n",
      "\n",
      "Masked Input: \n",
      "\t['ดุจดั่ง', 'ที่', 'หลาย', 'คน', 'บน', 'พื้น', 'xxfld', 'กำลัง', 'xxfld', 'อยู่', ' ', 'xxfld']\n",
      "Labeled Input: \n",
      "\t['ดุจดั่ง', 'ที่', 'หลาย', 'คน', 'บน', 'พื้น', 'เต้น', 'กำลัง', 'xxfld', 'อยู่', ' ', 'xxfld']\n",
      "Predictions: \n",
      "\t['ดุจดั่ง', 'ที่', 'หลาย', 'คน', 'บน', 'พื้น', 'นั้น', 'กำลัง', 'xxfld', 'อยู่', ' ', 'xxfld']\n",
      "\t['ดุจดั่ง', 'ที่', 'หลาย', 'คน', 'บน', 'พื้น', 'ที่', 'กำลัง', 'xxfld', 'อยู่', ' ', 'xxfld']\n",
      "\t['ดุจดั่ง', 'ที่', 'หลาย', 'คน', 'บน', 'พื้น', ' ', 'กำลัง', 'xxfld', 'อยู่', ' ', 'xxfld']\n",
      "\t['ดุจดั่ง', 'ที่', 'หลาย', 'คน', 'บน', 'พื้น', 'มี', 'กำลัง', 'xxfld', 'อยู่', ' ', 'xxfld']\n",
      "\t['ดุจดั่ง', 'ที่', 'หลาย', 'คน', 'บน', 'พื้น', 'และ', 'กำลัง', 'xxfld', 'อยู่', ' ', 'xxfld']\n",
      "\n",
      "Masked Input: \n",
      "\t['หลาย', 'คน', 'บน', 'พื้น', 'xxfld', 'กำลัง', 'xxfld', 'อยู่', ' ', 'xxfld', 'มิเช่นนั้น', 'ก็']\n",
      "Labeled Input: \n",
      "\t['หลาย', 'คน', 'บน', 'พื้น', 'xxfld', 'กำลัง', 'ทำ', 'อยู่', ' ', 'xxfld', 'มิเช่นนั้น', 'ก็']\n",
      "Predictions: \n",
      "\t['หลาย', 'คน', 'บน', 'พื้น', 'xxfld', 'กำลัง', 'นั่ง', 'อยู่', ' ', 'xxfld', 'มิเช่นนั้น', 'ก็']\n",
      "\t['หลาย', 'คน', 'บน', 'พื้น', 'xxfld', 'กำลัง', 'ยืน', 'อยู่', ' ', 'xxfld', 'มิเช่นนั้น', 'ก็']\n",
      "\t['หลาย', 'คน', 'บน', 'พื้น', 'xxfld', 'กำลัง', 'หายใจ', 'อยู่', ' ', 'xxfld', 'มิเช่นนั้น', 'ก็']\n",
      "\t['หลาย', 'คน', 'บน', 'พื้น', 'xxfld', 'กำลัง', 'นอน', 'อยู่', ' ', 'xxfld', 'มิเช่นนั้น', 'ก็']\n",
      "\t['หลาย', 'คน', 'บน', 'พื้น', 'xxfld', 'กำลัง', 'ทำ', 'อยู่', ' ', 'xxfld', 'มิเช่นนั้น', 'ก็']\n",
      "\n",
      "Masked Input: \n",
      "\t['พื้น', 'xxfld', 'กำลัง', 'xxfld', 'อยู่', ' ', 'xxfld', 'มิเช่นนั้น', 'ก็', 'หนวกหู', 'รำคาญ', 'ไป']\n",
      "Labeled Input: \n",
      "\t['พื้น', 'xxfld', 'กำลัง', 'xxfld', 'อยู่', ' ', 'หรือ', 'มิเช่นนั้น', 'ก็', 'หนวกหู', 'รำคาญ', 'ไป']\n",
      "Predictions: \n",
      "\t['พื้น', 'xxfld', 'กำลัง', 'xxfld', 'อยู่', ' ', 'แต่', 'มิเช่นนั้น', 'ก็', 'หนวกหู', 'รำคาญ', 'ไป']\n",
      "\t['พื้น', 'xxfld', 'กำลัง', 'xxfld', 'อยู่', ' ', 'ซึ่ง', 'มิเช่นนั้น', 'ก็', 'หนวกหู', 'รำคาญ', 'ไป']\n",
      "\t['พื้น', 'xxfld', 'กำลัง', 'xxfld', 'อยู่', ' ', 'และ', 'มิเช่นนั้น', 'ก็', 'หนวกหู', 'รำคาญ', 'ไป']\n",
      "\t['พื้น', 'xxfld', 'กำลัง', 'xxfld', 'อยู่', ' ', 'หรือ', 'มิเช่นนั้น', 'ก็', 'หนวกหู', 'รำคาญ', 'ไป']\n",
      "\t['พื้น', 'xxfld', 'กำลัง', 'xxfld', 'อยู่', ' ', 'แล้ว', 'มิเช่นนั้น', 'ก็', 'หนวกหู', 'รำคาญ', 'ไป']\n",
      "\n",
      "Masked Input: \n",
      "\t['ที่', 'ผม', 'กำลัง', 'รู้สึก', ' ', '\\n', 'xxfld', 'ชัยนาท', 'เห็น', 'ผม', 'แต่ไกล', 'ได้']\n",
      "Labeled Input: \n",
      "\t['ที่', 'ผม', 'กำลัง', 'รู้สึก', ' ', '\\n', ' ', 'ชัยนาท', 'เห็น', 'ผม', 'แต่ไกล', 'ได้']\n",
      "Predictions: \n",
      "\t['ที่', 'ผม', 'กำลัง', 'รู้สึก', ' ', '\\n', ' ', 'ชัยนาท', 'เห็น', 'ผม', 'แต่ไกล', 'ได้']\n",
      "\t['ที่', 'ผม', 'กำลัง', 'รู้สึก', ' ', '\\n', '  ', 'ชัยนาท', 'เห็น', 'ผม', 'แต่ไกล', 'ได้']\n",
      "\t['ที่', 'ผม', 'กำลัง', 'รู้สึก', ' ', '\\n', '\\n', 'ชัยนาท', 'เห็น', 'ผม', 'แต่ไกล', 'ได้']\n",
      "\t['ที่', 'ผม', 'กำลัง', 'รู้สึก', ' ', '\\n', ')', 'ชัยนาท', 'เห็น', 'ผม', 'แต่ไกล', 'ได้']\n",
      "\t['ที่', 'ผม', 'กำลัง', 'รู้สึก', ' ', '\\n', '(', 'ชัยนาท', 'เห็น', 'ผม', 'แต่ไกล', 'ได้']\n",
      "\n",
      "Masked Input: \n",
      "\t['แต่ไกล', 'ได้', 'อย่างไร', 'ไม่รู้', ' ', 'เขา', 'xxfld', 'คุ้นเคย', 'กับ', 'ห้องมืด', 'สลัว', 'ที่มี']\n",
      "Labeled Input: \n",
      "\t['แต่ไกล', 'ได้', 'อย่างไร', 'ไม่รู้', ' ', 'เขา', 'คง', 'คุ้นเคย', 'กับ', 'ห้องมืด', 'สลัว', 'ที่มี']\n",
      "Predictions: \n",
      "\t['แต่ไกล', 'ได้', 'อย่างไร', 'ไม่รู้', ' ', 'เขา', 'จึง', 'คุ้นเคย', 'กับ', 'ห้องมืด', 'สลัว', 'ที่มี']\n",
      "\t['แต่ไกล', 'ได้', 'อย่างไร', 'ไม่รู้', ' ', 'เขา', 'ก็', 'คุ้นเคย', 'กับ', 'ห้องมืด', 'สลัว', 'ที่มี']\n",
      "\t['แต่ไกล', 'ได้', 'อย่างไร', 'ไม่รู้', ' ', 'เขา', 'ไม่', 'คุ้นเคย', 'กับ', 'ห้องมืด', 'สลัว', 'ที่มี']\n",
      "\t['แต่ไกล', 'ได้', 'อย่างไร', 'ไม่รู้', ' ', 'เขา', 'เริ่ม', 'คุ้นเคย', 'กับ', 'ห้องมืด', 'สลัว', 'ที่มี']\n",
      "\t['แต่ไกล', 'ได้', 'อย่างไร', 'ไม่รู้', ' ', 'เขา', 'คง', 'คุ้นเคย', 'กับ', 'ห้องมืด', 'สลัว', 'ที่มี']\n",
      "\n",
      "Total Input Tokens: torch.Size([155])\n",
      "Total Correct for Top 5: 11\n",
      "Total Mask: 15\n",
      "Percent Correct: 73.33%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7333333333333333"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"ก้าวแรกของผมที่สัมผัสฝ่าเท้าลงภายในสถานเริงรมย์แห่งนั้น เสียงต่างๆรวมถึงภาพเคลื่อนไหวประดังประเดเข้าในโสตอย่างพร้อมเพรียง สร้างความอึกทึกมึนตึงไปหมด\n",
    "แสงสีบนเพดานที่กระพริบกวัดแกว่งไปมาจากการควบคุมด้วยเทคโนโลยี สลับลำแสงท่ามกลางความมืดจนตาลายจำแนกวัตถุไม่ออก ยิ่งสำหรับคนที่ไม่เคยชินอย่างผมที่เพิ่งย่างกรายเข้ามาเป็นหนแรก\n",
    "เสียงเพลงก็กระหึ่มกระแทกกระทั้นเร้าใจ กระตุ้นให้ทุกอณูส่วนของร่างกายอยากขยับเด้าดิ้นวาดลวดลายจนทรุดกองฮวบกับพื้น ดุจดั่งที่หลายคนบนพื้นเต้นกำลังทำอยู่ หรือมิเช่นนั้นก็หนวกหูรำคาญไปเลยอย่างที่ผมกำลังรู้สึก\n",
    "ชัยนาทเห็นผมแต่ไกลได้อย่างไรไม่รู้ เขาคงคุ้นเคยกับห้องมืดสลัวที่มีแสงไฟเปลี่ยนสีตวัดกวัดไกวไปมาจนปวดตา จึงเห็นผมได้ไม่ยากเย็น\"\"\"\n",
    "tokenized_input, labels = _mask_input(text, tokenizer, mlm_probability=0.1)\n",
    "_predict_masks(tokenized_input, labels, tokenizer , k=5, verbose=True, verbose_length=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tokenize on Own Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Input: \n",
      "\tัสดีครับ<mask>ื่อไนท์ ตอนน\n",
      "Labeled Input: \n",
      "\tัสดีครับ ผมชื่อไนท์ ตอนน\n",
      "Predictions: \n",
      "\tัสดีครับ ผมชื่อไนท์ ตอนน\n",
      "\tัสดีครับ เราชื่อไนท์ ตอนน\n",
      "\tัสดีครับ เพื่อไนท์ ตอนน\n",
      "\tัสดีครับ ชื่อไนท์ ตอนน\n",
      "\tัสดีครับ เมื่อไนท์ ตอนน\n",
      "\n",
      "Masked Input: \n",
      "\t้องไปโรงเรียนแล้<mask> น<mask>คือการ\n",
      "Labeled Input: \n",
      "\t้องไปโรงเรียนแล้ว น<mask>คือการ\n",
      "Predictions: \n",
      "\t้องไปโรงเรียนแล้ว น<mask>คือการ\n",
      "\t้องไปโรงเรียนแล้วนะ น<mask>คือการ\n",
      "\t้องไปโรงเรียนแล้วผม น<mask>คือการ\n",
      "\t้องไปโรงเรียนแล้วว น<mask>คือการ\n",
      "\t้องไปโรงเรียนแล้ววว น<mask>คือการ\n",
      "\n",
      "Masked Input: \n",
      "\tโรงเรียนแล้<mask> น<mask>คือการ<mask>้\n",
      "Labeled Input: \n",
      "\tโรงเรียนแล้<mask> นี่คือการ<mask>้\n",
      "Predictions: \n",
      "\tโรงเรียนแล้<mask> นี่คือการ<mask>้\n",
      "\tโรงเรียนแล้<mask> นี้คือการ<mask>้\n",
      "\tโรงเรียนแล้<mask> นีคือการ<mask>้\n",
      "\tโรงเรียนแล้<mask> นิคือการ<mask>้\n",
      "\tโรงเรียนแล้<mask> น็คือการ<mask>้\n",
      "\n",
      "Masked Input: \n",
      "\t<mask> น<mask>คือการ<mask>้นวรรคสองที\n",
      "Labeled Input: \n",
      "\t<mask> น<mask>คือการเว้นวรรคสองที\n",
      "Predictions: \n",
      "\t<mask> น<mask>คือการเว้นวรรคสองที\n",
      "\t<mask> น<mask>คือการลดต้นวรรคสองที\n",
      "\t<mask> น<mask>คือการ เว้นวรรคสองที\n",
      "\t<mask> น<mask>คือการได้นวรรคสองที\n",
      "\t<mask> น<mask>คือการเข้นวรรคสองที\n",
      "\n",
      "Masked Input: \n",
      "\t้นวรรคสองทีคร<mask>บ จะได้<mask><mask>\n",
      "Labeled Input: \n",
      "\t้นวรรคสองทีครับ จะได้<mask><mask>\n",
      "Predictions: \n",
      "\t้นวรรคสองทีครับ จะได้<mask><mask>\n",
      "\t้นวรรคสองทีครีบ จะได้<mask><mask>\n",
      "\t้นวรรคสองทีคร้บ จะได้<mask><mask>\n",
      "\t้นวรรคสองทีครึบ จะได้<mask><mask>\n",
      "\t้นวรรคสองทีคริบ จะได้<mask><mask>\n",
      "\n",
      "Masked Input: \n",
      "\tีคร<mask>บ จะได้<mask><mask>นสอง</s>\n",
      "Labeled Input: \n",
      "\tีคร<mask>บ จะได้ออกเป<mask>นสอง</s>\n",
      "Predictions: \n",
      "\tีคร<mask>บ จะได้เป<mask>นสอง</s>\n",
      "\tีคร<mask>บ จะได้ร<mask>นสอง</s>\n",
      "\tีคร<mask>บ จะได้มาเป<mask>นสอง</s>\n",
      "\tีคร<mask>บ จะได้ข<mask>นสอง</s>\n",
      "\tีคร<mask>บ จะได้ย<mask>นสอง</s>\n",
      "\n",
      "Masked Input: \n",
      "\tคร<mask>บ จะได้<mask><mask>นสอง</s>\n",
      "Labeled Input: \n",
      "\tคร<mask>บ จะได้<mask>็นสอง</s>\n",
      "Predictions: \n",
      "\tคร<mask>บ จะได้<mask>็นสอง</s>\n",
      "\tคร<mask>บ จะได้<mask>ันสอง</s>\n",
      "\tคร<mask>บ จะได้<mask>ึ้นสอง</s>\n",
      "\tคร<mask>บ จะได้<mask>้นสอง</s>\n",
      "\tคร<mask>บ จะได้<mask>่นสอง</s>\n",
      "\n",
      "Total Input Tokens: torch.Size([49])\n",
      "Total Correct for Top 5: 6\n",
      "Total Mask: 7\n",
      "Percent Correct: 85.71%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"สวัสดีครับ ผมชื่อไนท์ ตอนนี้ก็เป็นเวลาที่ผมต้องไปโรงเรียนแล้ว นี่คือการเว้นวรรคสองทีครับ จะได้ออกเป็นสอง\"\n",
    "tokenized_input, labels = _mask_input(text, tokenizer, mlm_probability=0.15)\n",
    "_predict_masks(tokenized_input, labels, tokenizer , k=5, verbose=True, verbose_length=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or trying the Vanilla way to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,    40,  2744,   789,  9774, 17624,    87,  6344,  2352,  1848,\n",
       "           522, 13701,  1207, 17624,    87, 19898,   584,  1164,   721,    18,\n",
       "           225,    57, 19100,  1207,    81, 12140,    73,   977,  1572,  1207,\n",
       "           676,   633,  3789, 12847, 10865,   731, 11955, 15051,    84,     4,\n",
       "         14855,  5963,    70,   524,   655,  8489,  3220,  3449,    18,     2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 40, 2744, 789, 9774, 17624, 87, 6344, 2352, 1848, 522, 13701, 1207, 17624, 87, 19898, 584, 1164, 721, 18, 225, 57, 19100, 1207, 81, 12140, 73, 977, 1572, 1207, 676, 633, 3789, 12847, 10865, 731, 11955, 15051, 84, 4, 14855, 5963, 70, 524, 655, 8489, 3220, 3449, 18, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([39])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_index = torch.where(_input == tokenizer.mask_token_id)[1]\n",
    "mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -6.8569, -11.4544,  -6.2892,  ...,  -3.3654,  -1.7394,  -3.8599],\n",
       "         [ -6.7165,  -7.8614, -10.1110,  ...,  -0.2116,  -1.2954,  -8.0255],\n",
       "         [ -6.4925,  -9.5634,  -9.8982,  ...,  -3.4279,  -1.0313,  -6.8084],\n",
       "         ...,\n",
       "         [ -6.2978,  -6.5940,  -9.4618,  ...,   0.5986,  -2.2195,  -8.1321],\n",
       "         [ -6.3295,  -9.3629,  -9.6262,  ...,   0.0172,  -8.5017,  -5.1815],\n",
       "         [ -6.8569, -11.4543,  -6.2891,  ...,  -3.3655,  -1.7393,  -3.8597]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_logits = model(_input)[0]\n",
    "token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6.0691,  -7.5146, -11.4188,  ...,   3.1234,  -1.2106,  -6.2130]],\n",
       "       grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "mask_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1228, 1187, 1796, 18, 12549]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "top_5_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help ed our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help  in our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help  to our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help . our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help  her our carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "for token in top_5_tokens:\n",
    "     print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Text-Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Generation is currently not supported for RobertaForMaskedLM. Please select a model from ['XLNetLMHeadModel', 'TransfoXLLMHeadModel', 'ReformerModelWithLMHead', 'GPT2LMHeadModel', 'OpenAIGPTLMHeadModel', 'CTRLLMHeadModel', 'TFXLNetLMHeadModel', 'TFTransfoXLLMHeadModel', 'TFGPT2LMHeadModel', 'TFOpenAIGPTLMHeadModel', 'TFCTRLLMHeadModel'] for generation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-cb55d1878408>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"As far as I am concerned, I will\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, return_tensors, return_text, clean_up_tokenization_spaces, *args, **generate_kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m             raise NotImplementedError(\n\u001b[1;32m    645\u001b[0m                 \"Generation is currently not supported for {}. Please select a model from {} for generation.\".format(\n\u001b[0;32m--> 646\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALLOWED_MODELS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m                 )\n\u001b[1;32m    648\u001b[0m             )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Generation is currently not supported for RobertaForMaskedLM. Please select a model from ['XLNetLMHeadModel', 'TransfoXLLMHeadModel', 'ReformerModelWithLMHead', 'GPT2LMHeadModel', 'OpenAIGPTLMHeadModel', 'CTRLLMHeadModel', 'TFXLNetLMHeadModel', 'TFTransfoXLLMHeadModel', 'TFGPT2LMHeadModel', 'TFOpenAIGPTLMHeadModel', 'TFCTRLLMHeadModel'] for generation."
     ]
    }
   ],
   "source": [
    "text_gen(\"As far as I am concerned, I will\", max_length=50, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1783,  275,  566,  278,  333,  275,  282]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['สว', 'ั', 'สด', 'ี', 'คร', 'ั', 'บ']"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"สวัสดีครับ\"\n",
    "_inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "print(_inputs)\n",
    "[tokenizer.decode([tok]) for tok in _inputs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2, 313,  15,   4,   3]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def _text_generation(prompt, max_length=250)\n",
    "prompt = \"สวัสดีครับ\"\n",
    "\n",
    "def _add_mask_to_last(prompt, tokenizer, device=None):\n",
    "    # Tokenize the sequence with special tokens inserted, remove last token and replace with mask\n",
    "    if type(prompt) is str:\n",
    "        _input = tokenizer.encode(prompt)\n",
    "        _input[0, -1] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "        _temp_tensor = torch.zeros((_input.shape[0], 1), dtype=torch.int64)\n",
    "        _temp_tensor[0][0] = tokenizer.sep_token_id\n",
    "        return torch.cat((_input, _temp_tensor), dim=1)\n",
    "    else: # if it is tensor\n",
    "        _input = prompt.clone()\n",
    "        _temp_tensor = torch.zeros((_input.shape[0], 2), dtype=torch.int64)\n",
    "        _temp_tensor[0][0] = tokenizer.mask_token_id\n",
    "        _temp_tensor[0][1] = tokenizer.sep_token_id\n",
    "#         _temp_tensor = _temp_tensor.to(device)\n",
    "        return torch.cat((prompt, _temp_tensor), dim=1)\n",
    "_add_mask_to_last(prompt, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(20000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=20000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_device = torch.device(\"cpu\")\n",
    "model.to(cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict_last_token(_input, tokenizer, k, top_p):\n",
    "    \"\"\"top_k – (optional) int The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.\n",
    "\n",
    "    top_p – (optional) float The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.\"\"\"\n",
    "    mask_token_index = torch.where(_input == tokenizer.mask_token_id)[1]\n",
    "#     _input.to(device)\n",
    "    token_logits = model(_input)[0]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    _mask_token_logits_cpu = mask_token_logits.clone().cpu()\n",
    "    top_k_tokens = torch.topk(_mask_token_logits_cpu, k, dim=1).indices[0].numpy()\n",
    "    if np.random.random() < top_p:\n",
    "        return top_k_tokens[0]\n",
    "    else:\n",
    "        return top_k_tokens[1:][np.random.randint(low=0, high=top_k_tokens.size-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _text_generate(prompt, tokenizer, length = 250, top_k=10, top_p=0.95, device=None):\n",
    "#     _input = _add_mask_to_last(prompt, tokenizer)\n",
    "    _input = prompt\n",
    "    for i in range(length):\n",
    "        _input = _add_mask_to_last(_input, tokenizer, device=device)\n",
    "#         _input = _input.to(device)\n",
    "        _input[0, -2] = _predict_last_token(_input, tokenizer, k=top_k, top_p = top_p)\n",
    "        _input = _input[:, :-1].clone()\n",
    "    print(f\"{list(map(lambda x: tokenizer.decode([x]) , _input[0]))}\")\n",
    "#     print(tokenizer.decode(_input[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 1783,  275,  566,  278,  333,  275,  282,    4]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxbos', 'การ', 'ตั้งกระทู้', 'และ', 'การ', 'เชียร์', 'บอล', ' ', 'รวมถึง', 'กิจกรรม', 'ทั้งหมด', 'ของ', 'ชมรม', ' ', 'ฯ', ' ', 'เป็นไป', 'เพื่อ', 'ความบันเทิง', 'ของ', 'สมาชิก', 'ทุกๆ', 'ท่าน', 'ทั้ง', ' ', 'xxnum', ' ', 'ท่าน', ' ', 'และ', 'ที่', ' ', 'xxnum', ' ', 'กิจกรรม', 'การ', 'ตั้งกระทู้', '+', 'การ', 'เชียร์', 'บอล', ' ', 'กิจกรรม', 'การ', 'ตั้งกระทู้', '+', 'การ', 'เชียร์', 'หงส์', 'ปาร์ตี้', ' ', 'และ', 'กิจกรรม', 'การ', 'เชียร์', 'หงส์', 'ปาร์ตี้', ' ', 'และ', 'เครื่องดื่ม', 'ต่าง ๆ', ' ', 'กิจกรรม', 'การ', 'ตั้งกระทู้', '+', 'การ', 'เชียร์', 'หงส์', 'สังสรรค์', 'ปาร์ตี้', ' ', 'และ', 'เครื่องดื่ม', 'ต่าง ๆ', ' ', 'กิจกรรม', 'การ', 'ตั้งกระทู้', '+', 'การ', 'เชียร์', 'หงส์', 'ปาร์ตี้', 'เครื่องดื่ม', 'ต่าง ๆ', ' ', 'สำหรับ', 'กิจกรรม', 'การ', 'ตั้งกระทู้', '+', 'การ', 'เชียร์', 'หงส์', 'ปาร์ตี้', ' ', 'เครื่องดื่ม', 'ต่าง ๆ', ' ', 'นอกจากนี้', 'กิจกรรม', 'การ', 'ตั้งกระทู้', '+', 'เป็นการ', 'เชียร์', 'หงส์', 'ปาร์ตี้', ' ', 'และ', 'เครื่องดื่ม', 'ต่าง ๆ', 'ที่', 'กิจกรรม', 'การ', 'บลัฟ', ' ', '-', ' ', '-', ' ', 'เล่น', 'ไม่', 'สร้างสรรค์', 'และ', 'ไม่', 'สร้างสรรค์', '-', ' ', 'เล่น', ' ', '-', ' ', 'เล่น', '-', ' ', 'เล่น', ' ', '-', ' ', 'เล่น', ' ', '-', 'เล่น', ' ', '-', '\"', ' ', '\\n', ' ', '\"', ' ', '-', ' ', 'เล่น', ' ', '-', ' ', 'เล่น', ' ', '-', ' ', 'เล่น', ' ', '-', ' ', 'เล่น', ' ', '-', ' ', 'เล่น', ' ', '-', ' ', 'เล่น', 'ไม่มี', '-', ' ', 'เล่น', '\"', ' ', '\\n', ' ', '\"คน', ' ', '=', ' ', 'เล่น', 'ไม่', 'สร้างสรรค์', '-', ' ', 'ไม่', 'พัฒนา', '-', '-', 'ไม่ได้', 'เล่น', '\"', ' ', '#', ' ', '\"', ' ', '\\n', ' ', '\"', ' ', '-', ' ', 'เล่น', '\"', ' ', '\\n', ' ', '\"', ' ', '-', ' ', 'เล่น', ' ', '*']\n"
     ]
    }
   ],
   "source": [
    "_text_generate(\"\"\"การตั้งกระทู้และการเชียร์บอล รวมถึงกิจกรรมทั้งหมดของชมรม ฯ เป็นไปเพื่อความบันเทิงของสมาชิกทุกๆท่าน\"\"\", tokenizer,length = 200,top_k=8, top_p=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxbos', 'ตอนนี้', 'เป็นเวลา', 'ที่', ' ', 'xxnum', ' ', 'แล้ว', 'นะ', ' ', '\\n', ' ', 'แต่', 'ถ้า', 'มี', 'เพื่อน', 'ที่', ' ', 'xxnum', ' ', '\\n', ' ', 'แล้ว', ' ', '\\n', ' ', 'ก็', 'จะ', 'มี', 'เพื่อน', 'ที่', ' ', 'xxnum', ' ', '\\n', ' ', 'แล้ว', ' ', 'ตอนนี้', 'กินเวลา', 'ได้', ' ', 'xxnum', '}', 'เลย', ' ', '\\n', ' ', 'แต่', 'ถ้า', 'มี', 'เพื่อน', 'ที่', ' ', 'xxnum', ' ', 'xxnum', ' ', '\\n', ' ', 'แล้ว', ' ', 'ตอนนี้', 'กินเวลา', 'ได้', ' ', 'xxnum', '}', 'เลย', ' ', '\\n', 'หมายเหตุ', 'ถ้า', 'มี', 'เพื่อน', 'ที่', ' ', 'xxnum', ' ', '\\n', ' ', 'แล้ว', ' ', 'ตอนนี้', 'กินเวลา', 'ได้', ' ', 'xxnum', '}', ' ', '\\n', 'หมายเหตุ', 'หมายเหตุ', ':', ' ', '\\n', ' ', 'ถ้า', 'มี', 'เพื่อน', 'ที่', ' ', 'xxnum', ' ', '/', ' ', 'xxnum', ' ', '\\n', ');', 'ใช้เวลานาน', 'ได้', ' ', 'xxnum', '}', 'เลย', ' ', 'ได้', ' ', 'xxnum', ' ', '\\n', ' ', 'ถ้า', 'มี', 'เพื่อน', 'ที่', ' ', 'xxnum', ' ', 'และ', 'xxnum', ' ', 'ใช้เวลานาน', 'ได้', ' ', 'xxnum', '}', ' ', '\\n', 'คำถาม', 'หมายเหตุ', ':', 'หมายเหตุ', ':', ' ', '\\n', 'อายุ', 'xxnum', ' ', 'ใช้เวลานาน', 'ได้', ' ', 'xxnum', ');', ' ', '/', ' ', 'ใช้เวลานาน', 'ได้', ' ', 'xxnum', ' ', '\\n', '  ', '(', 'xxnum', ')', 'ใช้เวลานาน', 'ได้', ' ', 'xxnum', ');', ' ', 'xxrep', ' ', '5', ' ', '\\n', ' ', 'แต่', 'ถ้า', 'มี', 'เพื่อน', 'แล้ว', ' ', 'xxnum', ' ', '\\n', ' ', 'ใช้เวลานาน', 'ได้', ' ', 'xxnum', ' ', '\\n', ' ', '(', 'xxnum', ')', 'ใช้เวลานาน', 'ได้', 'xxnum']\n"
     ]
    }
   ],
   "source": [
    "_text_generate(\"\"\"ตอนนี้เป็นเวลา\"\"\", tokenizer,length = 200,top_k=8, top_p=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_tokens.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48881225040001264"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_length = len(tokenizer.decode(_inputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "prompt_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1783,  275,  566,  278,  333,  275,  282,  282,  282,  282,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,  404,\n",
       "          404,  404,  404,  404,  404,  404,  404,  404,  404,  404]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(_inputs, max_length=250, do_sample=True, top_p=0.95, top_k=5)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'สวัสดีครับ                                                                                                                                                                                                                                                   '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length:]\n",
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'สวัสดีครับบบบบบbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
