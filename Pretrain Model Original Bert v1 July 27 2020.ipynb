{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How to train a language model__\tNotebook to Highlight all the steps to effectively train Transformer model on custom data\n",
    "https://github.com/huggingface/transformers/tree/master/notebooks\n",
    "\n",
    "https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb\n",
    "\n",
    "__Language Modeling__\n",
    "https://github.com/huggingface/transformers/tree/master/examples/language-modeling\n",
    "https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import CharBPETokenizer, Tokenizer, ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "# from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "import random\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast , AutoTokenizer,RobertaTokenizerFast, RobertaTokenizer\n",
    "from filelock import FileLock\n",
    "import logging\n",
    "import time\n",
    "import tqdm\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "# from concurrent.futures import ProcessPoolExecutor as Pool\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:1\")\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: nvidia-smi: not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check that PyTorch sees it\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAD_2.txt\n",
      "/datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAD_3.txt\n",
      "/datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAC_0.txt\n",
      "/datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAA_2.txt\n",
      "/datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAA_0.txt\n",
      "thwiki-20200601-extracted Amounts to a total of 566.79 MB\n",
      "/datadisk/raw_data_extraction/classification_dataset/dailynews_0.txt\n",
      "/datadisk/raw_data_extraction/classification_dataset/prachachat_0.txt\n",
      "/datadisk/raw_data_extraction/classification_dataset/thaipbs_0.txt\n",
      "/datadisk/raw_data_extraction/classification_dataset/pptv36_0.txt\n",
      "/datadisk/raw_data_extraction/classification_dataset/siamrath_0.txt\n",
      "classification_dataset Amounts to a total of 50.79 MB\n",
      "/datadisk/raw_data_extraction/another_website/pantip_430.txt\n",
      "/datadisk/raw_data_extraction/another_website/new18_2.txt\n",
      "/datadisk/raw_data_extraction/another_website/pantip_237.txt\n",
      "/datadisk/raw_data_extraction/another_website/pantip_217.txt\n",
      "/datadisk/raw_data_extraction/another_website/instagram_23.txt\n",
      "another_website Amounts to a total of 29552.82 MB\n",
      "/datadisk/raw_data_extraction/data_lm/Pantipdata_train.csv_111.txt\n",
      "/datadisk/raw_data_extraction/data_lm/Pantipdata_train.csv_330.txt\n",
      "/datadisk/raw_data_extraction/data_lm/Pantipdata_train.csv_269.txt\n",
      "/datadisk/raw_data_extraction/data_lm/Pantipdata_train.csv_90.txt\n",
      "/datadisk/raw_data_extraction/data_lm/Pantipdata_train.csv_212.txt\n",
      "Senior Project Amounts to a total of 10942.78 MB\n",
      "/datadisk/raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_1.txt\n",
      "/datadisk/raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_4.txt\n",
      "/datadisk/raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_0.txt\n",
      "/datadisk/raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_2.txt\n",
      "/datadisk/raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_3.txt\n",
      "GuruCrawler Amounts to a total of 171.21 MB\n",
      "\n",
      "I have a total of 1409 files!\n",
      "Amounts to a total of 41284.40 MB\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(\"/datadisk\")\n",
    "\n",
    "# DATA_RAW_PATH = DATA_PATH/\"raw\"\n",
    "DATA_RAW_EXTRACTED_PATH = DATA_PATH/\"raw_data_extraction\"\n",
    "\n",
    "# Output is in bytes - helper from Pathlib Path https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python\n",
    "def getStat(prev_value, cur_value):\n",
    "    if isinstance(prev_value, int):\n",
    "        return prev_value + cur_value.stat().st_size\n",
    "    return prev_value.stat().st_size + cur_value.stat().st_size\n",
    "\n",
    "# 1. The data from thwiki\n",
    "THWIKI_FOLDER = Path(\"thwiki-20200601-extracted\")\n",
    "WIKI_FILES = list((DATA_RAW_EXTRACTED_PATH/THWIKI_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , WIKI_FILES[:5]))\n",
    "print(f\"thwiki-20200601-extracted Amounts to a total of {reduce(getStat, WIKI_FILES)/1e6:.2f} MB\")\n",
    "\n",
    "# 2. The classification data from jung and ninja\n",
    "CLASSIFICATION_JUNG_NINJA_FOLDER = Path(\"classification_dataset\")\n",
    "CLASSIFICATION_FILES = list((DATA_RAW_EXTRACTED_PATH/CLASSIFICATION_JUNG_NINJA_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , CLASSIFICATION_FILES[:5]))\n",
    "print(f\"classification_dataset Amounts to a total of {reduce(getStat, CLASSIFICATION_FILES)/1e6:.2f} MB\")\n",
    "\n",
    "# 3. The Data from p'Moo Crawlers\n",
    "ANOTHER_WEBSITE_MOO_FOLDER = Path(\"another_website\")\n",
    "ANOTHER_WEBSITE_FILES = list((DATA_RAW_EXTRACTED_PATH/ANOTHER_WEBSITE_MOO_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , ANOTHER_WEBSITE_FILES[:5]))\n",
    "print(f\"another_website Amounts to a total of {reduce(getStat, ANOTHER_WEBSITE_FILES)/1e6:.2f} MB\")\n",
    "\n",
    "# 4. Senior Project Files\n",
    "SENIOR_PROJ_FOLDER = Path(\"data_lm\")\n",
    "SENIOR_PROJ_FILES = list((DATA_RAW_EXTRACTED_PATH/SENIOR_PROJ_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , SENIOR_PROJ_FILES[:5]))\n",
    "print(f\"Senior Project Amounts to a total of {reduce(getStat, SENIOR_PROJ_FILES)/1e6:.2f} MB\")\n",
    "\n",
    "# 5. Guru Crawler Files\n",
    "GURU_CRAWLER_FOLDER = Path(\"social_listening\")\n",
    "GURU_CRAWLER_FILES = list((DATA_RAW_EXTRACTED_PATH/GURU_CRAWLER_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , GURU_CRAWLER_FILES[:5]))\n",
    "print(f\"GuruCrawler Amounts to a total of {reduce(getStat, GURU_CRAWLER_FILES)/1e6:.2f} MB\")\n",
    "\n",
    "ALL_FILES = WIKI_FILES + CLASSIFICATION_FILES + ANOTHER_WEBSITE_FILES + SENIOR_PROJ_FILES + GURU_CRAWLER_FILES\n",
    "print(f\"\\nI have a total of {len(ALL_FILES)} files!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Amounts to a total of {reduce(getStat, ALL_FILES)/1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out BERT per Notebook \n",
    "\n",
    "From __HuggingFace Notebooks__ https://huggingface.co/transformers/notebooks.html: \n",
    "\n",
    "How to train a language model\tHighlight all the steps to effectively train Transformer model on custom data\n",
    "- Colab (ipynb) version : https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb\n",
    "- MD version: https://github.com/huggingface/blog/blob/master/how-to-train.md\n",
    "\n",
    "Pretrain Longformer\tHow to build a \"long\" version of existing pretrained models\tIz Beltagy  \n",
    "https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertConfig\n",
    "\n",
    "configuration = BertConfig(\n",
    "    vocab_size=30522,\n",
    "#     max_position_embeddings=512, # 512 + 2 more special tokens\n",
    "#     num_attention_heads=12,\n",
    "#     num_hidden_layers=12,\n",
    "#     type_vocab_size=1,\n",
    ")\n",
    "# configuration.vocab_size = 20000\n",
    "\n",
    "model = BertForMaskedLM(config=configuration)\n",
    "# model = RobertaForMaskedLM.from_pretrained('./Roberta/checkpoint-200000')\n",
    "\n",
    "# Accessing the model configuration\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110104890"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()\n",
    "# => 102 million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite Tokenizer of bert_itos_80k with special tokens in front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from senior_project_util import ThaiTokenizer, pre_rules_th, post_rules_th\n",
    "from fastai.text.transform import BaseTokenizer, Tokenizer, Vocab\n",
    "from fastai.text.data import TokenizeProcessor, NumericalizeProcessor\n",
    "\n",
    "TOK_PATH = Path('./senior_proj_itos')\n",
    "\n",
    "max_vocab = 80000\n",
    "\n",
    "BOS,EOS,FLD,UNK,PAD = 'xxbos','xxeos','xxfld','xxunk','xxpad'\n",
    "TK_REP,TK_WREP, TK_NUM, TK_LAUGH = 'xxrep','xxwrep', 'xxnum', 'xxlaugh'\n",
    "text_spec_tok = [UNK,PAD,BOS,EOS,FLD,TK_REP,TK_WREP, TK_NUM, TK_LAUGH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(TOK_PATH/\"bert_itos_80k.pkl\", 'rb') as f:\n",
    "#     itos = pickle.load(f)\n",
    "# len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for o in reversed(text_spec_tok):\n",
    "#     if o in itos: itos.remove(o)\n",
    "#     itos.insert(0, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itos = itos[:max_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully written vocabulary itos in senior_proj_itos/bert_itos_80k_cleaned.pkl\n"
     ]
    }
   ],
   "source": [
    "# with open(TOK_PATH/\"bert_itos_80k_cleaned.pkl\", 'wb') as f:\n",
    "#     pickle.dump(itos, f, pickle.HIGHEST_PROTOCOL)\n",
    "# print(f\"Successfully written vocabulary itos in {TOK_PATH/'bert_itos_80k_cleaned.pkl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(TOK_PATH/\"bert_itos_80k_cleaned.pkl\", 'rb') as f:\n",
    "    itos = pickle.load(f)\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'itos': ['xxunk',\n",
       "  'xxpad',\n",
       "  'xxbos',\n",
       "  'xxeos',\n",
       "  'xxfld',\n",
       "  'xxrep',\n",
       "  'xxwrep',\n",
       "  'xxnum',\n",
       "  'xxlaugh',\n",
       "  ' ',\n",
       "  'ที่',\n",
       "  'ก็',\n",
       "  'เรา',\n",
       "  'จะ',\n",
       "  'ไป',\n",
       "  'ครับ',\n",
       "  'มี',\n",
       "  'มา',\n",
       "  'ได้',\n",
       "  'แต่',\n",
       "  'ว่า',\n",
       "  'เป็น',\n",
       "  'เลย',\n",
       "  'ค่ะ',\n",
       "  'ไม่',\n",
       "  'ผม',\n",
       "  'แล้ว',\n",
       "  'และ',\n",
       "  'ให้',\n",
       "  'ๆ',\n",
       "  'ใน',\n",
       "  'ของ',\n",
       "  'คน',\n",
       "  'กับ',\n",
       "  '(',\n",
       "  ')',\n",
       "  'หรือ',\n",
       "  'มัน',\n",
       "  'นี้',\n",
       "  'กัน',\n",
       "  'มาก',\n",
       "  'อยาก',\n",
       "  'คือ',\n",
       "  'ต้อง',\n",
       "  'ด้วย',\n",
       "  'อยู่',\n",
       "  'ทำ',\n",
       "  '.',\n",
       "  'เขา',\n",
       "  '-',\n",
       "  'จาก',\n",
       "  'ถ้า',\n",
       "  'เพราะ',\n",
       "  'อะไร',\n",
       "  '3',\n",
       "  'ไม่ได้',\n",
       "  'เค้า',\n",
       "  'คะ',\n",
       "  'แบบ',\n",
       "  'ยัง',\n",
       "  '\"',\n",
       "  'ดี',\n",
       "  'เรื่อง',\n",
       "  'ดู',\n",
       "  'กว่า',\n",
       "  'ใช้',\n",
       "  'นะ',\n",
       "  'บ้าง',\n",
       "  'อีก',\n",
       "  'พอ',\n",
       "  'บาท',\n",
       "  'ไหม',\n",
       "  'เพื่อน',\n",
       "  'ขอ',\n",
       "  'ใคร',\n",
       "  'ไหน',\n",
       "  'นะคะ',\n",
       "  'บอก',\n",
       "  'เอา',\n",
       "  'ซื้อ',\n",
       "  'แฟน',\n",
       "  ':',\n",
       "  'ปี',\n",
       "  'ถึง',\n",
       "  'การ',\n",
       "  '/',\n",
       "  'ชอบ',\n",
       "  '?',\n",
       "  'ตัว',\n",
       "  'ตอนนี้',\n",
       "  'ช่วย',\n",
       "  'นั้น',\n",
       "  'หน่อย',\n",
       "  'ยังไง',\n",
       "  'วัน',\n",
       "  'แค่',\n",
       "  'ราคา',\n",
       "  'ซึ่ง',\n",
       "  'พี่',\n",
       "  'ขึ้น',\n",
       "  '5',\n",
       "  'ขอบคุณ',\n",
       "  'เคย',\n",
       "  'คุณ',\n",
       "  'ไม่มี',\n",
       "  ',',\n",
       "  'เวลา',\n",
       "  'เห็น',\n",
       "  'ไว้',\n",
       "  'ใหม่',\n",
       "  'บอ',\n",
       "  'ก่อน',\n",
       "  'แล้',\n",
       "  'เพื่อ',\n",
       "  'จน',\n",
       "  'นี่',\n",
       "  'เข้า',\n",
       "  'ประมาณ',\n",
       "  'แบบนี้',\n",
       "  'ทาง',\n",
       "  'เหมือน',\n",
       "  'ตาม',\n",
       "  'ถาม',\n",
       "  'ทำให้',\n",
       "  'มั้ย',\n",
       "  'แนะนำ',\n",
       "  'หลาย',\n",
       "  'บ้าน',\n",
       "  'อย่าง',\n",
       "  'ต่อ',\n",
       "  'เมื่อ',\n",
       "  'ขาย',\n",
       "  'โดย',\n",
       "  'หา',\n",
       "  'เล่น',\n",
       "  'ทั้ง',\n",
       "  'เอง',\n",
       "  'ว่าจะ',\n",
       "  'น่า',\n",
       "  'รถ',\n",
       "  'จริงๆ',\n",
       "  'ตอน',\n",
       "  'เจอ',\n",
       "  'เดือน',\n",
       "  'คิด',\n",
       "  'เธอ',\n",
       "  'ลง',\n",
       "  'ทำไม',\n",
       "  'เยอะ',\n",
       "  'ค่า',\n",
       "  'ไม่รู้',\n",
       "  'สามารถ',\n",
       "  'ส่ง',\n",
       "  'คุย',\n",
       "  'รู้',\n",
       "  'กลับ',\n",
       "  '4',\n",
       "  'เริ่ม',\n",
       "  'ละ',\n",
       "  'แม่',\n",
       "  'รู้สึก',\n",
       "  'สำหรับ',\n",
       "  'งาน',\n",
       "  'น้อง',\n",
       "  'ใส่',\n",
       "  'เรียน',\n",
       "  'ท่าน',\n",
       "  'เครื่อง',\n",
       "  'ตัวเอง',\n",
       "  'นึง',\n",
       "  'ถูก',\n",
       "  'คิดว่า',\n",
       "  'ลอง',\n",
       "  'ร้าน',\n",
       "  'รับ',\n",
       "  'รบกวน',\n",
       "  'ออก',\n",
       "  'ส่วน',\n",
       "  'คง',\n",
       "  'จึง',\n",
       "  'เงิน',\n",
       "  'เปิด',\n",
       "  'วันที่',\n",
       "  'หมด',\n",
       "  'ไทย',\n",
       "  'ทราบ',\n",
       "  'ควร',\n",
       "  'พอดี',\n",
       "  'ที่จะ',\n",
       "  'กิน',\n",
       "  'พวก',\n",
       "  'รอ',\n",
       "  'ติด',\n",
       "  'รัก',\n",
       "  'ไม่เคย',\n",
       "  'โดน',\n",
       "  'ทุก',\n",
       "  'วันนี้',\n",
       "  'หลัง',\n",
       "  'หนึ่ง',\n",
       "  'อ่ะ',\n",
       "  'แล้วก็',\n",
       "  'ทำงาน',\n",
       "  'แรก',\n",
       "  'เลือก',\n",
       "  'เดิน',\n",
       "  '+',\n",
       "  'โทร',\n",
       "  'ไม่ใช่',\n",
       "  '*',\n",
       "  'ผ่าน',\n",
       "  'ปกติ',\n",
       "  'กระทู้',\n",
       "  'อ่าน',\n",
       "  'ก็ได้',\n",
       "  'ห้อง',\n",
       "  'ไม่ค่อย',\n",
       "  'ช่วง',\n",
       "  'พูด',\n",
       "  'เกิด',\n",
       "  'เด็ก',\n",
       "  'เปลี่ยน',\n",
       "  'ออกมา',\n",
       "  'สอบถาม',\n",
       "  'ตอบ',\n",
       "  'สอง',\n",
       "  'ตลอด',\n",
       "  'ที',\n",
       "  'กลับมา',\n",
       "  'กลัว',\n",
       "  'อะ',\n",
       "  'ลูก',\n",
       "  'รูป',\n",
       "  '!',\n",
       "  'วิธี',\n",
       "  'กำลัง',\n",
       "  'นั่ง',\n",
       "  'นาน',\n",
       "  'เข้ามา',\n",
       "  'จบ',\n",
       "  'ความ',\n",
       "  'ไง',\n",
       "  'ตั้งแต่',\n",
       "  'น่าจะ',\n",
       "  'หนัง',\n",
       "  '“',\n",
       "  'อาจจะ',\n",
       "  'ชื่อ',\n",
       "  'ไม่ต้อง',\n",
       "  'ที่มี',\n",
       "  'อย่างไร',\n",
       "  '”',\n",
       "  'สวย',\n",
       "  'ผู้',\n",
       "  'ฟรี',\n",
       "  '%',\n",
       "  'ครั้ง',\n",
       "  'คับ',\n",
       "  '..',\n",
       "  'ภาพ',\n",
       "  'บริษัท',\n",
       "  'บาง',\n",
       "  'แจ้ง',\n",
       "  'อย่า',\n",
       "  'ผู้หญิง',\n",
       "  'เพลง',\n",
       "  'สุด',\n",
       "  'พ่อ',\n",
       "  'บน',\n",
       "  'อายุ',\n",
       "  'ใหญ่',\n",
       "  'เกี่ยวกับ',\n",
       "  'จริง',\n",
       "  'เดิม',\n",
       "  'รอบ',\n",
       "  'เพิ่ม',\n",
       "  'จ่าย',\n",
       "  'ต่าง ๆ',\n",
       "  'พร้อม',\n",
       "  'สิ่ง',\n",
       "  'เล',\n",
       "  'ข้อมูล',\n",
       "  'สี',\n",
       "  'ฟัง',\n",
       "  'อื่น',\n",
       "  'เหมือนกัน',\n",
       "  'ขนาด',\n",
       "  '#',\n",
       "  'ทุกคน',\n",
       "  'สาย',\n",
       "  'รุ่น',\n",
       "  'เกิน',\n",
       "  'เก่า',\n",
       "  'หรือเปล่า',\n",
       "  'ปล.',\n",
       "  'หาก',\n",
       "  'ฉัน',\n",
       "  'ทีม',\n",
       "  'มากกว่า',\n",
       "  'คบ',\n",
       "  'สนใจ',\n",
       "  'หนู',\n",
       "  'เข้าไป',\n",
       "  'นอน',\n",
       "  'น่ะ',\n",
       "  'ผู้ชาย',\n",
       "  'ตรง',\n",
       "  'ดีกว่า',\n",
       "  'ได้รับ',\n",
       "  'เช่น',\n",
       "  'อยู่ใน',\n",
       "  'ของเรา',\n",
       "  'กลุ่ม',\n",
       "  'สวัสดี',\n",
       "  'เกม',\n",
       "  'เปล่า',\n",
       "  'ช่อง',\n",
       "  'เสีย',\n",
       "  'ระหว่าง',\n",
       "  'ยิ่ง',\n",
       "  'คำ',\n",
       "  'ผิด',\n",
       "  'นิ้ว',\n",
       "  'กะ',\n",
       "  'พยายาม',\n",
       "  'คืน',\n",
       "  'สูง',\n",
       "  'จัด',\n",
       "  'อยากได้',\n",
       "  'เท่าไหร่',\n",
       "  ']',\n",
       "  'สัก',\n",
       "  'คนอื่น',\n",
       "  'พา',\n",
       "  'อยากรู้',\n",
       "  'ชีวิต',\n",
       "  'แอบ',\n",
       "  'สินค้า',\n",
       "  'เพิ่ง',\n",
       "  '[',\n",
       "  'นาง',\n",
       "  'ยาว',\n",
       "  'กด',\n",
       "  'สงสัย',\n",
       "  's',\n",
       "  'ลูกค้า',\n",
       "  'ปัญหา',\n",
       "  'ตั้ง',\n",
       "  'มอง',\n",
       "  'ล่ะ',\n",
       "  'ม.',\n",
       "  'เดียว',\n",
       "  'ใบ',\n",
       "  'น้ำ',\n",
       "  'บัตร',\n",
       "  'เหลือ',\n",
       "  'ที่สุด',\n",
       "  'น้อย',\n",
       "  'สุดท้าย',\n",
       "  'เบอร์',\n",
       "  'ที่ไหน',\n",
       "  'มว่า',\n",
       "  'เลิก',\n",
       "  'ด้าน',\n",
       "  'ใกล้',\n",
       "  'น.',\n",
       "  'ติดต่อ',\n",
       "  'อัน',\n",
       "  'คุยกับ',\n",
       "  'เข้าใจ',\n",
       "  'หรือไม่',\n",
       "  'สาขา',\n",
       "  'ถา',\n",
       "  'นำ',\n",
       "  'ใช่',\n",
       "  'มีปัญหา',\n",
       "  'คำแนะนำ',\n",
       "  'เสียง',\n",
       "  'มาจาก',\n",
       "  'the',\n",
       "  'หมอ',\n",
       "  'ตอนนั้น',\n",
       "  \"'\",\n",
       "  'ยนะ',\n",
       "  'ไร',\n",
       "  'พัก',\n",
       "  'แห่ง',\n",
       "  'ทุกอย่าง',\n",
       "  'ต่อไป',\n",
       "  'หาย',\n",
       "  'ระบบ',\n",
       "  'ล้าน',\n",
       "  'หลังจาก',\n",
       "  'สร้าง',\n",
       "  'เท่านั้น',\n",
       "  'ตา',\n",
       "  'เดินทาง',\n",
       "  'เนื่องจาก',\n",
       "  'ญี่ปุ่น',\n",
       "  'ทั้งหมด',\n",
       "  'ล่วงหน้า',\n",
       "  'คนเดียว',\n",
       "  'จะต้อง',\n",
       "  'อา',\n",
       "  'เก็บ',\n",
       "  'ต้องการ',\n",
       "  'ปิด',\n",
       "  'เต็ม',\n",
       "  'เที่ยว',\n",
       "  'ประเทศ',\n",
       "  't',\n",
       "  'ไปเที่ยว',\n",
       "  'จขกท',\n",
       "  'อาจ',\n",
       "  'เกือบ',\n",
       "  'กล้อง',\n",
       "  'ใจ',\n",
       "  'เสร็จ',\n",
       "  'เรียก',\n",
       "  'รู้จัก',\n",
       "  'อาหาร',\n",
       "  'วิ่ง',\n",
       "  'ลด',\n",
       "  'จ้า',\n",
       "  'ชุด',\n",
       "  'a',\n",
       "  'ที่บ้าน',\n",
       "  'ลงทะเบียน',\n",
       "  'อี',\n",
       "  'กระเป๋า',\n",
       "  'เพียง',\n",
       "  'อื่น ๆ',\n",
       "  'เขียน',\n",
       "  'อันนี้',\n",
       "  'พนักงาน',\n",
       "  'โลก',\n",
       "  'ทุกวัน',\n",
       "  'มือ',\n",
       "  'ไปหา',\n",
       "  'ง่าย',\n",
       "  'ยา',\n",
       "  'งง',\n",
       "  '**',\n",
       "  'ต่าง',\n",
       "  'สอบ',\n",
       "  'ซัก',\n",
       "  'เรื่อย ๆ',\n",
       "  'หัว',\n",
       "  'แก',\n",
       "  'ลืม',\n",
       "  'เล่า',\n",
       "  'จุด',\n",
       "  'อยู่แล้ว',\n",
       "  'คู่',\n",
       "  'ถ่าย',\n",
       "  'เมือง',\n",
       "  'แทน',\n",
       "  'ประกัน',\n",
       "  'เสื้อ',\n",
       "  '@',\n",
       "  'รวม',\n",
       "  'ครอบครัว',\n",
       "  'แนว',\n",
       "  'ตอนแรก',\n",
       "  'น่ารัก',\n",
       "  'ละคร',\n",
       "  'นาย',\n",
       "  'หุ้น',\n",
       "  'ไปดู',\n",
       "  'ที่ผ่านมา',\n",
       "  'ซะ',\n",
       "  'ไม่ทราบ',\n",
       "  'g',\n",
       "  'ชม',\n",
       "  'กี่',\n",
       "  'ยาก',\n",
       "  'ส่วนตัว',\n",
       "  'สมัคร',\n",
       "  'วัด',\n",
       "  'ปัจจุบัน',\n",
       "  'พระ',\n",
       "  'เหมือนเดิม',\n",
       "  'ชั้น',\n",
       "  'นาที',\n",
       "  'แถว',\n",
       "  'แก้',\n",
       "  'วจะ',\n",
       "  'ฝ่าย',\n",
       "  'อาทิตย์',\n",
       "  'ตาย',\n",
       "  'ใด',\n",
       "  'ทัก',\n",
       "  'แชร์',\n",
       "  'เดียวกัน',\n",
       "  'กู',\n",
       "  'ออกไป',\n",
       "  'คัน',\n",
       "  'มีอะไร',\n",
       "  'สั่ง',\n",
       "  'ดิฉัน',\n",
       "  'สาว',\n",
       "  'ตัด',\n",
       "  'รีวิว',\n",
       "  'ฝาก',\n",
       "  'ปล่อย',\n",
       "  'ทาน',\n",
       "  'x',\n",
       "  'แถม',\n",
       "  'แน่นอน',\n",
       "  'ศูนย์',\n",
       "  'บริการ',\n",
       "  'เช็ค',\n",
       "  'เน็ต',\n",
       "  'ใช้งาน',\n",
       "  'จัดส่ง',\n",
       "  'ตอนที่',\n",
       "  'สรุป',\n",
       "  'หรอก',\n",
       "  'หนัก',\n",
       "  'ตัดสินใจ',\n",
       "  'หลัก',\n",
       "  'โปร',\n",
       "  'ไม่สามารถ',\n",
       "  'รึเปล่า',\n",
       "  'บางที',\n",
       "  'ยมา',\n",
       "  'ด้วยกัน',\n",
       "  'จอง',\n",
       "  'มือถือ',\n",
       "  'อาการ',\n",
       "  'หายไป',\n",
       "  'โทรศัพท์',\n",
       "  'ไลน์',\n",
       "  'จอ',\n",
       "  'คำตอบ',\n",
       "  'ก็คือ',\n",
       "  '\\u200b',\n",
       "  'วเรา',\n",
       "  'ครบ',\n",
       "  'รายการ',\n",
       "  '_',\n",
       "  '>',\n",
       "  'ระดับ',\n",
       "  'เจ้า',\n",
       "  'ทำได้',\n",
       "  'ขา',\n",
       "  '=',\n",
       "  'มือสอง',\n",
       "  'สิ',\n",
       "  'นัด',\n",
       "  'ที่พัก',\n",
       "  'ส่วนใหญ่',\n",
       "  'เร็ว',\n",
       "  'แดง',\n",
       "  'โอเค',\n",
       "  'ณ',\n",
       "  '!!',\n",
       "  'ผู้รู้',\n",
       "  'ข่าว',\n",
       "  'คำถาม',\n",
       "  'รีบ',\n",
       "  'มากมาย',\n",
       "  'เย็น',\n",
       "  'ฯ',\n",
       "  'ดีมาก',\n",
       "  'โรงเรียน',\n",
       "  'ค่อนข้าง',\n",
       "  'ยังมี',\n",
       "  'กำลังจะ',\n",
       "  'ดัง',\n",
       "  'ดูแล',\n",
       "  'บางคน',\n",
       "  'แต่ละ',\n",
       "  'ภายใน',\n",
       "  'ออกจาก',\n",
       "  'เกิดขึ้น',\n",
       "  'ไม่ชอบ',\n",
       "  'ขับ',\n",
       "  'แรง',\n",
       "  'เว็บ',\n",
       "  'ที่นี่',\n",
       "  'คอม',\n",
       "  'สาม',\n",
       "  'ความรัก',\n",
       "  'ด่า',\n",
       "  'ไฟ',\n",
       "  'กล่อง',\n",
       "  'โอน',\n",
       "  'พ่อแม่',\n",
       "  'บ่อย',\n",
       "  'ที่มา',\n",
       "  'เนี่ย',\n",
       "  'ไม่ดี',\n",
       "  'ธนาคาร',\n",
       "  'ทิ้ง',\n",
       "  'โรงแรม',\n",
       "  'ยี่ห้อ',\n",
       "  'ข้อ',\n",
       "  'เพิ่มเติม',\n",
       "  'รายละเอียด',\n",
       "  'ความรู้สึก',\n",
       "  'มีโอกาส',\n",
       "  'เช้า',\n",
       "  'ย้าย',\n",
       "  'เอ',\n",
       "  'จำนวน',\n",
       "  'เห็นว่า',\n",
       "  'ครั้งแรก',\n",
       "  'ช่าง',\n",
       "  'ถือว่า',\n",
       "  'ค่าย',\n",
       "  'ais',\n",
       "  'ๆๆ',\n",
       "  'พบ',\n",
       "  'ได้ยิน',\n",
       "  'มาถึง',\n",
       "  'นัก',\n",
       "  'line',\n",
       "  '??',\n",
       "  'จับ',\n",
       "  'ร่วม',\n",
       "  'อีกครั้ง',\n",
       "  'ดนี้',\n",
       "  'com',\n",
       "  'แพง',\n",
       "  'ปรับ',\n",
       "  'นั่น',\n",
       "  'size',\n",
       "  'สอน',\n",
       "  'ทา',\n",
       "  'ล่าสุด',\n",
       "  'เจ้าของ',\n",
       "  'หรือว่า',\n",
       "  'ยอด',\n",
       "  'กลายเป็น',\n",
       "  'ชวน',\n",
       "  'นา',\n",
       "  'ภาค',\n",
       "  'แสน',\n",
       "  'พัน',\n",
       "  'หยุด',\n",
       "  'สภาพ',\n",
       "  'จีน',\n",
       "  'โอ',\n",
       "  'm',\n",
       "  'ครู',\n",
       "  'สมาชิก',\n",
       "  'มีอยู่',\n",
       "  'เล็ก',\n",
       "  '--',\n",
       "  'ที่แล้ว',\n",
       "  'ตก',\n",
       "  'มากขึ้น',\n",
       "  'ทั้งๆ',\n",
       "  'ควรจะ',\n",
       "  'ขอให้',\n",
       "  'สะดวก',\n",
       "  'ผล',\n",
       "  'พื้นที่',\n",
       "  'ข้าง',\n",
       "  'เกาหลี',\n",
       "  'เลี้ยง',\n",
       "  'แก้ไข',\n",
       "  'ที่ใช้',\n",
       "  'กรณี',\n",
       "  'ไอ',\n",
       "  'สามี',\n",
       "  'รักษา',\n",
       "  'ร้องไห้',\n",
       "  'ชั่วโมง',\n",
       "  'ได้ที่',\n",
       "  'ถนน',\n",
       "  'เจ้าหน้าที่',\n",
       "  'ข้อความ',\n",
       "  'ปรากฏ',\n",
       "  'ถ่ายรูป',\n",
       "  'ชาย',\n",
       "  'เก่ง',\n",
       "  'บอล',\n",
       "  'ไม่มีใคร',\n",
       "  'ดัน',\n",
       "  'สนุก',\n",
       "  'วมัน',\n",
       "  'พึ่ง',\n",
       "  'กรุงเทพ',\n",
       "  'เจ็บ',\n",
       "  'จำ',\n",
       "  'ประสบการณ์',\n",
       "  'ทะเลาะ',\n",
       "  'อย่างเดียว',\n",
       "  'คลิป',\n",
       "  '^^',\n",
       "  'เอกสาร',\n",
       "  'เครียด',\n",
       "  'ตลาด',\n",
       "  'ใช่ไหม',\n",
       "  'ไมล์',\n",
       "  'ให้ได้',\n",
       "  'หัวข้อ',\n",
       "  'ใค',\n",
       "  '&',\n",
       "  'หนังสือ',\n",
       "  'จัง',\n",
       "  'หมา',\n",
       "  'เงินเดือน',\n",
       "  'เฟส',\n",
       "  'ช่วงนี้',\n",
       "  'แน่',\n",
       "  'ต้น',\n",
       "  'รหัสสินค้า',\n",
       "  'เรียบร้อย',\n",
       "  'เน้น',\n",
       "  'บัญชี',\n",
       "  'เนื้อ',\n",
       "  'เกาะ',\n",
       "  'แสดง',\n",
       "  'ใช้ได้',\n",
       "  'รมี',\n",
       "  'แปลก',\n",
       "  'ประชาชน',\n",
       "  'ขอโทษ',\n",
       "  'งบ',\n",
       "  'อก',\n",
       "  'เติม',\n",
       "  'ยืน',\n",
       "  'iphone',\n",
       "  'ช้า',\n",
       "  'รัฐบาล',\n",
       "  'ไม่แน่ใจ',\n",
       "  'ปก',\n",
       "  'ให้กับ',\n",
       "  'พระเอก',\n",
       "  'กา',\n",
       "  'ชิ้น',\n",
       "  'ธุรกิจ',\n",
       "  'ผิว',\n",
       "  'ท้อง',\n",
       "  'เท่า',\n",
       "  'หลังจากนั้น',\n",
       "  'ตรวจ',\n",
       "  'เตรียม',\n",
       "  'คิดถึง',\n",
       "  'ที่อยู่',\n",
       "  'นอกจาก',\n",
       "  'โมง',\n",
       "  'แถ',\n",
       "  'ผ่านไป',\n",
       "  'โปรแกรม',\n",
       "  'หญิง',\n",
       "  'ไม่กล้า',\n",
       "  'แต่ง',\n",
       "  'แหละ',\n",
       "  'ประเทศไทย',\n",
       "  'ชาว',\n",
       "  'ทรู',\n",
       "  'ยู',\n",
       "  'มาก่อน',\n",
       "  'ไห',\n",
       "  'ก',\n",
       "  'บท',\n",
       "  'เหตุการณ์',\n",
       "  'เล่ม',\n",
       "  'ฉาก',\n",
       "  'กลับบ้าน',\n",
       "  'ทุกๆ',\n",
       "  'ลา',\n",
       "  'เล็กๆ',\n",
       "  'ดำ',\n",
       "  'i',\n",
       "  'ไปถึง',\n",
       "  'ร้อง',\n",
       "  'แก่',\n",
       "  'เหนื่อย',\n",
       "  'อร่อย',\n",
       "  'คนไทย',\n",
       "  'โอกาส',\n",
       "  'ทันที',\n",
       "  'ตี',\n",
       "  'ย.',\n",
       "  'แทบ',\n",
       "  'รายได้',\n",
       "  'ทริป',\n",
       "  'แมว',\n",
       "  'คอย',\n",
       "  'เผื่อ',\n",
       "  'ยื่น',\n",
       "  'หวัง',\n",
       "  'จากนั้น',\n",
       "  'หน่อ',\n",
       "  'อารมณ์',\n",
       "  'มีความสุข',\n",
       "  'b',\n",
       "  'มหาลัย',\n",
       "  'ครึ่ง',\n",
       "  'วาง',\n",
       "  'ไกล',\n",
       "  'ซ่อม',\n",
       "  'ภาษา',\n",
       "  'ต่อมา',\n",
       "  'ตั้งใจ',\n",
       "  'เฉยๆ',\n",
       "  'เป็นเรื่อง',\n",
       "  'สักพัก',\n",
       "  'ประตู',\n",
       "  'สิว',\n",
       "  'ครั้งนี้',\n",
       "  'ขาว',\n",
       "  'โครงการ',\n",
       "  'ลบ',\n",
       "  'เรียกว่า',\n",
       "  'เกินไป',\n",
       "  'ความคิด',\n",
       "  'ป้า',\n",
       "  'ขึ้นไป',\n",
       "  'สา',\n",
       "  'true',\n",
       "  'เองก็',\n",
       "  'ทน',\n",
       "  'เท่าไร',\n",
       "  'ตกลง',\n",
       "  'สัญญาณ',\n",
       "  'มส่ง',\n",
       "  'ที่ว่า',\n",
       "  'ได้มา',\n",
       "  'ทอง',\n",
       "  'แย่',\n",
       "  'ป',\n",
       "  'ฝั่ง',\n",
       "  'ติดตาม',\n",
       "  'ผ้า',\n",
       "  'ทีนี้',\n",
       "  'cr',\n",
       "  'พิเศษ',\n",
       "  'เหตุผล',\n",
       "  'ไม่ยอม',\n",
       "  'สู้',\n",
       "  'น้า',\n",
       "  'เจอกัน',\n",
       "  'เอาไว้',\n",
       "  'ใช้เวลา',\n",
       "  'ยอม',\n",
       "  'เค',\n",
       "  'วที่',\n",
       "  'เสียใจ',\n",
       "  'ดีขึ้น',\n",
       "  'ต่างประเทศ',\n",
       "  'วันนั้น',\n",
       "  'ขาด',\n",
       "  'แพ้',\n",
       "  'เพื่อให้',\n",
       "  'แอร์',\n",
       "  'ปรึกษา',\n",
       "  'ขออภัย',\n",
       "  'หน้าตา',\n",
       "  'ยอมรับ',\n",
       "  'นิดหน่อย',\n",
       "  'อย่างนี้',\n",
       "  'ตั๋ว',\n",
       "  'สังคม',\n",
       "  'คนละ',\n",
       "  'นางเอก',\n",
       "  'บางครั้ง',\n",
       "  'เบื่อ',\n",
       "  'ห่าง',\n",
       "  'ชาติ',\n",
       "  'เสริม',\n",
       "  'ของเขา',\n",
       "  'd',\n",
       "  'พันทิป',\n",
       "  'อิน',\n",
       "  'ทุกท่าน',\n",
       "  'หมื่น',\n",
       "  'กรอบ',\n",
       "  'ยบอก',\n",
       "  'ฝัน',\n",
       "  'อุปกรณ์',\n",
       "  'เส้น',\n",
       "  'ขับรถ',\n",
       "  'ธรรมดา',\n",
       "  'เดีย',\n",
       "  'ครับผม',\n",
       "  'ไม่เข้าใจ',\n",
       "  'of',\n",
       "  'ที่ทำงาน',\n",
       "  'gb',\n",
       "  'เพราะว่า',\n",
       "  'เฉพาะ',\n",
       "  'dtac',\n",
       "  'facebook',\n",
       "  'ยิ้ม',\n",
       "  'ง่ายๆ',\n",
       "  'ค่อย',\n",
       "  'ขนา',\n",
       "  'ที่ดี',\n",
       "  'มั้ง',\n",
       "  'พิมพ์',\n",
       "  'ตำแหน่ง',\n",
       "  'เดี๋ยว',\n",
       "  'จีบ',\n",
       "  'มิ',\n",
       "  'ซิม',\n",
       "  'ง.',\n",
       "  'นับ',\n",
       "  'รถยนต์',\n",
       "  'พร้อ',\n",
       "  'ตื่น',\n",
       "  'แวะ',\n",
       "  'บริเวณ',\n",
       "  'ประเด็น',\n",
       "  'ดาวน์',\n",
       "  'แยก',\n",
       "  'ยกเลิก',\n",
       "  'สมัย',\n",
       "  'เชื่อ',\n",
       "  'จนถึง',\n",
       "  'ทีวี',\n",
       "  'ครีม',\n",
       "  'มาหา',\n",
       "  'คา',\n",
       "  'อีกที',\n",
       "  'ห้าม',\n",
       "  'ทรง',\n",
       "  'คุ้ม',\n",
       "  'แน่ๆ',\n",
       "  'การเดินทาง',\n",
       "  'ยก',\n",
       "  'ตำรวจ',\n",
       "  'โดยเฉพาะ',\n",
       "  'โหลด',\n",
       "  'สัปดาห์',\n",
       "  'ทั่วไป',\n",
       "  'ใส',\n",
       "  'โต',\n",
       "  'ปาก',\n",
       "  'ไม่เจอ',\n",
       "  'ไอ้',\n",
       "  'กลาง',\n",
       "  'แล้วแต่',\n",
       "  'สัญญา',\n",
       "  'เป็นอะไร',\n",
       "  'เรื่องราว',\n",
       "  'in',\n",
       "  'หนี',\n",
       "  'ค่าใช้จ่าย',\n",
       "  'เป็นแบบ',\n",
       "  'ราย',\n",
       "  'ตรวจสอบ',\n",
       "  'คณะ',\n",
       "  'ประเภท',\n",
       "  'ต่างกัน',\n",
       "  'ทั้งสอง',\n",
       "  'สนามบิน',\n",
       "  'มีเรื่อง',\n",
       "  'อธิบาย',\n",
       "  'ไม่มีอะไร',\n",
       "  'เทียบ',\n",
       "  'คะแนน',\n",
       "  'ดังนั้น',\n",
       "  'ทั้งที่',\n",
       "  'จังหวัด',\n",
       "  'เสมอ',\n",
       "  'เว',\n",
       "  'บ่อยๆ',\n",
       "  'ตั้งกระทู้',\n",
       "  'ดังกล่าว',\n",
       "  'ตรงไหน',\n",
       "  'กำหนด',\n",
       "  'ข้าว',\n",
       "  'ประกาศ',\n",
       "  'ชา',\n",
       "  '–',\n",
       "  'แตก',\n",
       "  'เพจ',\n",
       "  'รา',\n",
       "  'ออนไลน์',\n",
       "  'สถานี',\n",
       "  'ได้ดี',\n",
       "  'วง',\n",
       "  'อันดับ',\n",
       "  'กฎหมาย',\n",
       "  'ปั่น',\n",
       "  'ฮา',\n",
       "  'บิน',\n",
       "  'อ.',\n",
       "  'อเมริกา',\n",
       "  'ยืนยัน',\n",
       "  'ถือ',\n",
       "  'รู้เรื่อง',\n",
       "  'ศึกษา',\n",
       "  'ตรงนี้',\n",
       "  'ไม่ไหว',\n",
       "  'ประกอบ',\n",
       "  'ตามที่',\n",
       "  'ไม่เห็น',\n",
       "  'ภาษาอังกฤษ',\n",
       "  'การใช้',\n",
       "  'ไม่ทัน',\n",
       "  'เมื่อวาน',\n",
       "  'ทหาร',\n",
       "  'ความเห็น',\n",
       "  ...]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocab(itos)\n",
    "vocab.__getstate__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ม่าย', 'เอา', 'เปง', 'ไง', 'บ้าง', 'น่ารัก', 'จุงเบย']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='ม่ายเอาเปงไงบ้างน่ารักจุงเบย'\n",
    "pyThai_tt = ThaiTokenizer()\n",
    "a = pyThai_tt.tokenizer(text)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ไม่', 'เอา', 'เปง', 'ไง', 'บ้าง', 'น่ารัก', 'จังเลย']]\n",
      "[[24, 78, 12028, 241, 67, 464, 3080]]\n"
     ]
    }
   ],
   "source": [
    "tt = Tokenizer(tok_func = ThaiTokenizer, lang = 'th', pre_rules = pre_rules_th, post_rules=post_rules_th, n_cpus=1)\n",
    "test_sample = tt._process_all_1([text[:1000]])\n",
    "print(test_sample)\n",
    "test_sample = [vocab.numericalize(seq) for seq in test_sample]\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.numericalize([\"asdw9eiqpwoied\"]) #UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RobertaTokenizer'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import RobertaTokenizer\n",
    "\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"./all-data-bytebpe-20000\", max_len=512)\n",
    "# tokenizer.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "['à¸ī', 'à¸±', 'à¸Ļà¹Ģà¸Ħà¸¢', 'à¹Ģà¸ģ', 'à¸·', 'à¸Ńà¸ļ', 'à¸ŀà¸¥à¸²à¸Ķ', 'à¸ª', 'à¸´à¹Ī', 'à¸ĩà¸Ĺ', 'à¸µà¹Ī', 'à¸Ķ', 'à¸µ', 'à¸Ĺ', 'à¸µà¹Ī', 'à¸ª', 'à¸¸', 'à¸Ķà¹ĥà¸Ļà¸Ĭ', 'à¸µ', 'à¸§', 'à¸´', 'à¸ķ', 'Ġà¸«à¸²à¸ģ', 'à¹ĥà¸Ļà¸§', 'à¸±', 'à¸Ļà¸Ĺ', 'à¸µà¹Ī', 'à¸ī', 'à¸±', 'à¸Ļà¸¥', 'à¹ī', 'à¸¡à¸Ńà¸¢', 'à¸¹à¹Ī', 'Ġà¹Ħà¸¡', 'à¹Ī', 'à¸¡', 'à¸µ', 'à¸«à¸Ļ', 'à¸¶à¹Ī', 'à¸ĩà¹ĥà¸Ī', 'à¸Ĥà¸Ńà¸ĩà¹Ģà¸ĺà¸Ń', 'Ġà¸Ŀ', 'à¸±', 'à¸Ļà¸Ħ', 'à¸ĩà¸Īà¸ļ', 'Ġà¸«à¸¥à¸²à¸¢', 'à¸ª', 'à¸´à¹Ī', 'à¸ĩà¸Ĺ', 'à¸µà¹Ī', 'à¸Ķ', 'à¸µ', 'à¸Ħ', 'à¸ĩà¸«à¸¡à¸Ķ', 'à¸Ĺà¸²à¸ĩ', 'à¹Ħà¸Ķ', 'à¹ī', 'à¹Ģà¸Īà¸Ń', 'Ġà¸«à¸Ļ', 'à¸¶à¹Ī', 'à¸ĩà¸ģ', 'à¸', '³', 'à¸¥', 'à¸±', 'à¸ĩà¹ĥà¸Īà¸Ĺ', 'à¸µà¹Ī', 'à¸¢', 'à¸´à¹Ī', 'à¸ĩà¹ĥà¸«à¸į', 'à¹Ī', 'Ġà¹Ħà¸¡', 'à¹Ī', 'à¸¥', 'à¸·', 'à¸¡à¹Ħà¸Ķ', 'à¹ī', 'à¹Ģà¸¥à¸¢', '...', 'Ġà¹ĥà¸Ħà¸£à¹Ģà¸Ħà¸¢à¸¡', 'à¸µ', 'à¹ģà¸Łà¸Ļà¸Ĺ', 'à¸µà¹Ī', 'à¸ģ', 'à¸´', 'à¸Ļà¸Ńà¸²à¸«à¸²à¸£', 'à¹Ħà¸¡', 'à¹Ī', 'à¸ĸ', 'à¸¹', 'à¸ģà¸Ľ', 'à¸²à¸ģà¸ģ', 'à¸±', 'à¸Ļà¹ģà¸¥', 'à¹ī', 'à¸§à¸£', 'à¸¹à¹ī', 'à¸ª', 'à¸¶', 'à¸ģà¹Ģà¸ª', 'à¸µ', 'à¸¢', 'à¸Ħà¸§à¸²à¸¡à¸ª', 'à¸¸', 'à¸Ĥ', 'à¹Ħà¸Ľà¸Ńà¸¢', 'à¹Ī', 'à¸²à¸ĩà¸Ļ', 'à¸¶', 'à¸ĩà¸ļ', 'à¹ī', 'à¸²à¸ĩà¸¡', 'à¸±à¹ī', 'à¸¢à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġ', 'Ġà¸ģ', 'à¹Ī', 'à¸Ńà¸Ļà¸Ń', 'à¸·à¹Ī', 'à¸Ļà¸ľà¸¡', 'à¸ķ', 'à¹ī', 'à¸Ńà¸ĩà¸ļà¸Ńà¸ģà¸ģ', 'à¹Ī', 'à¸Ńà¸Ļà¹Ģà¸¥à¸¢à¸§', 'à¹Ī', 'à¸²à¸Ħ', 'à¸Ļà¹Ģà¸£', 'à¸²à¸Īà¸°à¹Ģà¸¥', 'à¸·', 'à¸Ńà¸ģà¸ģ', 'à¸´', 'à¸Ļà¸Ńà¸²à¸«à¸²à¸£', 'à¹ģà¸ļà¸ļà¹Ħà¸«à¸Ļ', 'à¸Ĭà¸Ńà¸ļ', 'à¹ģà¸ļà¸ļà¹Ħà¸«à¸Ļ', 'à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¹Ģà¸£', 'à¸·à¹Ī', 'à¸Ńà¸ĩà¸Ĥà¸Ńà¸ĩ', 'à¸Ħà¸§à¸²à¸¡', 'à¸Ĭà¸Ńà¸ļà¸ª', 'à¹Ī', 'à¸§à¸Ļà¸ķ', 'à¸±', 'à¸§à¸Ļà¸°à¸Ħà¸£', 'à¸±', 'à¸ļà¸Ĺ', 'à¸¸', 'à¸ģà¸Ħà¸Ļà¸¡', 'à¸µ', 'à¸ª', 'à¸´', 'à¸Ĺà¸ĺ', 'à¸´', 'à¹ĥà¸Ļà¸ģà¸²à¸£à¹Ģà¸¥', 'à¸·', 'à¸Ńà¸ģ', 'à¸Ĥà¸Ńà¸ĩà¸Ĺ', 'à¸µà¹Ī', 'à¸Ĭà¸Ńà¸ļ', 'à¹ģà¸¥à¸°à¹Ħà¸¡', 'à¹Ī', 'à¸Ĭà¸Ńà¸ļà¸Ńà¸¢', 'à¸¹à¹Ī', 'à¹ģà¸¥', 'à¹ī', 'à¸§', 'Ġà¹ģà¸ķ', 'à¹Ī', 'à¸ľà¸¡à¸£', 'à¸¹à¹ī', 'à¸ª', 'à¸¶', 'à¸ģà¸§', 'à¹Ī', 'à¸²à¸ķà¸Ńà¸Ļà¸Ļ', 'à¸µà¹ī', 'à¸ľà¸¡à¸ģ', 'à¸', '³', 'à¸¥', 'à¸±', 'à¸ĩ', 'à¸Ľà¸£à¸°à¸ªà¸ļà¸Ľ', 'à¸±', 'à¸įà¸«à¸²à¸Ĺ', 'à¸µà¹Ī', 'à¸Ķ', 'à¸¹', 'à¹Ģà¸«à¸¡', 'à¸·', 'à¸Ńà¸Ļà¸Īà¸°', 'à¹Ģà¸¥', 'à¹ĩ', 'à¸ģà¹ģà¸ķ', 'à¹Ī', 'à¸ģà¸¥à¸²à¸¢à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¸§', 'à¹Ī', 'à¸²à¸¡', 'à¸±', 'à¸Ļà¸Ħ', 'à¹Ī', 'à¸Ńà¸Ļà¸Ĥ', 'à¹ī', 'à¸²à¸ĩà¹ĥà¸«à¸į', 'à¹Ī', 'Ġà¸ľà¸¡', 'à¸Ħà¸ļà¸ģ', 'à¸±', 'à¸ļà¹ģà¸Łà¸Ļà¸¡à¸²', '6', 'à¸Ľ', 'à¸µ', 'à¹ģà¸¥', 'à¹ī', 'à¸§à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġà¸ľà¸¡à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¸Ħà¸Ļ', 'à¸Ĭà¸Ńà¸ļà¸ģ', 'à¸´', 'à¸Ļà¸Ńà¸²à¸«à¸²à¸£', 'à¸į', 'à¸µà¹Ī', 'à¸Ľ', 'à¸¸à¹Ī', 'à¸Ļà¹ģà¸¥à¸°', 'à¸Ľà¸¥', 'à¸²à¸Ķ', 'à¸´', 'à¸ļà¹ģà¸ķ', 'à¹Ī', 'à¹ģà¸Łà¸Ļ', 'à¸ľà¸¡à¹Ħà¸¡', 'à¹Ī', 'à¸ģ', 'à¸´', 'à¸Ļà¸Ľà¸¥', 'à¸²à¸Ķ', 'à¸´', 'à¸ļà¹Ģà¸¥à¸¢', 'Ġà¸ľà¸¡à¸Ńà¸¢à¸²à¸ģ', 'à¸ģ', 'à¸´', 'à¸Ļà¸ļ', 'à¸¸', 'à¸Łà¹Ģà¸Ł', 'à¹Ī', 'à¹Ģà¸Ļ', 'à¸·à¹ī', 'à¸Ńà¹ģà¸ķ', 'à¹Ī', 'à¹ģà¸Łà¸Ļ', 'à¸ľà¸¡à¸ģ', 'à¹ĩ', 'à¹Ħà¸¡', 'à¹Ī', 'à¸ģ', 'à¸´', 'à¸Ļà¹Ģà¸Ļ', 'à¸·à¹ī', 'à¸Ń', 'Ġà¹Ģà¸£à¸²à¹Ģà¸¥à¸¢à¹Ħà¸¡', 'à¹Ī', 'à¹Ħà¸Ķ', 'à¹ī', 'à¹Ģà¸Ĥ', 'à¹ī', 'à¸²à¸Ĺ', 'à¸²à¸Ļà¸£', 'à¹ī', 'à¸²à¸Ļà¸ļ', 'à¸¸', 'à¸Łà¹Ģà¸Ł', 'à¹Ī', 'à¹Ģà¸Ļ', 'à¸·à¹ī', 'à¸Ńà¹ģà¸¥à¸°', 'à¸ļ', 'à¸¸', 'à¸Łà¹Ģà¸Ł', 'à¹Ī', 'à¸Ńà¸²à¸«à¸²à¸£', 'à¸į', 'à¸µà¹Ī', 'à¸Ľ', 'à¸¸à¹Ī', 'à¸Ļà¸ģ', 'à¸±', 'à¸Ļà¹Ģà¸ŀà¸£à¸²à¸°', 'à¸£', 'à¸¹à¹ī', 'à¸ª', 'à¸¶', 'à¸ģà¸¥', 'à¸±', 'à¸§', 'à¹ģà¸Łà¸Ļà¸ľà¸¡', 'à¸Ĺà¸²à¸Ļ', 'à¹Ħà¸¡', 'à¹Ī', 'à¸Ħ', 'à¸¸à¹ī', 'à¸¡', 'Ġà¹ģà¸¥à¸°à¹Ģà¸£', 'à¸·à¹Ī', 'à¸Ńà¸ĩà¹ĥà¸«à¸į', 'à¹Ī', 'à¹Ģà¸¥à¸¢à¸Ħ', 'à¸·', 'à¸Ńà¸ľà¸¡à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¸Ħà¸Ļà¸Ĭà¸Ńà¸ļ', 'à¸Ĺà¸²à¸Ļà¸Ńà¸²à¸«à¸²à¸£', 'à¸£à¸ª', 'à¸Ī', 'à¸±', 'à¸Ķà¹ģà¸¥à¸°', 'à¸£à¸ª', 'à¹Ģà¸ľ', 'à¹ĩ', 'à¸Ķà¸¡à¸²à¸ģ', 'Ġà¹ģà¸ķ', 'à¹Ī', 'à¹ģà¸Łà¸Ļà¸ľà¸¡', 'à¸Ĺà¸²à¸Ļ', 'à¹Ģà¸ľ', 'à¹ĩ', 'à¸Ķà¹Ħà¸¡', 'à¹Ī', 'à¹Ħà¸Ķ', 'à¹ī', 'à¹Ģà¸¥à¸¢', 'à¹Ģà¸§à¸¥à¸²', 'à¹Ģà¸£à¸²', 'à¹Ħà¸Ľà¸ģ', 'à¸´', 'à¸Ļà¸ª', 'à¹ī', 'à¸¡à¸ķ', 'à¸', '³', 'à¸ģ', 'à¸±', 'à¸Ļà¸ģ', 'à¹ĩ', 'à¸Īà¸°à¸ª', 'à¸±à¹Ī', 'à¸ĩ', 'Ġà¸ª', 'à¹ī', 'à¸¡à¸ķ', 'à¸', '³', 'à¹Ħà¸¡', 'à¹Ī', 'à¹ĥà¸ª', 'à¹Ī', 'à¸ŀà¸£', 'à¸´', 'à¸ģ', 'Ġà¸ķ', 'à¹ī', 'à¸¡', 'à¹ģà¸ĭ', 'à¹Ī', 'à¸ļà¹Ħà¸¡', 'à¹Ī', 'à¹ĥà¸ª', 'à¹Ī', 'à¸ŀà¸£', 'à¸´', 'à¸ģ', 'Ġà¸¥', 'à¸²à¸ļ', 'à¹Ħà¸¡', 'à¹Ī', 'à¹ĥà¸ª', 'à¹Ī', 'à¸ŀà¸£', 'à¸´', 'à¸ģ', 'Ġà¸£', 'à¹ī', 'à¸²à¸Ļà¸ģ', 'à¸±', 'à¸ļà¸Ĥ', 'à¹ī', 'à¸²à¸§à¸Ń', 'à¸·à¹Ī', 'à¸Ļà¹Ĩà¸ģ', 'à¹ĩ', 'à¹Ģà¸Ĭ', 'à¹Ī', 'à¸Ļà¸ģ', 'à¸±', 'à¸Ļà¹ģà¸Łà¸Ļ', 'à¸ľà¸¡', 'à¸Īà¸°à¹Ħà¸¡', 'à¹Ī', 'à¸Ĭà¸Ńà¸ļà¸ģ', 'à¸´', 'à¸Ļà¸ľ', 'à¸±', 'à¸ģà¹Ħà¸¡', 'à¹Ī', 'à¸Ħ', 'à¹Ī', 'à¸Ńà¸¢à¸ª', 'à¸±à¹Ī', 'à¸ĩà¸ģ', 'à¸±', 'à¸ļà¸Ĥ', 'à¹ī', 'à¸²à¸§à¸Ĺ', 'à¸µà¹Ī', 'à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¸ľ', 'à¸±', 'à¸ģà¹ģà¸¥', 'à¹ī', 'à¸§à¸ľà¸¡', 'à¸Ĭà¸Ńà¸ļà¸ľ', 'à¸±', 'à¸ģà¸ļ', 'à¸¸à¹ī', 'à¸ĩà¸Ĺ', 'à¸Ńà¸Ķ', 'à¸ģà¸£à¸Ńà¸ļ', 'Ġà¹Ģà¸«', 'à¹ĩ', 'à¸Ķà¸«', 'à¸Ńà¸¡', 'à¸ªà¸Ķ', 'à¸Ĺà¸Ńà¸Ķ', 'à¸¡à¸²à¸ģ', 'Ġà¹ģà¸ķ', 'à¹Ī', 'à¸ģ', 'à¹ĩ', 'à¹Ħà¸¡', 'à¹Ī', 'à¹Ħà¸Ķ', 'à¹ī', 'à¸ª', 'à¸±à¹Ī', 'à¸ĩ', 'à¹Ģà¸ŀà¸£à¸²à¸°à¸§', 'à¹Ī', 'à¸²à¹Ģà¸ĺ', 'à¸Ńà¹Ħà¸¡', 'à¹Ī', 'à¸ģ', 'à¸´', 'à¸Ļà¸ĸ', 'à¸¶', 'à¸ĩà¹Ģà¸Ħ', 'à¹ī', 'à¸²à¸Īà¸°', 'à¸ļà¸Ńà¸ģà¹ĥà¸«', 'à¹ī', 'à¸ª', 'à¸±à¹Ī', 'à¸ĩà¹Ģà¸¥à¸¢', 'à¹Ĩà¸ģ', 'à¹ĩ', 'à¹Ģà¸ĸà¸Ńà¸°', 'à¹ģà¸ķ', 'à¹Ī', 'à¸ľà¸¡à¸ģ', 'à¹ĩ', 'à¸¢', 'à¸±', 'à¸ĩà¹Ģà¸ģ', 'à¸£', 'à¸ĩà¹ĥà¸Ī', 'à¹Ģà¸ĺà¸Ńà¸Ńà¸¢', 'à¸¹à¹Ī', 'à¸Ķ', 'à¸µ', 'à¸Ń', 'à¹Ī', 'à¸°à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġà¸ľà¸¡à¸£', 'à¸¹à¹ī', 'à¸ª', 'à¸¶', 'à¸ģà¸ģ', 'à¸´', 'à¸Ļà¸Ńà¸²à¸«à¸²à¸£', 'à¹Ħà¸¡', 'à¹Ī', 'à¸¡', 'à¸µ', 'à¸Ħà¸§à¸²à¸¡à¸ª', 'à¸¸', 'à¸Ĥ', 'à¹Ģà¸¥à¸¢', 'à¸Ĭ', 'à¸µ', 'à¸§', 'à¸´', 'à¸ķà¸ľà¸¡', 'à¸Ĥà¸²à¸Ķ', 'à¸£à¸ª', 'à¹Ģà¸ľ', 'à¹ĩ', 'à¸Ķà¹Ħà¸Ľ', 'à¹Ģà¸«à¸¡', 'à¸·', 'à¸Ńà¸Ļà¸Īà¸°', 'à¸Ĥà¸²à¸Ķ', 'à¹ĥà¸Ī', 'à¹Ģà¸«à¸¡', 'à¸·', 'à¸Ńà¸Ļà¸¡', 'à¸±', 'à¸Ļà¸Ĺ', 'à¸', '³', 'à¹ĥà¸«', 'à¹ī', 'à¸Ĥà¸²à¸Ķ', 'à¸Ħà¸§à¸²à¸¡à¸ª', 'à¸¸', 'à¸Ĥ', 'à¹Ħà¸Ľà¸Ńà¸¢', 'à¹Ī', 'à¸²à¸ĩà¸Ļ', 'à¸¶', 'à¸ĩà¹Ģà¸¥à¸¢', 'à¸Ń', 'à¹Ī', 'à¸°à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġà¸¢', 'à¸´à¹Ī', 'à¸ĩà¸ĸ', 'à¹ī', 'à¸²à¹Ģà¸£à¸²', 'à¹ģà¸ķ', 'à¹Ī', 'à¸ĩà¸ĩà¸²à¸Ļà¸ģ', 'à¸±', 'à¸Ļà¹ģà¸¥', 'à¹ī', 'à¸§à¸ľà¸¡à¸ģ', 'à¹ĩ', 'à¸Ńà¸²à¸Īà¸Īà¸°à¸ķ', 'à¹ī', 'à¸Ńà¸ĩà¸¡', 'à¸µ', 'à¸Ľ', 'à¸±', 'à¸įà¸«à¸²à¹Ģà¸£', 'à¸·à¹Ī', 'à¸Ńà¸ĩà¸Ļ', 'à¸µà¹ī', 'à¸¡à¸²à¸ģà¸Ĥ', 'à¸¶à¹ī', 'à¸Ļ', 'Ġà¸ŀà¸Ń', 'à¸ľà¸¡à¹Ģà¸«', 'à¹ĩ', 'à¸Ļà¸Ħ', 'à¸¹à¹Ī', 'à¸Ĺ', 'à¸µà¹Ī', 'à¸Ĭà¸Ńà¸ļ', 'à¸Ĺà¸²à¸Ļà¸Ńà¸²à¸«à¸²à¸£', 'à¹Ģà¸«à¸¡', 'à¸·', 'à¸Ńà¸Ļà¹Ĩà¸ģ', 'à¸±', 'à¸Ļà¹Ģà¸«', 'à¹ĩ', 'à¸Ļà¹Ģà¸Ħ', 'à¹ī', 'à¸²à¸ģ', 'à¸´', 'à¸Ļà¸Ńà¸²à¸«à¸²à¸£', 'à¸ģ', 'à¸±', 'à¸Ļà¸Ńà¸¢', 'à¹Ī', 'à¸²à¸ĩà¸¡', 'à¸µ', 'à¸Ħà¸§à¸²à¸¡à¸ª', 'à¸¸', 'à¸Ĥ', 'à¹ģà¸¥', 'à¹ī', 'à¸§à¸ľà¸¡', 'à¸£', 'à¸¹à¹ī', 'à¸ª', 'à¸¶', 'à¸ģà¸Ń', 'à¸´', 'à¸Īà¸ī', 'à¸²à¸¡à¸²à¸ģà¹Ĩ', 'à¹Ģà¸¥à¸¢', 'Ġà¸¡', 'à¸µ', 'à¹ĥà¸Ħà¸£', 'à¹Ģà¸Ħà¸¢à¸¡', 'à¸µ', 'à¸Ľ', 'à¸±', 'à¸įà¸«à¸²', 'à¹ģà¸ļà¸ļ', 'à¸ľà¸¡à¸¡', 'à¸±à¹ī', 'à¸¢à¸Ħà¸£', 'à¸±', 'à¸ļà¹ģà¸¥', 'à¹ī', 'à¸§à¸Īà¸°', 'à¹ģà¸ģ', 'à¹ī', 'à¸Ľ', 'à¸±', 'à¸įà¸«à¸²à¸Ļ', 'à¸µà¹ī', 'à¸¢', 'à¸±', 'à¸ĩà¹Ħà¸ĩà¸Ķ', 'à¸µ', 'à¸Ħà¸£', 'à¸±', 'à¸ļ'] with length 643\n",
      "[560, 275, 5373, 363, 296, 349, 1647, 294, 406, 455, 302, 281, 278, 288, 302, 294, 305, 9422, 278, 279, 289, 290, 1266, 1630, 275, 368, 302, 560, 275, 843, 271, 3299, 370, 449, 266, 276, 278, 343, 440, 990, 8445, 1302, 275, 501, 12132, 2562, 294, 406, 455, 302, 281, 278, 283, 1217, 527, 332, 271, 912, 782, 440, 500, 261, 116, 284, 275, 5425, 302, 280, 406, 3013, 266, 449, 266, 284, 296, 1978, 271, 380, 642, 16378, 278, 7942, 302, 274, 289, 3645, 320, 266, 326, 304, 1439, 6178, 275, 1018, 271, 697, 345, 294, 330, 2377, 278, 280, 883, 305, 306, 3720, 266, 949, 330, 852, 271, 1373, 339, 1544, 275, 282, 225, 361, 266, 2353, 327, 2193, 290, 271, 9443, 266, 13457, 266, 354, 780, 5164, 296, 3251, 289, 3645, 4311, 596, 4311, 334, 301, 780, 327, 1426, 375, 10907, 266, 1088, 275, 5717, 275, 635, 305, 5843, 278, 294, 289, 637, 289, 6988, 296, 341, 1921, 302, 596, 3420, 266, 13802, 370, 324, 271, 279, 397, 266, 3525, 345, 294, 330, 438, 266, 3115, 340, 1179, 261, 116, 284, 275, 277, 10287, 275, 3676, 302, 281, 304, 558, 296, 2028, 338, 301, 2880, 266, 2119, 301, 531, 266, 319, 275, 501, 266, 1321, 271, 10271, 266, 530, 2631, 275, 15047, 26, 297, 278, 324, 271, 1392, 275, 282, 3901, 301, 705, 4939, 289, 3645, 356, 302, 297, 450, 1216, 553, 411, 289, 2571, 266, 746, 1925, 266, 274, 289, 4679, 411, 289, 4355, 6407, 274, 289, 718, 305, 6747, 266, 457, 426, 3358, 266, 746, 1179, 301, 320, 266, 274, 289, 1955, 426, 267, 14864, 266, 332, 271, 357, 271, 359, 2602, 271, 2926, 305, 6747, 266, 457, 426, 2906, 282, 305, 6747, 266, 1143, 356, 302, 297, 450, 456, 275, 1824, 273, 345, 294, 330, 381, 275, 279, 9399, 1591, 320, 266, 283, 619, 276, 5392, 327, 5456, 266, 889, 296, 12955, 301, 7998, 14431, 2094, 295, 275, 3242, 2094, 906, 301, 1819, 397, 266, 9399, 1591, 906, 301, 1754, 266, 332, 271, 380, 885, 459, 1385, 289, 478, 271, 814, 261, 116, 274, 275, 456, 301, 2531, 417, 277, 364, 271, 814, 261, 116, 320, 266, 656, 266, 539, 289, 274, 382, 271, 276, 2023, 266, 1397, 266, 656, 266, 539, 289, 274, 523, 480, 320, 266, 656, 266, 539, 289, 274, 409, 271, 1282, 275, 1065, 271, 3158, 327, 13024, 301, 461, 266, 456, 275, 3402, 390, 1224, 266, 4939, 289, 838, 275, 1531, 266, 283, 266, 2647, 417, 500, 275, 1065, 271, 1855, 302, 334, 301, 838, 275, 1561, 271, 2371, 6514, 275, 1564, 619, 455, 412, 2031, 651, 301, 1869, 384, 566, 8029, 396, 397, 266, 274, 301, 320, 266, 332, 271, 294, 417, 277, 4084, 266, 4760, 769, 266, 274, 289, 951, 330, 3194, 271, 465, 3148, 271, 294, 417, 2114, 2573, 301, 2005, 420, 266, 1179, 301, 280, 275, 1046, 273, 990, 16224, 370, 281, 278, 267, 266, 1232, 275, 282, 4112, 345, 294, 330, 521, 289, 3645, 320, 266, 276, 278, 883, 305, 306, 380, 307, 278, 279, 289, 18332, 1867, 2094, 906, 301, 1609, 558, 296, 2028, 1867, 400, 558, 296, 1556, 275, 368, 261, 116, 336, 271, 1867, 883, 305, 306, 3720, 266, 949, 330, 2114, 267, 266, 1232, 275, 282, 538, 406, 1253, 271, 1361, 420, 266, 4993, 275, 1018, 271, 5804, 301, 10164, 271, 813, 278, 297, 275, 4509, 327, 711, 340, 1902, 458, 264, 1202, 8652, 301, 501, 370, 288, 302, 596, 14431, 558, 296, 18548, 275, 4111, 301, 2435, 271, 310, 289, 3645, 274, 275, 730, 266, 1373, 278, 883, 305, 306, 324, 271, 2371, 273, 345, 294, 330, 1330, 289, 2460, 9131, 380, 371, 278, 512, 2064, 278, 297, 275, 1750, 454, 2947, 339, 1544, 275, 2587, 271, 2129, 568, 271, 297, 275, 4915, 340, 280, 275, 1831, 278, 333, 275, 282] with length 643\n",
      "[0, 560, 275, 5373, 363, 296, 349, 1647, 294, 406, 455, 302, 281, 278, 288, 302, 294, 305, 9422, 278, 279, 289, 290, 1266, 1630, 275, 368, 302, 560, 275, 843, 271, 3299, 370, 449, 266, 276, 278, 343, 440, 990, 8445, 1302, 275, 501, 12132, 2562, 294, 406, 455, 302, 281, 278, 283, 1217, 527, 332, 271, 912, 782, 440, 500, 261, 116, 284, 275, 5425, 302, 280, 406, 3013, 266, 449, 266, 284, 296, 1978, 271, 380, 642, 16378, 278, 7942, 302, 274, 289, 3645, 320, 266, 326, 304, 1439, 6178, 275, 1018, 271, 697, 345, 294, 330, 2377, 278, 280, 883, 305, 306, 3720, 266, 949, 330, 852, 271, 1373, 339, 1544, 275, 282, 225, 361, 266, 2353, 327, 2193, 290, 271, 9443, 266, 13457, 266, 354, 780, 5164, 296, 3251, 289, 3645, 4311, 596, 4311, 334, 301, 780, 327, 1426, 375, 10907, 266, 1088, 275, 5717, 275, 635, 305, 5843, 278, 294, 289, 637, 289, 6988, 296, 341, 1921, 302, 596, 3420, 266, 13802, 370, 324, 271, 279, 397, 266, 3525, 345, 294, 330, 438, 266, 3115, 340, 1179, 261, 116, 284, 275, 277, 10287, 275, 3676, 302, 281, 304, 558, 296, 2028, 338, 301, 2880, 266, 2119, 301, 531, 266, 319, 275, 501, 266, 1321, 271, 10271, 266, 530, 2631, 275, 15047, 26, 297, 278, 324, 271, 1392, 275, 282, 3901, 301, 705, 4939, 289, 3645, 356, 302, 297, 450, 1216, 553, 411, 289, 2571, 266, 746, 1925, 266, 274, 289, 4679, 411, 289, 4355, 6407, 274, 289, 718, 305, 6747, 266, 457, 426, 3358, 266, 746, 1179, 301, 320, 266, 274, 289, 1955, 426, 267, 14864, 266, 332, 271, 357, 271, 359, 2602, 271, 2926, 305, 6747, 266, 457, 426, 2906, 282, 305, 6747, 266, 1143, 356, 302, 297, 450, 456, 275, 1824, 273, 345, 294, 330, 381, 275, 279, 9399, 1591, 320, 266, 283, 619, 276, 5392, 327, 5456, 266, 889, 296, 12955, 301, 7998, 14431, 2094, 295, 275, 3242, 2094, 906, 301, 1819, 397, 266, 9399, 1591, 906, 301, 1754, 266, 332, 271, 380, 885, 459, 1385, 289, 478, 271, 814, 261, 116, 274, 275, 456, 301, 2531, 417, 277, 364, 271, 814, 261, 116, 320, 266, 656, 266, 539, 289, 274, 382, 271, 276, 2023, 266, 1397, 266, 656, 266, 539, 289, 274, 523, 480, 320, 266, 656, 266, 539, 289, 274, 409, 271, 1282, 275, 1065, 271, 3158, 327, 13024, 301, 461, 266, 456, 275, 3402, 390, 1224, 266, 4939, 289, 838, 275, 1531, 266, 283, 266, 2647, 417, 500, 275, 1065, 271, 1855, 302, 334, 301, 838, 275, 1561, 271, 2371, 6514, 275, 1564, 619, 455, 412, 2031, 651, 301, 1869, 384, 566, 8029, 396, 397, 266, 274, 301, 320, 266, 332, 271, 294, 417, 277, 4084, 266, 4760, 769, 266, 274, 289, 951, 330, 3194, 271, 465, 3148, 271, 294, 417, 2114, 2573, 301, 2005, 420, 266, 1179, 301, 280, 275, 1046, 273, 990, 16224, 370, 281, 278, 267, 266, 1232, 275, 282, 4112, 345, 294, 330, 521, 289, 3645, 320, 266, 276, 278, 883, 305, 306, 380, 307, 278, 279, 289, 18332, 1867, 2094, 906, 301, 1609, 558, 296, 2028, 1867, 400, 558, 296, 1556, 275, 368, 261, 116, 336, 271, 1867, 883, 305, 306, 3720, 266, 949, 330, 2114, 267, 266, 1232, 275, 282, 538, 406, 1253, 271, 1361, 420, 266, 4993, 275, 1018, 271, 5804, 301, 10164, 271, 813, 278, 297, 275, 4509, 327, 711, 340, 1902, 458, 264, 1202, 8652, 301, 501, 370, 288, 302, 596, 14431, 558, 296, 18548, 275, 4111, 301, 2435, 271, 310, 289, 3645, 274, 275, 730, 266, 1373, 278, 883, 305, 306, 324, 271, 2371, 273, 345, 294, 330, 1330, 289, 2460, 9131, 380, 371, 278, 512, 2064, 278, 297, 275, 1750, 454, 2947, 339, 1544, 275, 2587, 271, 2129, 568, 271, 297, 275, 4915, 340, 280, 275, 1831, 278, 333, 275, 282, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.cls_token_id)\n",
    "print(tokenizer.num_special_tokens_to_add())\n",
    "tokenized_text = tokenizer.tokenize(\"ฉันเคยเกือบพลาดสิ่งที่ดีที่สุดในชีวิต หากในวันที่ฉันล้มอยู่ ไม่มีหนึ่งใจของเธอ ฝันคงจบ หลายสิ่งที่ดีคงหมดทางได้เจอ หนึ่งกำลังใจที่ยิ่งใหญ่ ไม่ลืมได้เลย... ใครเคยมีแฟนที่กินอาหารไม่ถูกปากกันแล้วรู้สึกเสียความสุขไปอย่างนึงบ้างมั้ยครับ  ก่อนอื่นผมต้องบอกก่อนเลยว่าคนเราจะเลือกกินอาหารแบบไหนชอบแบบไหนเป็นเรื่องของความชอบส่วนตัวนะครับทุกคนมีสิทธิในการเลือกของที่ชอบและไม่ชอบอยู่แล้ว แต่ผมรู้สึกว่าตอนนี้ผมกำลังประสบปัญหาที่ดูเหมือนจะเล็กแต่กลายเป็นว่ามันค่อนข้างใหญ่ ผมคบกับแฟนมา6ปีแล้วครับ ผมเป็นคนชอบกินอาหารญี่ปุ่นและปลาดิบแต่แฟนผมไม่กินปลาดิบเลย ผมอยากกินบุฟเฟ่เนื้อแต่แฟนผมก็ไม่กินเนื้อ เราเลยไม่ได้เข้าทานร้านบุฟเฟ่เนื้อและบุฟเฟ่อาหารญี่ปุ่นกันเพราะรู้สึกลัวแฟนผมทานไม่คุ้ม และเรื่องใหญ่เลยคือผมเป็นคนชอบทานอาหารรสจัดและรสเผ็ดมาก แต่แฟนผมทานเผ็ดไม่ได้เลยเวลาเราไปกินส้มตำกันก็จะสั่ง ส้มตำไม่ใส่พริก ต้มแซ่บไม่ใส่พริก ลาบไม่ใส่พริก ร้านกับข้าวอื่นๆก็เช่นกันแฟนผมจะไม่ชอบกินผักไม่ค่อยสั่งกับข้าวที่เป็นผักแล้วผมชอบผักบุ้งทอดกรอบ เห็ดหอมสดทอดมาก แต่ก็ไม่ได้สั่งเพราะว่าเธอไม่กินถึงเค้าจะบอกให้สั่งเลยๆก็เถอะแต่ผมก็ยังเกรงใจเธออยู่ดีอ่ะครับ ผมรู้สึกกินอาหารไม่มีความสุขเลยชีวิตผมขาดรสเผ็ดไปเหมือนจะขาดใจเหมือนมันทำให้ขาดความสุขไปอย่างนึงเลยอ่ะครับ ยิ่งถ้าเราแต่งงานกันแล้วผมก็อาจจะต้องมีปัญหาเรื่องนี้มากขึ้น พอผมเห็นคู่ที่ชอบทานอาหารเหมือนๆกันเห็นเค้ากินอาหารกันอย่างมีความสุขแล้วผมรู้สึกอิจฉามากๆเลย มีใครเคยมีปัญหาแบบผมมั้ยครับแล้วจะแก้ปัญหานี้ยังไงดีครับ\")\n",
    "print(f\"{tokenized_text} with length {len(tokenized_text)}\")\n",
    "tokenized_text = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(f\"{tokenized_text} with length {len(tokenized_text)}\")\n",
    "print(tokenizer.build_inputs_with_special_tokens(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSeniorProjectTokenizer(object):\n",
    "    def __init__(self, TOK_PATH = Path('./senior_proj_itos'), BOS='xxbos', EOS='xxeos', FLD = 'xxfld', UNK='xxunk', PAD='xxpad',\n",
    "                 TK_REP='xxrep', TK_WREP='xxwrep', TK_NUM='xxnum', TK_LAUGH='xxlaugh', n_cpus=1,\n",
    "                ):\n",
    "        from senior_project_util import ThaiTokenizer, pre_rules_th, post_rules_th\n",
    "        from fastai.text.transform import BaseTokenizer, Tokenizer, Vocab\n",
    "        from fastai.text.data import TokenizeProcessor, NumericalizeProcessor\n",
    "\n",
    "        with open(TOK_PATH/\"bert_itos_80k_cleaned.pkl\", 'rb') as f:\n",
    "            itos = pickle.load(f)\n",
    "            \n",
    "        self.vocab = Vocab(itos)\n",
    "        self.tokenizer = Tokenizer(tok_func = ThaiTokenizer, lang = 'th', \n",
    "                                   pre_rules = pre_rules_th, post_rules=post_rules_th, n_cpus=n_cpus)\n",
    "        \n",
    "        self.cls_token_id = self.vocab.stoi[BOS]\n",
    "        self.sep_token_id = self.vocab.stoi[EOS]\n",
    "        \n",
    "#         tokenizer_processor = TokenizeProcessor(tokenizer=tt, chunksize=300000, mark_fields=False)\n",
    "#         numbericalize_processor = NumericalizeProcessor(vocab=vocab)\n",
    "        \n",
    "    def num_special_tokens_to_add(self, pair=False):\n",
    "        return 2\n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer._process_all_1([text])[0]\n",
    "#         return self.tokenizer.process_all([text])[0]\n",
    "    \n",
    "    def convert_tokens_to_ids(self, token_list):\n",
    "        return self.vocab.numericalize(token_list)\n",
    "    \n",
    "    def build_inputs_with_special_tokens(self, token_list):\n",
    "        # From https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_bert.py#L235\n",
    "        return [self.cls_token_id] + token_list + [self.sep_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "CustomSeniorProjectTokenizer\n",
      "['ใคร', 'เคย', 'มี', 'แฟน', 'ที่', 'กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'แล้ว', 'รู้สึก', 'เสีย', 'ความสุข', 'ไป', 'อย่าง', 'นึง', 'บ้าง', 'มั้ย', 'ครับ', ' ', 'ก่อนอื่น', 'ผม', 'ต้อง', 'บอก', 'ก่อน', 'เลย', 'ว่า', 'คนเรา', 'จะ', 'เลือก', 'กิน', 'อาหาร', 'แบบ', 'ไหน', 'ชอบ', 'แบบ', 'ไหน', 'เป็นเรื่อง', 'ของ', 'ความชอบ', 'ส่วนตัว', 'นะ', 'ครับ', 'ทุกคน', 'มี', 'สิทธิ', 'ใน', 'การเลือก', 'ของที่ชอบ', 'และ', 'ไม่ชอบ', 'อยู่แล้ว', ' ', 'แต่', 'ผม', 'รู้สึก', 'ว่า', 'ตอนนี้', 'ผม', 'กำลัง', 'ประสบปัญหา', 'ที่', 'ดูเหมือน', 'จะ', 'เล็ก', 'แต่', 'กลายเป็น', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', ' ', 'ผม', 'คบ', 'กับ', 'แฟน', 'มา', 'xxnum', ' ', 'ปี', 'แล้ว', 'ครับ', ' ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'กิน', 'อาหาร', 'ญี่ปุ่น', 'และ', 'ปลาดิบ', 'แต่', 'แฟน', 'ผม', 'ไม่', 'กิน', 'ปลาดิบ', 'เลย', ' ', 'ผม', 'อยากกิน', 'บุ', 'ฟเฟ่', 'เนื้อ', 'แต่', 'แฟน', 'ผม', 'ก็', 'ไม่', 'กิน', 'เนื้อ', ' ', 'เรา', 'เลย', 'ไม่ได้', 'เข้า', 'ทาน', 'ร้าน', 'บุ', 'ฟเฟ่', 'เนื้อ', 'และ', 'บุ', 'ฟเฟ่', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก', 'ลัว', 'แฟน', 'ผม', 'ทาน', 'ไม่', 'คุ้ม', ' ', 'และ', 'เรื่องใหญ่', 'เลย', 'คือ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'ทานอาหาร', 'รสจัด', 'และ', 'รส', 'เผ็ด', 'มาก', ' ', 'แต่', 'แฟน', 'ผม', 'ทาน', 'เผ็ด', 'ไม่ได้', 'เลยเวลา', 'เรา', 'ไป', 'กิน', 'ส้มตำ', 'กัน', 'ก็', 'จะ', 'สั่ง', ' ', 'ส้มตำ', 'ไม่', 'ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'ใส่', 'พริก', ' ', 'ลาบ', 'ไม่', 'ใส่', 'พริก', ' ', 'ร้าน', 'กับข้าว', 'อื่น ๆ', 'ก็', 'เช่นกัน', 'แฟน', 'ผม', 'จะ', 'ไม่ชอบ', 'กิน', 'ผัก', 'ไม่ค่อย', 'สั่ง', 'กับข้าว', 'ที่', 'เป็น', 'ผัก', 'แล้ว', 'ผม', 'ชอบ', 'ผักบุ้ง', 'ทอด', 'กรอบ', ' ', 'เห็ด', 'หอม', 'สด', 'ทอด', 'มาก', ' ', 'แต่', 'ก็', 'ไม่ได้', 'สั่ง', 'เพราะว่า', 'เธอ', 'ไม่', 'กิน', 'ถึง', 'เค้า', 'จะ', 'บอก', 'ให้', 'สั่ง', 'เลย', 'ๆ', 'ก็', 'เถอะ', 'แต่', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ', 'อยู่ดี', 'อ่ะ', 'ครับ', ' ', 'ผม', 'รู้สึก', 'กิน', 'อาหาร', 'ไม่', 'มีความสุข', 'เลย', 'ชีวิต', 'ผม', 'ขาด', 'รส', 'เผ็ด', 'ไป', 'เหมือน', 'จะ', 'ขาดใจ', 'เหมือน', 'มัน', 'ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'นึง', 'เลย', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'ถ้า', 'เรา', 'แต่งงาน', 'กัน', 'แล้ว', 'ผม', 'ก็', 'อาจ', 'จะต้อง', 'มีปัญหา', 'เรื่อง', 'นี้', 'มากขึ้น', ' ', 'พอ', 'ผม', 'เห็น', 'คู่', 'ที่', 'ชอบ', 'ทานอาหาร', 'เหมือน', 'ๆ', 'กัน', 'เห็น', 'เค้า', 'กิน', 'อาหาร', 'กัน', 'อย่างมีความสุข', 'แล้ว', 'ผม', 'รู้สึก', 'อิจฉา', 'มาก', 'ๆ', 'เลย', ' ', 'มี', 'ใคร', 'เคย', 'มีปัญหา', 'แบบผม', 'มั้ย', 'ครับ', 'แล้', 'วจะ', 'แก้ปัญหา', 'นี้', 'ยังไง', 'ดี', 'ครับ']\n",
      "[74, 102, 16, 80, 10, 189, 420, 26577, 39, 26, 160, 317, 1807, 14, 128, 169, 67, 124, 15, 9, 2090, 25, 43, 77, 111, 22, 20, 1838, 13, 204, 189, 420, 58, 75, 86, 58, 75, 801, 31, 4208, 476, 66, 15, 288, 16, 1064, 30, 2784, 41364, 27, 576, 452, 9, 19, 25, 160, 20, 89, 25, 235, 5816, 10, 1614, 13, 648, 19, 634, 20, 37, 566, 270, 9, 25, 299, 33, 80, 17, 7, 9, 82, 26, 15, 9, 25, 21, 32, 86, 189, 420, 398, 27, 10631, 19, 80, 25, 24, 189, 10631, 22, 9, 25, 3038, 3270, 0, 713, 19, 80, 25, 11, 24, 189, 713, 9, 12, 22, 55, 116, 505, 173, 3270, 0, 713, 27, 3270, 0, 420, 398, 39, 52, 160, 24486, 80, 25, 505, 24, 922, 9, 27, 5716, 22, 42, 25, 21, 32, 86, 3502, 17080, 27, 2206, 4384, 40, 9, 19, 80, 25, 505, 4384, 55, 9411, 12, 14, 189, 5963, 39, 11, 13, 498, 9, 5963, 24, 164, 3399, 9, 2587, 3727, 24, 164, 3399, 9, 11352, 24, 164, 3399, 9, 173, 4525, 431, 11, 1044, 80, 25, 13, 576, 189, 1864, 216, 498, 4525, 10, 21, 1864, 26, 25, 86, 14336, 2020, 872, 9, 4279, 1733, 1430, 2020, 40, 9, 19, 11, 55, 498, 885, 145, 24, 189, 83, 56, 13, 77, 28, 498, 22, 29, 11, 1019, 19, 25, 11, 59, 3131, 145, 1501, 200, 15, 9, 25, 160, 189, 420, 24, 790, 22, 335, 25, 847, 2206, 4384, 14, 120, 13, 14014, 120, 37, 123, 847, 1807, 14, 128, 169, 22, 200, 15, 9, 319, 51, 12, 1090, 39, 26, 25, 11, 413, 402, 375, 62, 38, 652, 9, 69, 25, 107, 453, 10, 86, 3502, 120, 29, 39, 107, 56, 189, 420, 39, 8309, 26, 25, 160, 3531, 40, 29, 22, 9, 16, 74, 102, 375, 3136, 124, 15, 112, 486, 1884, 38, 93, 61, 15]\n",
      "[2, 74, 102, 16, 80, 10, 189, 420, 26577, 39, 26, 160, 317, 1807, 14, 128, 169, 67, 124, 15, 9, 2090, 25, 43, 77, 111, 22, 20, 1838, 13, 204, 189, 420, 58, 75, 86, 58, 75, 801, 31, 4208, 476, 66, 15, 288, 16, 1064, 30, 2784, 41364, 27, 576, 452, 9, 19, 25, 160, 20, 89, 25, 235, 5816, 10, 1614, 13, 648, 19, 634, 20, 37, 566, 270, 9, 25, 299, 33, 80, 17, 7, 9, 82, 26, 15, 9, 25, 21, 32, 86, 189, 420, 398, 27, 10631, 19, 80, 25, 24, 189, 10631, 22, 9, 25, 3038, 3270, 0, 713, 19, 80, 25, 11, 24, 189, 713, 9, 12, 22, 55, 116, 505, 173, 3270, 0, 713, 27, 3270, 0, 420, 398, 39, 52, 160, 24486, 80, 25, 505, 24, 922, 9, 27, 5716, 22, 42, 25, 21, 32, 86, 3502, 17080, 27, 2206, 4384, 40, 9, 19, 80, 25, 505, 4384, 55, 9411, 12, 14, 189, 5963, 39, 11, 13, 498, 9, 5963, 24, 164, 3399, 9, 2587, 3727, 24, 164, 3399, 9, 11352, 24, 164, 3399, 9, 173, 4525, 431, 11, 1044, 80, 25, 13, 576, 189, 1864, 216, 498, 4525, 10, 21, 1864, 26, 25, 86, 14336, 2020, 872, 9, 4279, 1733, 1430, 2020, 40, 9, 19, 11, 55, 498, 885, 145, 24, 189, 83, 56, 13, 77, 28, 498, 22, 29, 11, 1019, 19, 25, 11, 59, 3131, 145, 1501, 200, 15, 9, 25, 160, 189, 420, 24, 790, 22, 335, 25, 847, 2206, 4384, 14, 120, 13, 14014, 120, 37, 123, 847, 1807, 14, 128, 169, 22, 200, 15, 9, 319, 51, 12, 1090, 39, 26, 25, 11, 413, 402, 375, 62, 38, 652, 9, 69, 25, 107, 453, 10, 86, 3502, 120, 29, 39, 107, 56, 189, 420, 39, 8309, 26, 25, 160, 3531, 40, 29, 22, 9, 16, 74, 102, 375, 3136, 124, 15, 112, 486, 1884, 38, 93, 61, 15, 3]\n"
     ]
    }
   ],
   "source": [
    "text = \"ใครเคยมีแฟนที่กินอาหารไม่ถูกปากกันแล้วรู้สึกเสียความสุขไปอย่างนึงบ้างมั้ยครับ  ก่อนอื่นผมต้องบอกก่อนเลยว่าคนเราจะเลือกกินอาหารแบบไหนชอบแบบไหนเป็นเรื่องของความชอบส่วนตัวนะครับทุกคนมีสิทธิในการเลือกของที่ชอบและไม่ชอบอยู่แล้ว แต่ผมรู้สึกว่าตอนนี้ผมกำลังประสบปัญหาที่ดูเหมือนจะเล็กแต่กลายเป็นว่ามันค่อนข้างใหญ่ ผมคบกับแฟนมา6ปีแล้วครับ ผมเป็นคนชอบกินอาหารญี่ปุ่นและปลาดิบแต่แฟนผมไม่กินปลาดิบเลย ผมอยากกินบุฟเฟ่เนื้อแต่แฟนผมก็ไม่กินเนื้อ เราเลยไม่ได้เข้าทานร้านบุฟเฟ่เนื้อและบุฟเฟ่อาหารญี่ปุ่นกันเพราะรู้สึกลัวแฟนผมทานไม่คุ้ม และเรื่องใหญ่เลยคือผมเป็นคนชอบทานอาหารรสจัดและรสเผ็ดมาก แต่แฟนผมทานเผ็ดไม่ได้เลยเวลาเราไปกินส้มตำกันก็จะสั่ง ส้มตำไม่ใส่พริก ต้มแซ่บไม่ใส่พริก ลาบไม่ใส่พริก ร้านกับข้าวอื่นๆก็เช่นกันแฟนผมจะไม่ชอบกินผักไม่ค่อยสั่งกับข้าวที่เป็นผักแล้วผมชอบผักบุ้งทอดกรอบ เห็ดหอมสดทอดมาก แต่ก็ไม่ได้สั่งเพราะว่าเธอไม่กินถึงเค้าจะบอกให้สั่งเลยๆก็เถอะแต่ผมก็ยังเกรงใจเธออยู่ดีอ่ะครับ ผมรู้สึกกินอาหารไม่มีความสุขเลยชีวิตผมขาดรสเผ็ดไปเหมือนจะขาดใจเหมือนมันทำให้ขาดความสุขไปอย่างนึงเลยอ่ะครับ ยิ่งถ้าเราแต่งงานกันแล้วผมก็อาจจะต้องมีปัญหาเรื่องนี้มากขึ้น พอผมเห็นคู่ที่ชอบทานอาหารเหมือนๆกันเห็นเค้ากินอาหารกันอย่างมีความสุขแล้วผมรู้สึกอิจฉามากๆเลย มีใครเคยมีปัญหาแบบผมมั้ยครับแล้วจะแก้ปัญหานี้ยังไงดีครับ\"\n",
    "tokenizer = CustomSeniorProjectTokenizer()\n",
    "print(tokenizer.num_special_tokens_to_add(pair=False))\n",
    "print(tokenizer.__class__.__name__)\n",
    "value = tokenizer.tokenize(text)\n",
    "print(value)\n",
    "value = tokenizer.convert_tokens_to_ids(value)\n",
    "print(value)\n",
    "value = tokenizer.build_inputs_with_special_tokens(value)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing tokenizer wrapper based on [@theblackcat102 #259](https://github.com/huggingface/tokenizers/issues/259#issuecomment-625905930)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building our dataset\n",
    "\n",
    "Build it with `from torch.utils.data.dataset import Dataset` just like [TextDataset](https://github.com/huggingface/transformers/blob/448c467256332e4be8c122a159b482c1ef039b98/src/transformers/data/datasets/language_modeling.py) and [LineByLineTextDataset](https://github.com/huggingface/transformers/blob/448c467256332e4be8c122a159b482c1ef039b98/src/transformers/data/datasets/language_modeling.py#L78)\n",
    "\n",
    "Note: Training with multiple files is currently not supported [issue/3445](https://github.com/huggingface/transformers/issues/3445)\n",
    "\n",
    "padding documentation [link](https://github.com/huggingface/tokenizers/blob/master/bindings/python/tokenizers/implementations/base_tokenizer.py#L52)\n",
    "\n",
    "Potential Improvements\n",
    "- การทำให้ Dataset นั้น dynamically tokenize + dynamically open file : ตอนนี้เวลาทำ Dataset จาก torch.utils.data.dataset จะทำการ tokenize เลยตอนอยู่ใน constructor  , กำลังคิดว่าถ้าเกิดว่า Data ใหญ่มากๆ อาจจะไม่เหมาะสมกับการทำแบบนี้  เพราะว่า Ram จะต้องมีขนาดเท่าๆกับ data ที่เราใส่เข้าไป  ซึ่งเป็นไปได้ยากหาก Data มีขนาดใหญ่มากๆ   ผมได้ทำการ Search ดูแล้วก็พบว่าจาก Discussion Forum ของ Pytorch: https://discuss.pytorch.org/t/how-to-use-a-huge-line-corpus-text-with-dataset-dataloader/30872 \n",
    "Option1: ใช้ pd.Dataframe ในการเปิด File แบบ small chunks of data https://discuss.pytorch.org/t/data-processing-as-a-batch-way/14154/4?u=ptrblck\n",
    "Option2: ใช้ byte Offsets จากไฟล์ใหญ่ๆเพื่อที่จะ lookup .seek(): https://github.com/pytorch/text/issues/130#issuecomment-510412877\n",
    "More Examples: https://github.com/pytorch/text/blob/master/torchtext/datasets/unsupervised_learning.py , https://github.com/pytorch/text/blob/a5880a3da7928dd7dd529507eec943a307204de7/examples/text_classification/iterable_train.py#L169-L214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "class TextDatasetParallel(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"         \n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, sample_path: [], block_size: int, overwrite_cache=False,\n",
    "                num_processes=8, cached_directory = \"/workdir/Code/bma_transformer_model/data/cached_data\"):\n",
    "        # assert os.path.isfile(file_path)\n",
    "        # For Loop MultiFile\n",
    "        self.examples = []\n",
    "        self.sample_path = sample_path\n",
    "#         print(f\"THIS IS SAMPLE PATH {sample_path}\")\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Set block size to be the blocksize-special tokens\n",
    "        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n",
    "        \n",
    "        self.overwrite_cache = overwrite_cache\n",
    "        self.cached_directory = cached_directory\n",
    "        if not os.path.exists(cached_directory):\n",
    "            os.makedirs(cached_directory)\n",
    "        \n",
    "        # Multiprocess for getting examples\n",
    "        with Pool(processes=num_processes) as p:\n",
    "            self.examples = list(tqdm.tqdm(p.imap(self.load_data_tokenized, self.sample_path), total=len(self.sample_path)))\n",
    "#         with Pool(max_workers=num_processes) as p:\n",
    "#             self.examples = list(tqdm.tqdm(p.map(self.load_data_tokenized, self.sample_path), total=len(self.sample_path)))\n",
    "#         for path in tqdm.tqdm(self.sample_path):\n",
    "#             self.examples.append(self.load_data_tokenized(path))\n",
    "        \n",
    "        \n",
    "        # Convert from 3d list to 2d \n",
    "        # self.examples from [[[3], [4]], [[5], [6]], [[7], [8]]] => [[3], [4], [5], [6], [7], [8]]\n",
    "        self.examples = [each_batch for each_file in self.examples for each_batch in each_file]\n",
    "        \n",
    "\n",
    "    def load_data_tokenized(self, file_path):\n",
    "#         print(f\"I AM DOING {file_path}\")\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            self.cached_directory, f\"cached_lm_{tokenizer.__class__.__name__}_{str(self.block_size)}_{filename}\",\n",
    "        )\n",
    "\n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "            if os.path.exists(cached_features_file) and not self.overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    temp_examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "            else:\n",
    "                temp_examples = []\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "#                 print(\"I finished reading \", file_path)\n",
    "                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "#                 print(\"I finished tokenizing \", file_path)\n",
    "                for i in range(0, len(tokenized_text) - self.block_size + 1, self.block_size):  # Truncate in block of block_size\n",
    "                    temp_examples.append(\n",
    "                        tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + self.block_size])\n",
    "                    )\n",
    "#                     if i%20 == 0:\n",
    "#                         print(\"I finished special tok \", file_path)\n",
    "#                 print(\"I finished every tokenizing \", file_path)\n",
    "                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n",
    "                # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
    "                # can change this behavior by adding (model specific) padding.\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(temp_examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "        return temp_examples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1811625.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(ALL_FILES[0], encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "len(text)/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer from Pretrained copied from SimpleTransformers [link](https://github.com/ThilinaRajapakse/simpletransformers/blob/master/simpletransformers/language_modeling/language_modeling_model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# logging.basicConfig(level=logging.WARN)\n",
    "tokenizer = CustomSeniorProjectTokenizer()\n",
    "dataset = TextDatasetParallel(tokenizer, \n",
    "#                               sample_path=list(map(str, ALL_FILES)), \n",
    "                              sample_path=list(map(str, GURU_CRAWLER_FILES)), \n",
    "                              block_size=512, \n",
    "                              cached_directory= \"/workdir/cached_data\",\n",
    "                              overwrite_cache=True, # make sure this is false when you have cache!!\n",
    "                              num_processes=10,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cached_directory= \"/workdir/cached_data\"\n",
    "# def load_data_tokenized(file_path):\n",
    "#     directory, filename = os.path.split(file_path)\n",
    "#     cached_features_file = os.path.join(\n",
    "#         cached_directory, f\"cached_lm_something_{str(123)}_{filename}\",\n",
    "#     )\n",
    "#     temp_examples = []\n",
    "#     with open(file_path, encoding=\"utf-8\") as f:\n",
    "#         text = f.read()\n",
    "#     print(\"I finished reading \", file_path)\n",
    "#     tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[:1000]))\n",
    "#     print(\"I finished tokenizing \", tokenized_text[:100])\n",
    "#     return f\"{file_path}+123\"\n",
    "# # Multiprocess for getting examples\n",
    "# with Pool(processes=2) as p:\n",
    "#     examples = list(tqdm.tqdm(p.imap(load_data_tokenized, list(map(str, GURU_CRAWLER_FILES))), total=len(list(map(str, GURU_CRAWLER_FILES)))))\n",
    "# print(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14033510"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,  2398,   289, 12660,   345,  6648,   295,   275,   836,   275,\n",
       "        11206, 10813,   278,   273,   278,   306,   275,   851,   317,   203,\n",
       "          295,   275,   836,   275, 11206, 10813,   278,   273,   278,   306,\n",
       "          275,   851,   317,   371,   278,  2126,   338,   296,  1208,   339,\n",
       "        15232,   289, 12660,   345,  6648,   485,  3752,   508,   294,   312,\n",
       "          613,   275,  8811,   296,  1208,   339,   277,   401,    18,   376,\n",
       "           18,  1884,    13,  1640,   278,  2398,   289, 12660,   345,  6648,\n",
       "          454,  1383,   266,  1470, 12168,   296,  1208,   339,   952,   271,\n",
       "          485,   616,  3713,   339,  1217,  6299,  1861,   417,  3466,  1603,\n",
       "         8517,   345,  6648,   515,   203,   372,   275,  2216,   302,  1371,\n",
       "          594,   302,  2591, 17573,  2020,   278,   401,    18,   376,    18,\n",
       "         1639,  3227,   604,   271,   276,   278,  1618,   275, 16404,   296,\n",
       "         1208,   339, 15232,   289, 12660,   345,  6648,   334,   301,   829,\n",
       "          339,  1651,   429,   327,   572,   275,   368,   302,  1101,  4527,\n",
       "          289,  4218,   401,    18,   376,    18,  1639,  8438,  7459,   296,\n",
       "         1208,   339,  1070,   339,   535,   340,   334,   301,   264,   660,\n",
       "         2148,   296,  1208,   339,   455,  1684,   271, 17587,   339,  1651,\n",
       "          350,   333,   339,  1445,   278,   425,  5754,     6,  9497,   275,\n",
       "          836,   275, 11206, 10813,   278,   273,   278,   306,   275,   851,\n",
       "          317,   276,   278,  2398,   289, 12660,   345,  6648,  9556,   391,\n",
       "          296,   267,   877,   276,   406,   277,   586,   377,   317,   329,\n",
       "          347,   304,   203,  2398,   289, 12660,   345,  6648,   295,   275,\n",
       "          836,   275,  1785,  9322,   203,   295,   275,   836,   275,  1785,\n",
       "         9322,   371,   278,  2126,   338,   296,  1208,   339, 15232,   289,\n",
       "        12660,   345,  6648,   485,  3752,   508,   294,   312,   613,   275,\n",
       "         8811,   296,  1208,   339,   277,   401,    18,   376,    18,  1884,\n",
       "           13,  1640,   278,  2398,   289, 12660,   345,  6648,   454,  1383,\n",
       "          266,  1470, 12168,   296,  1208,   339,   952,   271,   485,   616,\n",
       "         3713,   339,  1217,  6299,  1861,   417,  3466,  1603,  8517,   345,\n",
       "         6648,   515,   203,   372,   275,  2216,   302,  1371,   594,   302,\n",
       "         2591, 17573,  2020,   278,   401,    18,   376,    18,  1639,  3227,\n",
       "          604,   271,   276,   278,  1618,   275, 16404,   296,  1208,   339,\n",
       "        15232,   289, 12660,   345,  6648,   334,   301,   829,   339,  1651,\n",
       "          429,   327,   572,   275,   368,   302,  1101,  4527,   289,  4218,\n",
       "          401,    18,   376,    18,  1639,  8438,  7459,   296,  1208,   339,\n",
       "         1070,   339,   535,   340,   334,   301,   264,   660,  2148,   296,\n",
       "         1208,   339,   455,  1684,   271, 17587,   339,  1651,   350,   333,\n",
       "          339,  1445,   278,   425,  5754,     6,   569,  5092,   339,   744,\n",
       "         6847,   315,   275,   773,   301,   985,   278,  7691,   312,  1908,\n",
       "         1664,   440, 16316,   275,   836,   275,  1804,   278,  3770,   203,\n",
       "          297,   278,   401,    18,   376,    18,   772,   965,   401,  6847,\n",
       "         1152,  2456,   306,   458,   756,   301,   747,   275,   836,   275,\n",
       "          281,  3291,   271,   276,   278,  2148,   296,  1208,   339, 15232,\n",
       "          289, 12660,   345,  6648,  4937,   275,   836,   275,  1973,   301,\n",
       "          829,   339,  1651,   508,  6988,   296,  1208,   339,   277,   401,\n",
       "           18,   376,    18,   772,  2512,    13,  1997,   278,  2398,   289,\n",
       "        12660,   345,  6648,   307,   305,  7676,   391,   296,   267,  7435,\n",
       "          308, 19523,   301,   274,   382,   275,   264,  4626,   277,    16,\n",
       "          877,  9728,   275,   851,   317,   433,   275,   846,   305,   307,\n",
       "         2944,   271,  2883,   312,  1032,   419,   278,   460,   275,  1252,\n",
       "          289,   290,  5311,  1977,   275,  8981,   203,   274,  2858,   289,\n",
       "          267,     2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(text[:1000])\n",
    "# print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[:1000])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For text[:100000]__  \n",
    "\n",
    "Rust implementation\n",
    ">\n",
    "\n",
    "Python\n",
    ">CPU times: user 900 ms, sys: 4 ms, total: 904 ms\n",
    "Wall time: 903 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[:100000]))\n",
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For text[:1000000]__  \n",
    "\n",
    "Rust implementation\n",
    ">\n",
    "\n",
    "Python\n",
    ">CPU times: user 7.27 s, sys: 40 ms, total: 7.31 s\n",
    "Wall time: 7.31 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[:1000000]))\n",
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For text[:3000000]__  \n",
    "\n",
    "Rust implementation\n",
    ">CPU times: user 6.38 s, sys: 328 ms, total: 6.7 s  \n",
    "Wall time: 5.18 s\n",
    "\n",
    "Python\n",
    ">CPU times: user 15.5 s, sys: 72 ms, total: 15.6 s  \n",
    "Wall time: 15.6 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[:3000000]))\n",
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For text[:8000000]__  \n",
    "\n",
    "Rust implementation\n",
    ">\n",
    "\n",
    "Python\n",
    ">CPU times: user 36.1 s, sys: 340 ms, total: 36.4 s  \n",
    "Wall time: 36.4 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[:8000000]))\n",
    "# None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i_batch, sample_batched in enumerate(dataloader):\n",
    "#     print(i_batch, sample_batched)\n",
    "#     oumodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = CharBPETokenizer(vocab_file='vocab.json',merges_file ='merges.txt' )\n",
    "# no_accent_strip = BertNormalizer(strip_accents=False)\n",
    "# tokenizer._tokenizer.normalizer = no_accent_strip\n",
    "# tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "#     (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "#     (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "# )\n",
    "\n",
    "# input_ids = torch.tensor(tokenizer.encode(u\"สวัสดีครับ ผมชื่อไนท์ ตอนนี้ก็เป็นเวลาที่ผมต้องไปโรงเรียนแล้ว  นี่คือการเว้นวรรคสองทีครับ  จะได้ออกเป็นสอง Spaces\").ids).unsqueeze(0)\n",
    "# print(input_ids)\n",
    "# outputs = model(input_ids, labels=input_ids)\n",
    "# print(outputs)\n",
    "# loss, prediction_scores = outputs[:2]\n",
    "# print(loss, prediction_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.__getitem__(1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from transformers import TextDataset, LineByLineTextDataset\n",
    "\n",
    "# # dataset = LineByLineTextDataset(\n",
    "# #     tokenizer=pretrain_tokenizer,\n",
    "# #     file_path=\"../data/text/AA/wiki_01\",\n",
    "# #     block_size=128,\n",
    "# # )\n",
    "\n",
    "# dataset = TextDataset(\n",
    "#     tokenizer=pretrain_tokenizer,\n",
    "#     file_path=\"../data/text/AA/wiki_01\",\n",
    "#     block_size=128,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_doc = list(Path(\"../data/text/AA/\").glob(\"wiki*\"))[0].read_text(encoding=\"utf-8\").splitlines()\n",
    "# tokenizer = Tokenizer.from_file(\"./thwiki-sentencepiecebpe.tokenizer.json\")\n",
    "# tokenizer.encode_batch(one_doc[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_doc = list(Path(\"../data/text/AA/\").glob(\"wiki*\"))[0].read_text(encoding=\"utf-8\").splitlines()\n",
    "# tokenizer = RobertaTokenizerFast(vocab_file='vocab.json',merges_file ='merges.txt', max_len=512)\n",
    "# tokenizer.batch_encode_plus(one_doc[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(tokenizer.encode_batch(one_doc[:8])[5].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_doc[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfomers Trainer [link](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L133)\n",
    "\n",
    "```python\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer is a simple but feature-complete training and eval loop for PyTorch,\n",
    "    optimized for Transformers.\n",
    "    Args:\n",
    "        prediction_loss_only:\n",
    "            (Optional) in evaluation and prediction, only return the loss\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        args: TrainingArguments,\n",
    "        data_collator: Optional[DataCollator] = None,\n",
    "        train_dataset: Optional[Dataset] = None,\n",
    "        eval_dataset: Optional[Dataset] = None,\n",
    "        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n",
    "        prediction_loss_only=False,\n",
    "        tb_writer: Optional[\"SummaryWriter\"] = None,\n",
    "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = None,\n",
    "```\n",
    "\n",
    "[TrainingArguments](https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py#L33) is referenced here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Roberta\",\n",
    "    overwrite_output_dir=False,  #\"Use this to continue training if output_dir points to a checkpoint directory.\"\n",
    "    \n",
    "#     fp16=True,\n",
    "#     fp16_opt_level='O0',\n",
    "    \n",
    "    \n",
    "    do_train=True, #Whether to run training.\n",
    "#     do_eval=True, #Whether to run eval on the dev set.\n",
    "#     do_predict=True, # Whether to run predictions on the test set.\n",
    "    \n",
    "    num_train_epochs=200, # Total number of training epochs to perform.\n",
    "    \n",
    "    \n",
    "    per_device_train_batch_size=10, # Batch size per GPU/TPU core/CPU for training.\n",
    "#     per_device_eval_batch_size=256, # Batch size per GPU/TPU core/CPU for evaluation.\n",
    "    \n",
    "    learning_rate=5e-5,  #The initial learning rate for Adam.\n",
    "    weight_decay=0.0,\n",
    "    max_grad_norm=1.0,\n",
    "    adam_epsilon=1e-8, #Epsilon for Adam optimizer.\n",
    "    \n",
    "    #Logging\n",
    "#     logging_dir='', default_logdir -> return os.path.join(\"runs\", current_time + \"_\" + socket.gethostname())\n",
    "    logging_first_step= True,\n",
    "    logging_steps = 500,\n",
    "    \n",
    "    save_steps=10_000,  #Save checkpoint every X updates steps.\n",
    "    save_total_limit=2, #\"Limit the total amount of checkpoints. Deletes the older checkpoints in the output_dir. Default is unlimited checkpoints\n",
    "    \n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "#     eval_dataset=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed96bc781e2417b82df20588187e274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=200.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b25353537747d5b75aba95e1b168f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=233892.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./Roberta_Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
