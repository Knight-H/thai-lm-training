{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 subword algorithms help to improve your NLP model performance\n",
    "- Byte Pair Encoding (BPE)\n",
    "- WordPiece\n",
    "- Unigram Language Model\n",
    "- SentencePiece  \n",
    "\n",
    "Subword balances vocabulary size and footprint. Extreme case is we can only use 26 token (i.e. character) to present all English word. 16k or 32k subwords are recommended vocabulary size to have a good result.\n",
    "\n",
    "Many Asian language word cannot be separated by space. Therefore, the initial vocabulary is larger than English a lot. You may need to prepare over 10k initial word to kick start the word segmentation. From Schuster and Nakajima research, they propose to use 22k word and 11k word for Japanese and Korean respectively.  \n",
    "https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform subword tokenization, BPE is slightly modified in its implementation such that the frequently occurring subword pairs are merged together instead of being replaced by another byte to enable compression. This would basically lead the rare word athazagoraphobia to be split up into more frequent subwords such as ['▁ath', 'az', 'agor', 'aphobia'].\n",
    "https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers: How machines read - 28 JANUARY 2020\n",
    "Recommended read on tokenization  \n",
    "\n",
    "- **BPE**: Just uses the frequency of occurrences to identify the best match at every iteration until it reaches the predefined vocabulary size.\n",
    "- **WordPiece**: Similar to BPE and uses frequency occurrences to identify potential merges but makes the final decision based on the likelihood of the merged token\n",
    "- **Unigram**: A fully probabilistic model which does not use frequency occurrences. Instead, it trains a LM using a probabilistic model, removing the token which improves the overall likelihood the least and then starting over until it reaches the final token limit.\n",
    "- **SentencePiece** basically tries to bring all the subword tokenization tools and techniques under one banner. _\" SentencePiece is a re-implementation of sub-word units, an effective way to alleviate the open vocabulary problems in neural machine translation. SentencePiece supports two segmentation algorithms, byte-pair-encoding (BPE) [Sennrich et al.] and unigram language model [Kudo.]. \"_ (BPE and Unigram are reimplemented with improvements).\n",
    "    - __All other models assume input is already tokenized__: BPE and Unigram are great models but they share one big disadvantage- they both need to have their input already tokenized. BPE needs to have the input tokenized so that every character (including word-boundary characters) are tokenized. Only then can BPE count frequencies and start to merge tokens. Usually this is done by simply doing word level tokenization but, as we discussed earlier, this is a problem with tokenization since not all languages are space segmented. Similarly, the unigram model needs to have its input tokenized before it can start discarding tokens based on their probability distribution. SentencePiece deals with this by simply taking in an input in raw text and then doing everything (which we will discuss below) needed on that input to perform subword tokenization.\n",
    "    - __Encode everything as unicode ...__: SentencePiece first converts all the input into unicode characters. This means it doesn’t have to worry about different languages or characters or symbols. If it uses unicode it can just treat all input in the same way, which allows it to be language agnostic\n",
    "    - __… including  the spaces__: To get around the word segmenting issues, SentencePiece simply encodes spaces as a unicode symbol. Specifically it encodes it as unicode value U+2581 (underscore ‘_’ to those of us who don’t speak unicode). This helps with the language agnostic issues and the decoding issue. Since spaces are unicode encoded then they can be easily reversed or decoded and treated (i.e learned) like a normal language character. It sounds like a simple approach and I guess it is, but the best ideas tend to seem that way in the end\n",
    "\n",
    "\n",
    "https://blog.floydhub.com/tokenization-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Huggingface `tokenizers`__ : \n",
    "Provided Tokenizers\n",
    "- CharBPETokenizer: The original BPE\n",
    "- ByteLevelBPETokenizer: The byte level version of the BPE\n",
    "- SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
    "- BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece  \n",
    " \n",
    "We designed the library so that it provides all the required blocks to create end-to-end tokenizers in an interchangeable way. In that sense, we provide\n",
    "these various components: \n",
    "\n",
    "- **Normalizer**: Executes all the initial transformations over the initial input string. For example when you need to\n",
    "lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer. \n",
    "- **PreTokenizer**: In charge of splitting the initial input string. That's the component that decides where and how to\n",
    "pre-segment the origin string. The simplest example would be like we saw before, to simply split on spaces.\n",
    "- **Model**: Handles all the sub-token discovery and generation, this part is trainable and really dependant\n",
    " of your input data.\n",
    "- **Post-Processor**: Provides advanced construction features to be compatible with some of the Transformers-based SoTA\n",
    "models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens.\n",
    "- **Decoder**: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according\n",
    "to the `PreTokenizer` we used previously.\n",
    "- **Trainer**: Provides training capabilities to each model. \n",
    "\n",
    "Notebook for Tokenizers: https://github.com/huggingface/transformers/blob/master/notebooks/01-training-tokenizers.ipynb  \n",
    "Github Link for Python Binding: https://github.com/huggingface/tokenizers/tree/master/bindings/python\n",
    "\n",
    "Implementation: https://github.com/huggingface/tokenizers/tree/master/bindings/python/tokenizers/implementations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (0.8.0rc3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                            CharBPETokenizer,\n",
    "                            SentencePieceBPETokenizer,\n",
    "                            BertWordPieceTokenizer)\n",
    "from tokenizers.normalizers import BertNormalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../raw_data_extraction/thwiki-20200601-extracted/WikiAD_3.txt\n",
      "../raw_data_extraction/thwiki-20200601-extracted/WikiAE_3.txt\n",
      "../raw_data_extraction/thwiki-20200601-extracted/WikiAF_0.txt\n",
      "../raw_data_extraction/thwiki-20200601-extracted/WikiAF_2.txt\n",
      "../raw_data_extraction/thwiki-20200601-extracted/WikiAD_1.txt\n",
      "../raw_data_extraction/classification_dataset/dailynews_0.txt\n",
      "../raw_data_extraction/classification_dataset/pptv36_0.txt\n",
      "../raw_data_extraction/classification_dataset/prbangkok_0.txt\n",
      "../raw_data_extraction/classification_dataset/siamrath_0.txt\n",
      "../raw_data_extraction/classification_dataset/springnews_0.txt\n",
      "../raw_data_extraction/another_website/khaosod_16.txt\n",
      "../raw_data_extraction/another_website/pantip_470.txt\n",
      "../raw_data_extraction/another_website/pantip_415.txt\n",
      "../raw_data_extraction/another_website/naewna_2.txt\n",
      "../raw_data_extraction/another_website/brighttv_5.txt\n",
      "../raw_data_extraction/data_lm/Pantipdata_train.csv_351.txt\n",
      "../raw_data_extraction/data_lm/Pantipdata_train.csv_81.txt\n",
      "../raw_data_extraction/data_lm/Pantipdata_train.csv_64.txt\n",
      "../raw_data_extraction/data_lm/Pantipdata_train.csv_118.txt\n",
      "../raw_data_extraction/data_lm/Pantipdata_train.csv_176.txt\n",
      "../raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_5.txt\n",
      "../raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_1.txt\n",
      "../raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_3.txt\n",
      "../raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_2.txt\n",
      "../raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_0.txt\n",
      "\n",
      "I have a total of 1409 files!\n",
      "Amounts to a total of 41284.40 MB\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(\"..\")\n",
    "\n",
    "# DATA_RAW_PATH = DATA_PATH/\"raw\"\n",
    "DATA_RAW_EXTRACTED_PATH = DATA_PATH/\"raw_data_extraction\"\n",
    "\n",
    "# 1. The data from thwiki\n",
    "THWIKI_FOLDER = Path(\"thwiki-20200601-extracted\")\n",
    "WIKI_FILES = list((DATA_RAW_EXTRACTED_PATH/THWIKI_FOLDER).glob(\"Wiki*.txt\"))\n",
    "list(map(print , WIKI_FILES[:5]))\n",
    "\n",
    "\n",
    "# 2. The classification data from jung and ninja\n",
    "CLASSIFICATION_JUNG_NINJA_FOLDER = Path(\"classification_dataset\")\n",
    "CLASSIFICATION_FILES = list((DATA_RAW_EXTRACTED_PATH/CLASSIFICATION_JUNG_NINJA_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , CLASSIFICATION_FILES[:5]))\n",
    "\n",
    "# 3. The Data from p'Moo Crawlers\n",
    "ANOTHER_WEBSITE_MOO_FOLDER = Path(\"another_website\")\n",
    "ANOTHER_WEBSITE_FILES = list((DATA_RAW_EXTRACTED_PATH/ANOTHER_WEBSITE_MOO_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , ANOTHER_WEBSITE_FILES[:5]))\n",
    "\n",
    "\n",
    "# 4. Senior Project Files\n",
    "SENIOR_PROJ_FOLDER = Path(\"data_lm\")\n",
    "SENIOR_PROJ_FILES = list((DATA_RAW_EXTRACTED_PATH/SENIOR_PROJ_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , SENIOR_PROJ_FILES[:5]))\n",
    "\n",
    "# 5. Guru Crawler Files\n",
    "GURU_CRAWLER_FOLDER = Path(\"social_listening\")\n",
    "GURU_CRAWLER_FILES = list((DATA_RAW_EXTRACTED_PATH/GURU_CRAWLER_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , GURU_CRAWLER_FILES[:5]))\n",
    "\n",
    "ALL_FILES = WIKI_FILES + CLASSIFICATION_FILES + ANOTHER_WEBSITE_FILES + SENIOR_PROJ_FILES + GURU_CRAWLER_FILES\n",
    "print(f\"\\nI have a total of {len(ALL_FILES)} files!\")\n",
    "\n",
    "\n",
    "# Output is in bytes - helper from Pathlib Path https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python\n",
    "def getStat(prev_value, cur_value):\n",
    "    if isinstance(prev_value, int):\n",
    "        return prev_value + cur_value.stat().st_size\n",
    "    return prev_value.stat().st_size + cur_value.stat().st_size\n",
    "\n",
    "from functools import reduce\n",
    "print(f\"Amounts to a total of {reduce(getStat, ALL_FILES)/1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[file for file in list(map(str, ALL_FILES)) if os.path.isdir(file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chardet\n",
    "# for filename in list(map(str, ALL_FILES))[::-1]:\n",
    "#     with open(filename, 'rb') as f:\n",
    "#         content_bytes = f.read()\n",
    "#     detected = chardet.detect(content_bytes)\n",
    "#     encoding = detected['encoding']\n",
    "#     print(f\"{filename}: detected as {encoding}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../raw_data_extraction/classification_dataset/dailynews_0.txt'),\n",
       " PosixPath('../raw_data_extraction/classification_dataset/pptv36_0.txt'),\n",
       " PosixPath('../raw_data_extraction/classification_dataset/prbangkok_0.txt'),\n",
       " PosixPath('../raw_data_extraction/classification_dataset/siamrath_0.txt'),\n",
       " PosixPath('../raw_data_extraction/classification_dataset/springnews_0.txt'),\n",
       " PosixPath('../raw_data_extraction/classification_dataset/naewna_0.txt'),\n",
       " PosixPath('../raw_data_extraction/classification_dataset/thaipbs_0.txt'),\n",
       " PosixPath('../raw_data_extraction/classification_dataset/prachachat_0.txt')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list((DATA_RAW_EXTRACTED_PATH/CLASSIFICATION_JUNG_NINJA_FOLDER).glob(\"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train `ByteLevelBPETokenizer` [link](https://github.com/huggingface/tokenizers/blob/master/bindings/python/tokenizers/implementations/byte_level_bpe.py#L9)\n",
    "\n",
    " ```python\n",
    "class ByteLevelBPETokenizer(BaseTokenizer):\n",
    "    \"\"\" ByteLevelBPETokenizer\n",
    "    Represents a Byte-level BPE as introduced by OpenAI with their GPT-2 model\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file: Optional[str] = None,\n",
    "        merges_file: Optional[str] = None,\n",
    "        add_prefix_space: bool = False, \n",
    "        lowercase: bool = False,\n",
    "        dropout: Optional[float] = None,\n",
    "        unicode_normalizer: Optional[str] = None,\n",
    "        continuing_subword_prefix: Optional[str] = None,\n",
    "        end_of_word_suffix: Optional[str] = None,\n",
    "        trim_offsets: bool = False,\n",
    "```\n",
    "- [`add_prefix_space`](https://github.com/huggingface/tokenizers/blob/74d812d40180032d2dbb6ca59e2e10f0257ef46b/bindings/python/tokenizers/pre_tokenizers/__init__.pyi#L26) Whether to add a space to the first word if there isn't already one. This\n",
    "lets us treat `hello` exactly like `say hello`.\n",
    "- [`continuing_subword_prefix`](https://github.com/huggingface/tokenizers/blob/master/bindings/python/tokenizers/implementations/byte_level_bpe.py#L33) and `end_of_word_suffix` are suffixes to add when there are words/continuing words\n",
    "- [`trim_offsets`](https://github.com/huggingface/tokenizers/blob/74d812d40180032d2dbb6ca59e2e10f0257ef46b/bindings/python/tokenizers/processors/__init__.pyi#L84) This post-processor takes care of trimming the offsets. By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don't want the offsets to include these whitespaces, then this PostProcessor must be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is tokenizer that I will train:  Tokenizer(vocabulary_size=0, model=ByteLevelBPE, add_prefix_space=True, lowercase=True, dropout=None, unicode_normalizer=nfkc, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=True,                 # adds lowercase to normalizer\n",
    "                                  unicode_normalizer=\"nfkc\",      # adds unicode normalizer nfkc to normalizer\n",
    "                                  add_prefix_space=True,          # add space to the first word\n",
    "                                 )\n",
    "print(\"This is tokenizer that I will train: \", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Tokens\n",
    "> SentencePiece reserves vocabulary ids for special meta symbols, e.g., unknown symbol (<unk\\>), BOS (<s\\>), EOS (</s\\>) and padding (<pad\\>). Their actual ids are configured with command line flags. We can also define custom meta symbols to encode contextual information as virtual tokens. Examples include the language- indicators, <2ja> and <2de>, for multilingual models\n",
    "\n",
    "-- From SentencePiece Paper\n",
    "\n",
    "Bert uses the special tokens `[UNK] [CLS] [SEP] [PAD] [MASK]`\n",
    "\n",
    "- Unknown: `[UNK]` `<unk>`\n",
    "- Beginning of Sentence (BOS): `[CLS]` `<s>`\n",
    "- Ending of Sentence (EOS): `[SEP]` `</s>`\n",
    "- Padding: `[PAD]`  `<pad>`\n",
    "- Mask: `[MASK]` `<mask>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained vocab size: 20000\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(files=list(map(str, ALL_FILES)), \n",
    "                vocab_size=20000, \n",
    "                min_frequency=2,\n",
    "                show_progress=True,\n",
    "                special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"],\n",
    "               )\n",
    "print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘all-data-bytebpe-20000’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir all-data-bytebpe-20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tokenizers.Tokenizer' object has no attribute 'save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-238981ed5fcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# And finally save it somewhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ! mkdir all-data-bytebpe-30522\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all-data-bytebpe-20000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./all-data-bytebpe-20000.tokenizer.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tokenizers.Tokenizer' object has no attribute 'save_model'"
     ]
    }
   ],
   "source": [
    "# And finally save it somewhere\n",
    "# ! mkdir all-data-bytebpe-30522\n",
    "tokenizer.save_model(\"all-data-bytebpe-20000\")\n",
    "tokenizer.save(\"./all-data-bytebpe-20000.tokenizer.json\", pretty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tokenize on Pantip Sample ใครเคยมีแฟนที่กินอาหารไม่ถูกปากกันแล้วรู้สึกเสียความสุขไปอย่างนึงบ้างมั้ยครับ\n",
    "https://pantip.com/topic/40006922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1125, 275, 566, 278, 333, 275, 282, 16456, 327, 2565, 368, 317, 830, 340, 274, 301, 334, 301, 13155, 302, 3555, 271, 1303, 1468, 278, 6036, 271, 279, 225, 407, 302, 283, 296, 1273, 444, 271, 19117, 785, 14369, 278, 333, 275, 282, 225, 1986, 271, 12879, 301, 11970, 1474, 793, 1050]\n",
      "['Ġà¸ªà¸§', 'à¸±', 'à¸ªà¸Ķ', 'à¸µ', 'à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġà¸ľà¸¡à¸Ĭ', 'à¸·à¹Ī', 'à¸Ńà¹Ħ', 'à¸Ļà¸Ĺ', 'à¹Į', 'Ġà¸ķà¸Ńà¸Ļà¸Ļ', 'à¸µà¹ī', 'à¸ģ', 'à¹ĩ', 'à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¹Ģà¸§à¸¥à¸²à¸Ĺ', 'à¸µà¹Ī', 'à¸ľà¸¡à¸ķ', 'à¹ī', 'à¸Ńà¸ĩà¹Ħà¸Ľ', 'à¹Ĥà¸£à¸ĩà¹Ģà¸£', 'à¸µ', 'à¸¢à¸Ļà¹ģà¸¥', 'à¹ī', 'à¸§', 'Ġ', 'Ġà¸Ļ', 'à¸µà¹Ī', 'à¸Ħ', 'à¸·', 'à¸Ńà¸ģà¸²à¸£', 'à¹Ģà¸§', 'à¹ī', 'à¸Ļà¸§à¸£', 'à¸£à¸Ħ', 'à¸ªà¸Ńà¸ĩà¸Ĺ', 'à¸µ', 'à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġ', 'Ġà¸Īà¸°à¹Ħà¸Ķ', 'à¹ī', 'à¸Ńà¸Ńà¸ģà¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¸ªà¸Ńà¸ĩ', 'Ġsp', 'ac', 'es']\n",
      "[' สว', 'ั', 'สด', 'ี', 'คร', 'ั', 'บ', ' ผมช', 'ื่', 'อไ', 'นท', '์', ' ตอนน', 'ี้', 'ก', '็', 'เป', '็', 'นเวลาท', 'ี่', 'ผมต', '้', 'องไป', 'โรงเร', 'ี', 'ยนแล', '้', 'ว', ' ', ' น', 'ี่', 'ค', 'ื', 'อการ', 'เว', '้', 'นวร', 'รค', 'สองท', 'ี', 'คร', 'ั', 'บ', ' ', ' จะได', '้', 'ออกเป', '็', 'นสอง', ' sp', 'ac', 'es']\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(u\"สวัสดีครับ ผมชื่อไนท์ ตอนนี้ก็เป็นเวลาที่ผมต้องไปโรงเรียนแล้ว  นี่คือการเว้นวรรคสองทีครับ  จะได้ออกเป็นสอง Spaces\")\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)\n",
    "print(list(map(lambda x : tokenizer.decode([x]), encoded.ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[622, 15891, 6731, 837, 578, 12051, 1187, 18354, 18, 8937, 2931, 6731, 2372, 75, 638, 3830, 81, 3256, 9298, 932, 1286, 1677, 714, 1036, 9480, 3339, 731, 3169, 18]\n",
      "['Ġh', 'ello', 'Ġthis', 'is', 'Ġa', 'Ġtest', 'Ġin', 'Ġenglish', '.', 'Ġhow', 'Ġis', 'Ġthis', 'Ġal', 'g', 'or', 'ith', 'm', 'Ġle', 'arn', 'ing', '??', 'Ġi', 'Ġd', 'un', 'no', 'Ġas', 'Ġw', 'ell', '.']\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(u\"Hello Thisis a test in English. How is this algorithm learning?? I dunno as well.\")\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tokenize on Pantip Sample ใครเคยมีแฟนที่กินอาหารไม่ถูกปากกันแล้วรู้สึกเสียความสุขไปอย่างนึงบ้างมั้ยครับ\n",
    "https://pantip.com/topic/40006922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16378, 278, 7942, 302, 274, 289, 3645, 320, 266, 326, 304, 1439, 6178, 275, 1018, 271, 697, 345, 294, 330, 2377, 278, 280, 883, 305, 306, 3720, 266, 949, 330, 852, 271, 1373, 339, 1544, 275, 282, 225, 361, 266, 2353, 327, 2193, 290, 271, 9443, 266, 13457, 266, 354, 780, 5164, 296, 3251, 289, 3645, 4311, 596, 4311, 334, 301, 780, 327, 1426, 375, 10907, 266, 1088, 275, 5717, 275, 635, 305, 5843, 278, 294, 289, 637, 289, 6988, 296, 341, 1921, 302, 596, 3420, 266, 13802, 370, 324, 271, 279, 397, 266, 3525, 345, 294, 330, 438, 266, 3115, 340, 1179, 312, 423, 275, 277, 10287, 275, 3676, 302, 281, 304, 558, 296, 2028, 338, 301, 2880, 266, 2119, 301, 531, 266, 319, 275, 501, 266, 1321, 271, 10271, 266, 530, 2631, 275, 15047, 26, 297, 278, 324, 271, 1392, 275, 282, 3901, 301, 705, 4939, 289, 3645, 356, 302, 297, 450, 1216, 553, 411, 289, 2571, 266, 746, 1925, 266, 274, 289, 4679, 411, 289, 4355, 6407, 274, 289, 718, 305, 6747, 266, 457, 426, 3358, 266, 746, 1179, 301, 320, 266, 274, 289, 1955, 426, 267, 14864, 266, 332, 271, 357, 271, 359, 2602, 271, 2926, 305, 6747, 266, 457, 426, 2906, 282, 305, 6747, 266, 1143, 356, 302, 297, 450, 456, 275, 1824, 273, 345, 294, 330, 381, 275, 279, 9399, 1591, 320, 266, 283, 619, 276, 5392, 327, 5456, 266, 889, 296, 12955, 301, 7998, 14431, 2094, 295, 275, 3242, 2094, 906, 301, 1819, 397, 266, 9399, 1591, 906, 301, 1754, 266, 332, 271, 380, 885, 459, 1385, 289, 478, 271, 814, 312, 310, 275, 456, 301, 2531, 417, 277, 364, 271, 814, 312, 476, 266, 656, 266, 539, 289, 274, 382, 271, 276, 2023, 266, 1397, 266, 656, 266, 539, 289, 274, 523, 480, 320, 266, 656, 266, 539, 289, 274, 409, 271, 1282, 275, 1065, 271, 3158, 327, 13024, 301, 461, 266, 456, 275, 3402, 390, 1224, 266, 4939, 289, 838, 275, 1531, 266, 283, 266, 2647, 417, 500, 275, 1065, 271, 1855, 302, 334, 301, 838, 275, 1561, 271, 2371, 6514, 275, 1564, 619, 455, 412, 2031, 651, 301, 1869, 384, 566, 8029, 396, 397, 266, 274, 301, 320, 266, 332, 271, 294, 417, 277, 4084, 266, 4760, 769, 266, 274, 289, 951, 330, 3194, 271, 465, 3148, 271, 294, 417, 2114, 2573, 301, 2005, 420, 266, 1179, 301, 280, 275, 1046, 273, 990, 16224, 370, 281, 278, 267, 266, 1232, 275, 282, 4112, 345, 294, 330, 521, 289, 3645, 320, 266, 276, 278, 883, 305, 306, 380, 307, 278, 279, 289, 18332, 1867, 2094, 906, 301, 1609, 558, 296, 2028, 1867, 400, 558, 296, 1556, 275, 368, 312, 504, 271, 1867, 883, 305, 306, 3720, 266, 949, 330, 2114, 267, 266, 1232, 275, 282, 538, 406, 1253, 271, 1361, 420, 266, 4993, 275, 1018, 271, 5804, 301, 10164, 271, 813, 278, 297, 275, 4509, 327, 711, 340, 1902, 458, 264, 1202, 8652, 301, 501, 370, 288, 302, 596, 14431, 558, 296, 18548, 275, 4111, 301, 2435, 271, 310, 289, 3645, 274, 275, 730, 266, 1373, 278, 883, 305, 306, 324, 271, 2371, 273, 345, 294, 330, 1330, 289, 2460, 9131, 380, 371, 278, 512, 2064, 278, 297, 275, 1750, 454, 2947, 339, 1544, 275, 2587, 271, 2129, 568, 271, 297, 275, 4915, 340, 280, 275, 1831, 278, 333, 275, 282]\n",
      "['Ġà¹ĥà¸Ħà¸£à¹Ģà¸Ħà¸¢à¸¡', 'à¸µ', 'à¹ģà¸Łà¸Ļà¸Ĺ', 'à¸µà¹Ī', 'à¸ģ', 'à¸´', 'à¸Ļà¸Ńà¸²à¸«à¸²à¸£', 'à¹Ħà¸¡', 'à¹Ī', 'à¸ĸ', 'à¸¹', 'à¸ģà¸Ľ', 'à¸²à¸ģà¸ģ', 'à¸±', 'à¸Ļà¹ģà¸¥', 'à¹ī', 'à¸§à¸£', 'à¸¹à¹ī', 'à¸ª', 'à¸¶', 'à¸ģà¹Ģà¸ª', 'à¸µ', 'à¸¢', 'à¸Ħà¸§à¸²à¸¡à¸ª', 'à¸¸', 'à¸Ĥ', 'à¹Ħà¸Ľà¸Ńà¸¢', 'à¹Ī', 'à¸²à¸ĩà¸Ļ', 'à¸¶', 'à¸ĩà¸ļ', 'à¹ī', 'à¸²à¸ĩà¸¡', 'à¸±à¹ī', 'à¸¢à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġ', 'Ġà¸ģ', 'à¹Ī', 'à¸Ńà¸Ļà¸Ń', 'à¸·à¹Ī', 'à¸Ļà¸ľà¸¡', 'à¸ķ', 'à¹ī', 'à¸Ńà¸ĩà¸ļà¸Ńà¸ģà¸ģ', 'à¹Ī', 'à¸Ńà¸Ļà¹Ģà¸¥à¸¢à¸§', 'à¹Ī', 'à¸²à¸Ħ', 'à¸Ļà¹Ģà¸£', 'à¸²à¸Īà¸°à¹Ģà¸¥', 'à¸·', 'à¸Ńà¸ģà¸ģ', 'à¸´', 'à¸Ļà¸Ńà¸²à¸«à¸²à¸£', 'à¹ģà¸ļà¸ļà¹Ħà¸«à¸Ļ', 'à¸Ĭà¸Ńà¸ļ', 'à¹ģà¸ļà¸ļà¹Ħà¸«à¸Ļ', 'à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¹Ģà¸£', 'à¸·à¹Ī', 'à¸Ńà¸ĩà¸Ĥà¸Ńà¸ĩ', 'à¸Ħà¸§à¸²à¸¡', 'à¸Ĭà¸Ńà¸ļà¸ª', 'à¹Ī', 'à¸§à¸Ļà¸ķ', 'à¸±', 'à¸§à¸Ļà¸°à¸Ħà¸£', 'à¸±', 'à¸ļà¸Ĺ', 'à¸¸', 'à¸ģà¸Ħà¸Ļà¸¡', 'à¸µ', 'à¸ª', 'à¸´', 'à¸Ĺà¸ĺ', 'à¸´', 'à¹ĥà¸Ļà¸ģà¸²à¸£à¹Ģà¸¥', 'à¸·', 'à¸Ńà¸ģ', 'à¸Ĥà¸Ńà¸ĩà¸Ĺ', 'à¸µà¹Ī', 'à¸Ĭà¸Ńà¸ļ', 'à¹ģà¸¥à¸°à¹Ħà¸¡', 'à¹Ī', 'à¸Ĭà¸Ńà¸ļà¸Ńà¸¢', 'à¸¹à¹Ī', 'à¹ģà¸¥', 'à¹ī', 'à¸§', 'Ġà¹ģà¸ķ', 'à¹Ī', 'à¸ľà¸¡à¸£', 'à¸¹à¹ī', 'à¸ª', 'à¸¶', 'à¸ģà¸§', 'à¹Ī', 'à¸²à¸ķà¸Ńà¸Ļà¸Ļ', 'à¸µà¹ī', 'à¸ľà¸¡à¸ģ', 'à¹į', 'à¸²à¸¥', 'à¸±', 'à¸ĩ', 'à¸Ľà¸£à¸°à¸ªà¸ļà¸Ľ', 'à¸±', 'à¸įà¸«à¸²à¸Ĺ', 'à¸µà¹Ī', 'à¸Ķ', 'à¸¹', 'à¹Ģà¸«à¸¡', 'à¸·', 'à¸Ńà¸Ļà¸Īà¸°', 'à¹Ģà¸¥', 'à¹ĩ', 'à¸ģà¹ģà¸ķ', 'à¹Ī', 'à¸ģà¸¥à¸²à¸¢à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¸§', 'à¹Ī', 'à¸²à¸¡', 'à¸±', 'à¸Ļà¸Ħ', 'à¹Ī', 'à¸Ńà¸Ļà¸Ĥ', 'à¹ī', 'à¸²à¸ĩà¹ĥà¸«à¸į', 'à¹Ī', 'Ġà¸ľà¸¡', 'à¸Ħà¸ļà¸ģ', 'à¸±', 'à¸ļà¹ģà¸Łà¸Ļà¸¡à¸²', '6', 'à¸Ľ', 'à¸µ', 'à¹ģà¸¥', 'à¹ī', 'à¸§à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġà¸ľà¸¡à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¸Ħà¸Ļ', 'à¸Ĭà¸Ńà¸ļà¸ģ', 'à¸´', 'à¸Ļà¸Ńà¸²à¸«à¸²à¸£', 'à¸į', 'à¸µà¹Ī', 'à¸Ľ', 'à¸¸à¹Ī', 'à¸Ļà¹ģà¸¥à¸°', 'à¸Ľà¸¥', 'à¸²à¸Ķ', 'à¸´', 'à¸ļà¹ģà¸ķ', 'à¹Ī', 'à¹ģà¸Łà¸Ļ', 'à¸ľà¸¡à¹Ħà¸¡', 'à¹Ī', 'à¸ģ', 'à¸´', 'à¸Ļà¸Ľà¸¥', 'à¸²à¸Ķ', 'à¸´', 'à¸ļà¹Ģà¸¥à¸¢', 'Ġà¸ľà¸¡à¸Ńà¸¢à¸²à¸ģ', 'à¸ģ', 'à¸´', 'à¸Ļà¸ļ', 'à¸¸', 'à¸Łà¹Ģà¸Ł', 'à¹Ī', 'à¹Ģà¸Ļ', 'à¸·à¹ī', 'à¸Ńà¹ģà¸ķ', 'à¹Ī', 'à¹ģà¸Łà¸Ļ', 'à¸ľà¸¡à¸ģ', 'à¹ĩ', 'à¹Ħà¸¡', 'à¹Ī', 'à¸ģ', 'à¸´', 'à¸Ļà¹Ģà¸Ļ', 'à¸·à¹ī', 'à¸Ń', 'Ġà¹Ģà¸£à¸²à¹Ģà¸¥à¸¢à¹Ħà¸¡', 'à¹Ī', 'à¹Ħà¸Ķ', 'à¹ī', 'à¹Ģà¸Ĥ', 'à¹ī', 'à¸²à¸Ĺ', 'à¸²à¸Ļà¸£', 'à¹ī', 'à¸²à¸Ļà¸ļ', 'à¸¸', 'à¸Łà¹Ģà¸Ł', 'à¹Ī', 'à¹Ģà¸Ļ', 'à¸·à¹ī', 'à¸Ńà¹ģà¸¥à¸°', 'à¸ļ', 'à¸¸', 'à¸Łà¹Ģà¸Ł', 'à¹Ī', 'à¸Ńà¸²à¸«à¸²à¸£', 'à¸į', 'à¸µà¹Ī', 'à¸Ľ', 'à¸¸à¹Ī', 'à¸Ļà¸ģ', 'à¸±', 'à¸Ļà¹Ģà¸ŀà¸£à¸²à¸°', 'à¸£', 'à¸¹à¹ī', 'à¸ª', 'à¸¶', 'à¸ģà¸¥', 'à¸±', 'à¸§', 'à¹ģà¸Łà¸Ļà¸ľà¸¡', 'à¸Ĺà¸²à¸Ļ', 'à¹Ħà¸¡', 'à¹Ī', 'à¸Ħ', 'à¸¸à¹ī', 'à¸¡', 'Ġà¹ģà¸¥à¸°à¹Ģà¸£', 'à¸·à¹Ī', 'à¸Ńà¸ĩà¹ĥà¸«à¸į', 'à¹Ī', 'à¹Ģà¸¥à¸¢à¸Ħ', 'à¸·', 'à¸Ńà¸ľà¸¡à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¸Ħà¸Ļà¸Ĭà¸Ńà¸ļ', 'à¸Ĺà¸²à¸Ļà¸Ńà¸²à¸«à¸²à¸£', 'à¸£à¸ª', 'à¸Ī', 'à¸±', 'à¸Ķà¹ģà¸¥à¸°', 'à¸£à¸ª', 'à¹Ģà¸ľ', 'à¹ĩ', 'à¸Ķà¸¡à¸²à¸ģ', 'Ġà¹ģà¸ķ', 'à¹Ī', 'à¹ģà¸Łà¸Ļà¸ľà¸¡', 'à¸Ĺà¸²à¸Ļ', 'à¹Ģà¸ľ', 'à¹ĩ', 'à¸Ķà¹Ħà¸¡', 'à¹Ī', 'à¹Ħà¸Ķ', 'à¹ī', 'à¹Ģà¸¥à¸¢', 'à¹Ģà¸§à¸¥à¸²', 'à¹Ģà¸£à¸²', 'à¹Ħà¸Ľà¸ģ', 'à¸´', 'à¸Ļà¸ª', 'à¹ī', 'à¸¡à¸ķ', 'à¹į', 'à¸²à¸ģ', 'à¸±', 'à¸Ļà¸ģ', 'à¹ĩ', 'à¸Īà¸°à¸ª', 'à¸±à¹Ī', 'à¸ĩ', 'Ġà¸ª', 'à¹ī', 'à¸¡à¸ķ', 'à¹į', 'à¸²à¹Ħà¸¡', 'à¹Ī', 'à¹ĥà¸ª', 'à¹Ī', 'à¸ŀà¸£', 'à¸´', 'à¸ģ', 'Ġà¸ķ', 'à¹ī', 'à¸¡', 'à¹ģà¸ĭ', 'à¹Ī', 'à¸ļà¹Ħà¸¡', 'à¹Ī', 'à¹ĥà¸ª', 'à¹Ī', 'à¸ŀà¸£', 'à¸´', 'à¸ģ', 'Ġà¸¥', 'à¸²à¸ļ', 'à¹Ħà¸¡', 'à¹Ī', 'à¹ĥà¸ª', 'à¹Ī', 'à¸ŀà¸£', 'à¸´', 'à¸ģ', 'Ġà¸£', 'à¹ī', 'à¸²à¸Ļà¸ģ', 'à¸±', 'à¸ļà¸Ĥ', 'à¹ī', 'à¸²à¸§à¸Ń', 'à¸·à¹Ī', 'à¸Ļà¹Ĩà¸ģ', 'à¹ĩ', 'à¹Ģà¸Ĭ', 'à¹Ī', 'à¸Ļà¸ģ', 'à¸±', 'à¸Ļà¹ģà¸Łà¸Ļ', 'à¸ľà¸¡', 'à¸Īà¸°à¹Ħà¸¡', 'à¹Ī', 'à¸Ĭà¸Ńà¸ļà¸ģ', 'à¸´', 'à¸Ļà¸ľ', 'à¸±', 'à¸ģà¹Ħà¸¡', 'à¹Ī', 'à¸Ħ', 'à¹Ī', 'à¸Ńà¸¢à¸ª', 'à¸±à¹Ī', 'à¸ĩà¸ģ', 'à¸±', 'à¸ļà¸Ĥ', 'à¹ī', 'à¸²à¸§à¸Ĺ', 'à¸µà¹Ī', 'à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¸ľ', 'à¸±', 'à¸ģà¹ģà¸¥', 'à¹ī', 'à¸§à¸ľà¸¡', 'à¸Ĭà¸Ńà¸ļà¸ľ', 'à¸±', 'à¸ģà¸ļ', 'à¸¸à¹ī', 'à¸ĩà¸Ĺ', 'à¸Ńà¸Ķ', 'à¸ģà¸£à¸Ńà¸ļ', 'Ġà¹Ģà¸«', 'à¹ĩ', 'à¸Ķà¸«', 'à¸Ńà¸¡', 'à¸ªà¸Ķ', 'à¸Ĺà¸Ńà¸Ķ', 'à¸¡à¸²à¸ģ', 'Ġà¹ģà¸ķ', 'à¹Ī', 'à¸ģ', 'à¹ĩ', 'à¹Ħà¸¡', 'à¹Ī', 'à¹Ħà¸Ķ', 'à¹ī', 'à¸ª', 'à¸±à¹Ī', 'à¸ĩ', 'à¹Ģà¸ŀà¸£à¸²à¸°à¸§', 'à¹Ī', 'à¸²à¹Ģà¸ĺ', 'à¸Ńà¹Ħà¸¡', 'à¹Ī', 'à¸ģ', 'à¸´', 'à¸Ļà¸ĸ', 'à¸¶', 'à¸ĩà¹Ģà¸Ħ', 'à¹ī', 'à¸²à¸Īà¸°', 'à¸ļà¸Ńà¸ģà¹ĥà¸«', 'à¹ī', 'à¸ª', 'à¸±à¹Ī', 'à¸ĩà¹Ģà¸¥à¸¢', 'à¹Ĩà¸ģ', 'à¹ĩ', 'à¹Ģà¸ĸà¸Ńà¸°', 'à¹ģà¸ķ', 'à¹Ī', 'à¸ľà¸¡à¸ģ', 'à¹ĩ', 'à¸¢', 'à¸±', 'à¸ĩà¹Ģà¸ģ', 'à¸£', 'à¸ĩà¹ĥà¸Ī', 'à¹Ģà¸ĺà¸Ńà¸Ńà¸¢', 'à¸¹à¹Ī', 'à¸Ķ', 'à¸µ', 'à¸Ń', 'à¹Ī', 'à¸°à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġà¸ľà¸¡à¸£', 'à¸¹à¹ī', 'à¸ª', 'à¸¶', 'à¸ģà¸ģ', 'à¸´', 'à¸Ļà¸Ńà¸²à¸«à¸²à¸£', 'à¹Ħà¸¡', 'à¹Ī', 'à¸¡', 'à¸µ', 'à¸Ħà¸§à¸²à¸¡à¸ª', 'à¸¸', 'à¸Ĥ', 'à¹Ģà¸¥à¸¢', 'à¸Ĭ', 'à¸µ', 'à¸§', 'à¸´', 'à¸ķà¸ľà¸¡', 'à¸Ĥà¸²à¸Ķ', 'à¸£à¸ª', 'à¹Ģà¸ľ', 'à¹ĩ', 'à¸Ķà¹Ħà¸Ľ', 'à¹Ģà¸«à¸¡', 'à¸·', 'à¸Ńà¸Ļà¸Īà¸°', 'à¸Ĥà¸²à¸Ķ', 'à¹ĥà¸Ī', 'à¹Ģà¸«à¸¡', 'à¸·', 'à¸Ńà¸Ļà¸¡', 'à¸±', 'à¸Ļà¸Ĺ', 'à¹į', 'à¸²à¹ĥà¸«', 'à¹ī', 'à¸Ĥà¸²à¸Ķ', 'à¸Ħà¸§à¸²à¸¡à¸ª', 'à¸¸', 'à¸Ĥ', 'à¹Ħà¸Ľà¸Ńà¸¢', 'à¹Ī', 'à¸²à¸ĩà¸Ļ', 'à¸¶', 'à¸ĩà¹Ģà¸¥à¸¢', 'à¸Ń', 'à¹Ī', 'à¸°à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġà¸¢', 'à¸´à¹Ī', 'à¸ĩà¸ĸ', 'à¹ī', 'à¸²à¹Ģà¸£à¸²', 'à¹ģà¸ķ', 'à¹Ī', 'à¸ĩà¸ĩà¸²à¸Ļà¸ģ', 'à¸±', 'à¸Ļà¹ģà¸¥', 'à¹ī', 'à¸§à¸ľà¸¡à¸ģ', 'à¹ĩ', 'à¸Ńà¸²à¸Īà¸Īà¸°à¸ķ', 'à¹ī', 'à¸Ńà¸ĩà¸¡', 'à¸µ', 'à¸Ľ', 'à¸±', 'à¸įà¸«à¸²à¹Ģà¸£', 'à¸·à¹Ī', 'à¸Ńà¸ĩà¸Ļ', 'à¸µà¹ī', 'à¸¡à¸²à¸ģà¸Ĥ', 'à¸¶à¹ī', 'à¸Ļ', 'Ġà¸ŀà¸Ń', 'à¸ľà¸¡à¹Ģà¸«', 'à¹ĩ', 'à¸Ļà¸Ħ', 'à¸¹à¹Ī', 'à¸Ĺ', 'à¸µà¹Ī', 'à¸Ĭà¸Ńà¸ļ', 'à¸Ĺà¸²à¸Ļà¸Ńà¸²à¸«à¸²à¸£', 'à¹Ģà¸«à¸¡', 'à¸·', 'à¸Ńà¸Ļà¹Ĩà¸ģ', 'à¸±', 'à¸Ļà¹Ģà¸«', 'à¹ĩ', 'à¸Ļà¹Ģà¸Ħ', 'à¹ī', 'à¸²à¸ģ', 'à¸´', 'à¸Ļà¸Ńà¸²à¸«à¸²à¸£', 'à¸ģ', 'à¸±', 'à¸Ļà¸Ńà¸¢', 'à¹Ī', 'à¸²à¸ĩà¸¡', 'à¸µ', 'à¸Ħà¸§à¸²à¸¡à¸ª', 'à¸¸', 'à¸Ĥ', 'à¹ģà¸¥', 'à¹ī', 'à¸§à¸ľà¸¡', 'à¸£', 'à¸¹à¹ī', 'à¸ª', 'à¸¶', 'à¸ģà¸Ń', 'à¸´', 'à¸Īà¸ī', 'à¸²à¸¡à¸²à¸ģà¹Ĩ', 'à¹Ģà¸¥à¸¢', 'Ġà¸¡', 'à¸µ', 'à¹ĥà¸Ħà¸£', 'à¹Ģà¸Ħà¸¢à¸¡', 'à¸µ', 'à¸Ľ', 'à¸±', 'à¸įà¸«à¸²', 'à¹ģà¸ļà¸ļ', 'à¸ľà¸¡à¸¡', 'à¸±à¹ī', 'à¸¢à¸Ħà¸£', 'à¸±', 'à¸ļà¹ģà¸¥', 'à¹ī', 'à¸§à¸Īà¸°', 'à¹ģà¸ģ', 'à¹ī', 'à¸Ľ', 'à¸±', 'à¸įà¸«à¸²à¸Ļ', 'à¸µà¹ī', 'à¸¢', 'à¸±', 'à¸ĩà¹Ħà¸ĩà¸Ķ', 'à¸µ', 'à¸Ħà¸£', 'à¸±', 'à¸ļ']\n",
      "[' ใครเคยม', 'ี', 'แฟนท', 'ี่', 'ก', 'ิ', 'นอาหาร', 'ไม', '่', 'ถ', 'ู', 'กป', 'ากก', 'ั', 'นแล', '้', 'วร', 'ู้', 'ส', 'ึ', 'กเส', 'ี', 'ย', 'ความส', 'ุ', 'ข', 'ไปอย', '่', 'างน', 'ึ', 'งบ', '้', 'างม', 'ั้', 'ยคร', 'ั', 'บ', ' ', ' ก', '่', 'อนอ', 'ื่', 'นผม', 'ต', '้', 'องบอกก', '่', 'อนเลยว', '่', 'าค', 'นเร', 'าจะเล', 'ื', 'อกก', 'ิ', 'นอาหาร', 'แบบไหน', 'ชอบ', 'แบบไหน', 'เป', '็', 'นเร', 'ื่', 'องของ', 'ความ', 'ชอบส', '่', 'วนต', 'ั', 'วนะคร', 'ั', 'บท', 'ุ', 'กคนม', 'ี', 'ส', 'ิ', 'ทธ', 'ิ', 'ในการเล', 'ื', 'อก', 'ของท', 'ี่', 'ชอบ', 'และไม', '่', 'ชอบอย', 'ู่', 'แล', '้', 'ว', ' แต', '่', 'ผมร', 'ู้', 'ส', 'ึ', 'กว', '่', 'าตอนน', 'ี้', 'ผมก', 'ํ', 'าล', 'ั', 'ง', 'ประสบป', 'ั', 'ญหาท', 'ี่', 'ด', 'ู', 'เหม', 'ื', 'อนจะ', 'เล', '็', 'กแต', '่', 'กลายเป', '็', 'นว', '่', 'าม', 'ั', 'นค', '่', 'อนข', '้', 'างใหญ', '่', ' ผม', 'คบก', 'ั', 'บแฟนมา', '6', 'ป', 'ี', 'แล', '้', 'วคร', 'ั', 'บ', ' ผมเป', '็', 'นคน', 'ชอบก', 'ิ', 'นอาหาร', 'ญ', 'ี่', 'ป', 'ุ่', 'นและ', 'ปล', 'าด', 'ิ', 'บแต', '่', 'แฟน', 'ผมไม', '่', 'ก', 'ิ', 'นปล', 'าด', 'ิ', 'บเลย', ' ผมอยาก', 'ก', 'ิ', 'นบ', 'ุ', 'ฟเฟ', '่', 'เน', 'ื้', 'อแต', '่', 'แฟน', 'ผมก', '็', 'ไม', '่', 'ก', 'ิ', 'นเน', 'ื้', 'อ', ' เราเลยไม', '่', 'ได', '้', 'เข', '้', 'าท', 'านร', '้', 'านบ', 'ุ', 'ฟเฟ', '่', 'เน', 'ื้', 'อและ', 'บ', 'ุ', 'ฟเฟ', '่', 'อาหาร', 'ญ', 'ี่', 'ป', 'ุ่', 'นก', 'ั', 'นเพราะ', 'ร', 'ู้', 'ส', 'ึ', 'กล', 'ั', 'ว', 'แฟนผม', 'ทาน', 'ไม', '่', 'ค', 'ุ้', 'ม', ' และเร', 'ื่', 'องใหญ', '่', 'เลยค', 'ื', 'อผมเป', '็', 'นคนชอบ', 'ทานอาหาร', 'รส', 'จ', 'ั', 'ดและ', 'รส', 'เผ', '็', 'ดมาก', ' แต', '่', 'แฟนผม', 'ทาน', 'เผ', '็', 'ดไม', '่', 'ได', '้', 'เลย', 'เวลา', 'เรา', 'ไปก', 'ิ', 'นส', '้', 'มต', 'ํ', 'าก', 'ั', 'นก', '็', 'จะส', 'ั่', 'ง', ' ส', '้', 'มต', 'ํ', 'าไม', '่', 'ใส', '่', 'พร', 'ิ', 'ก', ' ต', '้', 'ม', 'แซ', '่', 'บไม', '่', 'ใส', '่', 'พร', 'ิ', 'ก', ' ล', 'าบ', 'ไม', '่', 'ใส', '่', 'พร', 'ิ', 'ก', ' ร', '้', 'านก', 'ั', 'บข', '้', 'าวอ', 'ื่', 'นๆก', '็', 'เช', '่', 'นก', 'ั', 'นแฟน', 'ผม', 'จะไม', '่', 'ชอบก', 'ิ', 'นผ', 'ั', 'กไม', '่', 'ค', '่', 'อยส', 'ั่', 'งก', 'ั', 'บข', '้', 'าวท', 'ี่', 'เป', '็', 'นผ', 'ั', 'กแล', '้', 'วผม', 'ชอบผ', 'ั', 'กบ', 'ุ้', 'งท', 'อด', 'กรอบ', ' เห', '็', 'ดห', 'อม', 'สด', 'ทอด', 'มาก', ' แต', '่', 'ก', '็', 'ไม', '่', 'ได', '้', 'ส', 'ั่', 'ง', 'เพราะว', '่', 'าเธ', 'อไม', '่', 'ก', 'ิ', 'นถ', 'ึ', 'งเค', '้', 'าจะ', 'บอกให', '้', 'ส', 'ั่', 'งเลย', 'ๆก', '็', 'เถอะ', 'แต', '่', 'ผมก', '็', 'ย', 'ั', 'งเก', 'ร', 'งใจ', 'เธออย', 'ู่', 'ด', 'ี', 'อ', '่', 'ะคร', 'ั', 'บ', ' ผมร', 'ู้', 'ส', 'ึ', 'กก', 'ิ', 'นอาหาร', 'ไม', '่', 'ม', 'ี', 'ความส', 'ุ', 'ข', 'เลย', 'ช', 'ี', 'ว', 'ิ', 'ตผม', 'ขาด', 'รส', 'เผ', '็', 'ดไป', 'เหม', 'ื', 'อนจะ', 'ขาด', 'ใจ', 'เหม', 'ื', 'อนม', 'ั', 'นท', 'ํ', 'าให', '้', 'ขาด', 'ความส', 'ุ', 'ข', 'ไปอย', '่', 'างน', 'ึ', 'งเลย', 'อ', '่', 'ะคร', 'ั', 'บ', ' ย', 'ิ่', 'งถ', '้', 'าเรา', 'แต', '่', 'งงานก', 'ั', 'นแล', '้', 'วผมก', '็', 'อาจจะต', '้', 'องม', 'ี', 'ป', 'ั', 'ญหาเร', 'ื่', 'องน', 'ี้', 'มากข', 'ึ้', 'น', ' พอ', 'ผมเห', '็', 'นค', 'ู่', 'ท', 'ี่', 'ชอบ', 'ทานอาหาร', 'เหม', 'ื', 'อนๆก', 'ั', 'นเห', '็', 'นเค', '้', 'าก', 'ิ', 'นอาหาร', 'ก', 'ั', 'นอย', '่', 'างม', 'ี', 'ความส', 'ุ', 'ข', 'แล', '้', 'วผม', 'ร', 'ู้', 'ส', 'ึ', 'กอ', 'ิ', 'จฉ', 'ามากๆ', 'เลย', ' ม', 'ี', 'ใคร', 'เคยม', 'ี', 'ป', 'ั', 'ญหา', 'แบบ', 'ผมม', 'ั้', 'ยคร', 'ั', 'บแล', '้', 'วจะ', 'แก', '้', 'ป', 'ั', 'ญหาน', 'ี้', 'ย', 'ั', 'งไงด', 'ี', 'คร', 'ั', 'บ']\n"
     ]
    }
   ],
   "source": [
    "text = \"ใครเคยมีแฟนที่กินอาหารไม่ถูกปากกันแล้วรู้สึกเสียความสุขไปอย่างนึงบ้างมั้ยครับ  ก่อนอื่นผมต้องบอกก่อนเลยว่าคนเราจะเลือกกินอาหารแบบไหนชอบแบบไหนเป็นเรื่องของความชอบส่วนตัวนะครับทุกคนมีสิทธิในการเลือกของที่ชอบและไม่ชอบอยู่แล้ว แต่ผมรู้สึกว่าตอนนี้ผมกำลังประสบปัญหาที่ดูเหมือนจะเล็กแต่กลายเป็นว่ามันค่อนข้างใหญ่ ผมคบกับแฟนมา6ปีแล้วครับ ผมเป็นคนชอบกินอาหารญี่ปุ่นและปลาดิบแต่แฟนผมไม่กินปลาดิบเลย ผมอยากกินบุฟเฟ่เนื้อแต่แฟนผมก็ไม่กินเนื้อ เราเลยไม่ได้เข้าทานร้านบุฟเฟ่เนื้อและบุฟเฟ่อาหารญี่ปุ่นกันเพราะรู้สึกลัวแฟนผมทานไม่คุ้ม และเรื่องใหญ่เลยคือผมเป็นคนชอบทานอาหารรสจัดและรสเผ็ดมาก แต่แฟนผมทานเผ็ดไม่ได้เลยเวลาเราไปกินส้มตำกันก็จะสั่ง ส้มตำไม่ใส่พริก ต้มแซ่บไม่ใส่พริก ลาบไม่ใส่พริก ร้านกับข้าวอื่นๆก็เช่นกันแฟนผมจะไม่ชอบกินผักไม่ค่อยสั่งกับข้าวที่เป็นผักแล้วผมชอบผักบุ้งทอดกรอบ เห็ดหอมสดทอดมาก แต่ก็ไม่ได้สั่งเพราะว่าเธอไม่กินถึงเค้าจะบอกให้สั่งเลยๆก็เถอะแต่ผมก็ยังเกรงใจเธออยู่ดีอ่ะครับ ผมรู้สึกกินอาหารไม่มีความสุขเลยชีวิตผมขาดรสเผ็ดไปเหมือนจะขาดใจเหมือนมันทำให้ขาดความสุขไปอย่างนึงเลยอ่ะครับ ยิ่งถ้าเราแต่งงานกันแล้วผมก็อาจจะต้องมีปัญหาเรื่องนี้มากขึ้น พอผมเห็นคู่ที่ชอบทานอาหารเหมือนๆกันเห็นเค้ากินอาหารกันอย่างมีความสุขแล้วผมรู้สึกอิจฉามากๆเลย มีใครเคยมีปัญหาแบบผมมั้ยครับแล้วจะแก้ปัญหานี้ยังไงดีครับ\"\n",
    "encoded = tokenizer.encode(text)\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)\n",
    "print(list(map(lambda x : tokenizer.decode([x]), encoded.ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tokenize on Pantip Sample อาการแบบนี้คือไรกัน?\n",
    "https://pantip.com/topic/40009518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16263, 340, 283, 296, 4319, 274, 275, 264, 35, 225, 7454, 305, 511, 275, 1291, 1722, 601, 17, 26, 670, 296, 325, 428, 1626, 295, 278, 1897, 3044, 361, 301, 283, 305, 511, 275, 970, 562, 5432, 816, 1940, 266, 262, 470, 271, 570, 13859, 16706, 441, 327, 1931, 680, 271, 9022, 1124, 334, 301, 3402, 459, 1555, 992, 285, 3988, 266, 396, 646, 397, 266, 1733, 266, 359, 312, 319, 339, 280, 362, 312, 354, 266, 285, 14120, 273, 345, 279, 266, 9525, 305, 511, 275, 1309, 327, 325, 441, 327, 3441, 5296, 266, 262, 409, 345, 294, 330, 1462, 275, 1846, 275, 1812, 612, 1454, 367, 327, 1715, 266, 262, 16418, 266, 816, 1830, 323, 370, 1400, 278, 425, 538, 275, 556, 266, 539, 271, 384, 1508, 275, 6341, 6322, 340, 3587, 406, 1410, 345, 294, 330, 438, 266, 262, 362, 312, 476, 1841, 271, 620, 312, 10347, 339, 264, 470, 271, 816, 3342, 275, 274, 409, 275, 551, 302, 334, 301, 466, 275, 10083, 266, 332, 271, 1393, 700, 275, 880, 16002, 406, 276, 1218, 7359, 271, 3523, 397, 266, 280, 275, 491, 305, 511, 275, 1705, 296, 1386, 289, 276, 428, 7522, 4057, 271, 3190, 278, 13405, 449, 266, 288, 275, 1873, 333, 440, 669, 275, 264, 2102, 289, 14131, 278, 6873, 325, 406, 500, 275, 2199, 461, 271, 354, 266, 285, 1202, 1014, 266, 288, 275, 1873, 8172, 312, 618, 844, 301, 295, 470, 330, 3952, 437, 275, 1565, 922, 2183, 301, 288, 275, 1082, 351, 266, 262, 361, 289, 741, 271, 2134, 275, 264, 905, 301, 280, 406, 277, 3602, 361, 301, 283, 289, 650, 266, 262, 612, 2366, 2959, 289, 892, 275, 11486, 305, 511, 275, 11096, 305, 438, 275, 264, 4711, 407, 302, 2764, 266, 332, 271, 1218, 1603, 266, 2980, 339, 264, 905, 301, 1016, 4651, 289, 13448, 327, 652, 296, 519, 278, 352, 1490, 275, 1170, 373, 278, 1812, 552, 2417, 266, 14229, 302, 282, 271, 313, 905, 301, 12941, 266, 504, 271, 357, 1716, 275, 669, 266, 262, 795, 271, 1069, 317, 337, 795, 271, 357, 3993, 266, 1076, 370, 420, 266, 320, 287, 266, 281, 271, 264, 417, 2249, 283, 275, 791, 278, 685, 275, 264, 905, 301, 306, 275, 2308, 459, 1861, 302, 4060, 278, 1812, 1354, 301, 306, 275, 767, 275, 933, 357, 2441, 266, 1765, 266, 277, 4711, 1202, 461, 271, 319, 10876, 275, 1082, 319, 325, 406, 277, 361, 301, 600, 3587, 327, 6358, 327, 652, 296, 264, 3464, 312, 2486, 302, 381, 275, 1221, 330, 9756, 320, 266, 332, 271, 288, 275, 1873, 541, 922, 526, 271, 279, 266, 262, 470, 330, 852, 271, 2228, 271, 2953, 1709, 278, 715, 1015, 339, 491, 296, 1063, 2183, 301, 8822, 327, 6358, 327, 652, 296, 531, 266, 262, 782, 450, 276, 2417, 266, 455, 302, 282, 271, 3759, 301, 9368, 271, 318, 1747, 420, 266, 329, 327, 1426, 328, 345, 3519, 302, 4060, 278, 11096, 339, 669, 275, 264, 3094, 19508, 302, 6742, 327, 790, 301, 280, 275, 925, 275, 282, 4020, 267, 278, 2031, 1463, 19443, 668, 340, 283, 296, 13505, 2082, 4644, 7745, 7890, 370, 1400, 278, 425, 397, 266, 716, 1550, 278, 4352, 271, 1722, 362, 312, 476, 8892, 330, 590, 278, 18004, 340, 371, 1269, 668, 340, 348, 469, 1798, 391, 296, 7444, 275, 264, 1780, 266, 5495, 289, 1874, 9118, 881, 4478, 12524, 271, 282, 271, 1702, 468, 266, 262, 2118, 668, 340, 276, 275, 501, 296, 1611, 409, 345, 294, 330, 6577, 323, 370]\n",
      "['Ġà¸Ńà¸²à¸ģà¸²à¸£à¹ģà¸ļà¸ļà¸Ļ', 'à¸µà¹ī', 'à¸Ħ', 'à¸·', 'à¸Ńà¹Ħà¸£', 'à¸ģ', 'à¸±', 'à¸Ļ', '?', 'Ġ', 'Ġà¹Ģà¸Ĥà¸²à¸Ħ', 'à¸¸', 'à¸¢à¸ģ', 'à¸±', 'à¸ļà¹Ģà¸£', 'à¸²à¸¡à¸²', 'Ġ5', '-', '6', 'Ġà¹Ģà¸Ķ', 'à¸·', 'à¸Ńà¸Ļ', 'Ġà¹Ģà¸£', 'à¸²à¸ķà¸²à¸¡', 'à¸Ī', 'à¸µ', 'à¸ļà¹Ģà¸Ĥ', 'à¸²à¸Ļà¸°à¸Ħà¸°', 'Ġà¸ģ', 'à¹ĩ', 'à¸Ħ', 'à¸¸', 'à¸¢à¸ģ', 'à¸±', 'à¸Ļà¸¡à¸²', 'Ġà¹ĥà¸Ļ', 'à¸£à¸°à¸¢à¸°à¹Ģà¸§à¸¥', 'à¸²à¹Ģà¸Ĥ', 'à¸²à¸ļà¸Ńà¸ģà¸§', 'à¹Ī', 'à¸²', 'Ġà¸ĸ', 'à¹ī', 'à¸²à¹Ģà¸£', 'à¸²à¸¥à¸Ķ', 'Ġà¸Ļà¸Ļ', 'Ġà¹Ģà¸ŀ', 'à¸·à¹Ī', 'à¸Ńà¹Ģà¸Ĥ', 'à¸²à¹Ħà¸Ķ', 'à¹ī', 'Ġà¹Ģà¸Ĥà¸²à¸Īà¸°', 'à¸¢à¸Ńà¸¡', 'à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¹ģà¸Łà¸Ļ', 'à¹Ģà¸£à¸²', 'Ġà¸ķà¸£', 'à¸£à¸ģ', 'à¸°', 'à¹Ĥà¸ĩ', 'à¹Ī', 'à¸¡à¸²à¸ģ', 'à¸Ļà¸°à¸Ħà¸°', 'Ġà¹ģà¸ķ', 'à¹Ī', 'à¸ĸà¸²à¸¡à¸§', 'à¹Ī', 'à¸²à¸Ĺ', 'à¹į', 'à¸²à¸¡', 'à¸±à¹ī', 'à¸¢', 'Ġà¸Ĺ', 'à¹į', 'à¸²à¸Ħ', 'à¹Ī', 'à¸°', 'Ġà¸ŀà¸Ńà¹Ħà¸Ľ', 'à¸£', 'à¸¹à¹ī', 'à¸§', 'à¹Ī', 'à¸²à¹Ģà¸Ĥà¸²à¸Ħ', 'à¸¸', 'à¸¢à¸ģ', 'à¸±', 'à¸ļà¹Ģà¸ŀ', 'à¸·à¹Ī', 'à¸Ńà¸Ļ', 'Ġà¹Ģà¸ŀ', 'à¸·à¹Ī', 'à¸Ńà¸Ļà¹Ģà¸Ĥ', 'à¸²à¸ĸà¸²à¸¡à¸§', 'à¹Ī', 'à¸²', 'Ġà¸£', 'à¸¹à¹ī', 'à¸ª', 'à¸¶', 'à¸ģà¸¢', 'à¸±', 'à¸ĩà¹Ħà¸ĩà¸ģ', 'à¸±', 'à¸ļà¹Ģà¸£à¸²', 'Ġà¹Ģà¸Ĥ', 'à¸²à¸ķà¸Ńà¸ļ', 'à¹Ģà¸ŀ', 'à¸·à¹Ī', 'à¸Ńà¸Ļà¸§', 'à¹Ī', 'à¸²', 'Ġà¹Ģà¸Ĥà¸²à¸§', 'à¹Ī', 'à¸²à¹Ģà¸Ĥ', 'à¸²à¸Ħà¸§à¸£', 'à¸Ńà¸¢', 'à¸¹à¹Ī', 'à¸Ħà¸Ļà¹Ģà¸Ķ', 'à¸µ', 'à¸¢à¸§', 'Ġà¸¢', 'à¸±', 'à¸ĩà¹Ħà¸¡', 'à¹Ī', 'à¸ŀà¸£', 'à¹ī', 'à¸Ńà¸¡', 'à¸Īà¸°à¸£', 'à¸±', 'à¸ģà¹ĥà¸Ħà¸£', 'Ġà¸Īà¸Ļà¸ķà¸Ńà¸Ļà¸Ļ', 'à¸µà¹ī', 'à¹Ģà¸£à¸²à¹Ģà¸£', 'à¸´à¹Ī', 'à¸¡à¸£', 'à¸¹à¹ī', 'à¸ª', 'à¸¶', 'à¸ģà¸§', 'à¹Ī', 'à¸²', 'Ġà¸Ĺ', 'à¹į', 'à¸²à¹Ħà¸¡', 'à¹Ģà¸£à¸²à¸ķ', 'à¹ī', 'à¸Ńà¸ĩà¸Ĺ', 'à¹į', 'à¸²à¸Ĥà¸Ļà¸²à¸Ķà¸Ļ', 'à¸±à¹ī', 'à¸Ļ', 'Ġà¸ĸ', 'à¹ī', 'à¸²à¹Ģà¸Ĥ', 'à¸²à¸Īà¸°à¸£', 'à¸±', 'à¸ģ', 'Ġà¸£', 'à¸±', 'à¸ģà¸Ĺ', 'à¸µà¹Ī', 'à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¸ķ', 'à¸±', 'à¸§à¹Ģà¸£à¸²à¹Ħà¸¡', 'à¹Ī', 'à¹Ħà¸Ķ', 'à¹ī', 'à¸«à¸£à¸Ń', 'Ġà¸«à¸¥', 'à¸±', 'à¸ĩà¹Ĩ', 'à¹Ģà¸¥à¸¢à¹Ģà¸£', 'à¸´à¹Ī', 'à¸¡', 'à¸ªà¸Ļà¹ĥà¸Ī', 'à¹Ģà¸Ĥà¸²à¸Ļ', 'à¹ī', 'à¸Ńà¸¢à¸¥à¸ĩ', 'Ġà¹ģà¸ķ', 'à¹Ī', 'à¸¢', 'à¸±', 'à¸ĩà¸Ħ', 'à¸¸', 'à¸¢à¸ģ', 'à¸±', 'à¸Ļà¹Ģà¸«à¸¡', 'à¸·', 'à¸Ńà¸Ļà¹Ģà¸Ķ', 'à¸´', 'à¸¡', 'Ġà¹Ģà¸£', 'à¸²à¸¥à¸Ńà¸ĩ', 'à¹ģà¸ģà¸¥', 'à¹ī', 'à¸ĩà¹Ģà¸ĩ', 'à¸µ', 'à¸¢à¸ļà¹Ħà¸Ľ', 'Ġà¹Ħà¸¡', 'à¹Ī', 'à¸Ĺ', 'à¸±', 'à¸ģà¹Ħà¸Ľ', 'à¸Ħà¸£', 'à¸¶à¹Ī', 'à¸ĩà¸§', 'à¸±', 'à¸Ļ', 'Ġà¸Ľà¸ģà¸ķ', 'à¸´', 'à¹Ģà¸£à¸²à¸Īà¸°à¸¡', 'à¸µ', 'à¸ģà¸²à¸£à¸¡', 'à¸Ńà¸Ļ', 'à¸´à¹Ī', 'à¸ĩà¸ģ', 'à¸±', 'à¸Ļà¸ķà¸Ńà¸Ļ', 'à¹Ģà¸Ĭ', 'à¹ī', 'à¸²à¸Ħ', 'à¹Ī', 'à¸°', 'Ġà¸ŀà¸Ń', 'à¹Ģà¸£à¸²à¹Ħà¸¡', 'à¹Ī', 'à¸Ĺ', 'à¸±', 'à¸ģà¹Ħà¸Ľ', 'Ġà¹Ģà¸Ĥà¸²à¸Ĺ', 'à¹į', 'à¸²à¸ĩà¸²à¸Ļ', 'à¹Ģà¸ªà¸£', 'à¹ĩ', 'à¸Ī', 'Ġà¸ĸ', 'à¸¶', 'à¸ĩà¹Ģà¸§à¸¥', 'à¸²à¸ŀ', 'à¸±', 'à¸ģà¸Ĥà¸Ńà¸ĩ', 'à¹Ģà¸Ĥà¸²', 'Ġà¹Ģà¸Ĥà¸²à¸ģ', 'à¹ĩ', 'à¸Ĺ', 'à¸±', 'à¸ģà¸¡', 'à¸²à¸§', 'à¹Ī', 'à¸²', 'Ġà¸ģ', 'à¸´', 'à¸Ļà¸Ĥ', 'à¹ī', 'à¸²à¸§à¸ģ', 'à¸±', 'à¸Ļ', 'Ġà¹Ģà¸£à¸²à¸ģ', 'à¹ĩ', 'à¸¢', 'à¸´à¹Ī', 'à¸ĩ', 'Ġà¸ĩà¸ĩ', 'Ġà¸ģ', 'à¹ĩ', 'à¸Ħ', 'à¸´', 'à¸Ķà¸§', 'à¹Ī', 'à¸²', 'Ġà¹Ģà¸Ĥ', 'à¸²à¸Ńà¸²à¸Ī', 'à¸Īà¸°à¸Ĭ', 'à¸´', 'à¸Ļà¸«', 'à¸±', 'à¸ļà¸ģà¸²à¸£à¸Ħ', 'à¸¸', 'à¸¢à¸ģ', 'à¸±', 'à¸ļà¹Ģà¸£à¸²à¸Ĺ', 'à¸¸', 'à¸ģà¸§', 'à¸±', 'à¸Ļ', 'à¹Ģà¸īà¸¢à¹Ĩ', 'Ġà¸Ļ', 'à¸µà¹Ī', 'à¹Ģà¸¥à¸¢à¹Ħà¸¡', 'à¹Ī', 'à¹Ħà¸Ķ', 'à¹ī', 'à¸ªà¸Ļà¹ĥà¸Ī', 'à¹ĥà¸Ļà¸ª', 'à¹Ī', 'à¸§à¸Ļà¸Ļ', 'à¸±à¹ī', 'à¸Ļ', 'Ġà¹Ģà¸£à¸²à¸ģ', 'à¹ĩ', 'à¸ķà¸Ńà¸ļ', 'à¸ķà¸²à¸¡à¸Ľà¸ģà¸ķ', 'à¸´', 'Ġà¸Īà¸Ļà¹Ģà¸¡', 'à¸·à¹Ī', 'à¸Ńà¸Ħ', 'à¸·', 'à¸Ļà¸¡', 'à¸µ', 'à¸Ħà¸Ļ', 'à¸¡à¸²à¸Ĺ', 'à¸±', 'à¸ģà¹Ģà¸£', 'à¸²à¸Ī', 'à¸µ', 'à¸ļà¹Ģà¸£à¸²', 'Ġà¸Īà¸°', 'à¹Ħà¸Ľà¸ª', 'à¹Ī', 'à¸ĩà¹Ģà¸£à¸²à¸Ĺ', 'à¸µà¹Ī', 'à¸ļ', 'à¹ī', 'à¸²à¸Ļ', 'Ġà¹Ģà¸£à¸²à¸ģ', 'à¹ĩ', 'à¹Ģà¸¥à¸¢à¹Ģà¸¥', 'à¹Ī', 'à¸²à¹ĥà¸«', 'à¹ī', 'à¹Ģà¸Ĥ', 'à¸²à¸Ł', 'à¸±', 'à¸ĩà¸§', 'à¹Ī', 'à¸²', 'Ġà¹ĥà¸«', 'à¹ī', 'à¹Ħà¸¥à¸Ļ', 'à¹Į', 'à¹Ħà¸Ľ', 'Ġà¹ĥà¸«', 'à¹ī', 'à¹Ģà¸Ĥ', 'à¸²à¹Ħà¸Ľà¸ª', 'à¹Ī', 'à¸ĩà¸Ńà¸¢', 'à¸¹à¹Ī', 'à¹ģà¸ķ', 'à¹Ī', 'à¹Ħà¸¡', 'à¹Ħ', 'à¹Ī', 'à¸Ķ', 'à¹ī', 'à¸Ļ', 'à¸±à¹Ī', 'à¸ĩà¸£à¸ĸ', 'à¸Ħ', 'à¸±', 'à¸Ļà¹Ģà¸Ķ', 'à¸µ', 'à¸¢à¸§à¸ģ', 'à¸±', 'à¸Ļ', 'Ġà¹Ģà¸£à¸²à¸ģ', 'à¹ĩ', 'à¸Ĥ', 'à¸±', 'à¸ļà¸Ĥà¸Ńà¸ĩ', 'à¹Ģà¸£à¸²', 'Ġà¸Ħà¸Ļà¸Ĺ', 'à¸µà¹Ī', 'à¸¡à¸²à¸Ī', 'à¸µ', 'à¸ļà¹Ģà¸£à¸²', 'à¹Ģà¸Ĥà¸²à¸ģ', 'à¹ĩ', 'à¸Ĥ', 'à¸±', 'à¸ļà¸Ħ', 'à¸±', 'à¸Ļà¸Ĥà¸Ńà¸ĩ', 'à¹Ģà¸Ĥ', 'à¸²à¹ģà¸Ħ', 'à¹Ī', 'à¸¡à¸²à¸ª', 'à¹Ī', 'à¸ĩ', 'à¹Ģà¸īà¸¢à¹Ĩ', 'Ġà¸ŀà¸Ń', 'à¹Ģà¸Ĭ', 'à¹ī', 'à¸²à¸¡', 'à¸²à¹Ģà¸Ĥà¸²à¸Ĺ', 'à¸±', 'à¸ģà¸¡', 'à¸²à¸¡', 'à¸Ńà¸Ļ', 'à¸´à¹Ī', 'à¸ĩ', 'Ġà¸ģ', 'à¹ĩ', 'à¸ĸà¸²à¸¡', 'à¹Ģà¸£à¸²à¹Ģà¸£', 'à¸·à¹Ī', 'à¸Ńà¸ĩà¹Ģà¸¡', 'à¸·à¹Ī', 'à¸Ńà¸Ħ', 'à¸·', 'à¸Ļ', 'Ġà¹Ģà¸£à¸²à¸Ĺ', 'à¹į', 'à¸²à¸ĩà¸²à¸Ļà¸Ĺ', 'à¸µà¹Ī', 'à¸ģà¸¥', 'à¸±', 'à¸ļà¸Ķ', 'à¸¶', 'à¸ģà¸¡à¸²à¸ģà¹Ĩ', 'à¹Ħà¸¡', 'à¹Ī', 'à¹Ħà¸Ķ', 'à¹ī', 'à¸Ĺ', 'à¸±', 'à¸ģà¹Ħà¸Ľ', 'à¸ļà¸Ńà¸ģ', 'à¹Ģà¸Ĥà¸²', 'à¹Ħà¸§', 'à¹ī', 'à¸§', 'à¹Ī', 'à¸²', 'Ġà¸ĸ', 'à¸¶', 'à¸ĩà¸ļ', 'à¹ī', 'à¸²à¸Ļà¹ģà¸¥', 'à¹ī', 'à¸§à¸Ļà¸°', 'Ġà¹Ģà¸ĩ', 'à¸µ', 'à¸¢à¸ļ', 'à¹Ħà¸Ľà¸Ĺ', 'à¸±à¹ī', 'à¸ĩà¸Ħ', 'à¸·', 'à¸Ļà¹Ģà¸¥à¸¢', 'Ġà¹Ģà¸Ĥà¸²à¸ģ', 'à¹ĩ', 'à¸ĸà¸²à¸¡à¹Ģà¸£', 'à¸·à¹Ī', 'à¸Ńà¸ĩà¹Ģà¸¡', 'à¸·à¹Ī', 'à¸Ńà¸Ħ', 'à¸·', 'à¸Ļà¸§', 'à¹Ī', 'à¸²', 'Ġà¸«à¸Ļ', 'à¸¸à¹Ī', 'à¸¡', 'à¹Ħà¸Ľà¸ª', 'à¹Ī', 'à¸ĩà¸Ĺ', 'à¸µà¹Ī', 'à¸ļ', 'à¹ī', 'à¸²à¸Ļà¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¹Ħà¸ĩà¸ļ', 'à¹ī', 'à¸²à¸ĩ', 'Ġà¸ĸà¸²à¸¡', 'à¹ģà¸ķ', 'à¹Ī', 'à¹Ģà¸£', 'à¸·à¹Ī', 'à¸Ńà¸ĩà¸Ĥà¸Ńà¸ĩ', 'à¸ľ', 'à¸¹à¹ī', 'à¸Ĭà¸²à¸¢à¸Ĺ', 'à¸µà¹Ī', 'à¸¡à¸²à¸Ī', 'à¸µ', 'à¸ļà¹Ģà¸£à¸²à¸Ĺ', 'à¸±à¹ī', 'à¸ĩà¸§', 'à¸±', 'à¸Ļ', 'Ġà¸Īà¸Ļ', 'à¹Ģà¸£à¸²à¹Ģà¸Ľà¸¥', 'à¸µà¹Ī', 'à¸¢à¸Ļà¹Ģà¸£', 'à¸·à¹Ī', 'à¸Ńà¸ĩà¸ģ', 'à¹ĩ', 'à¸¢', 'à¸±', 'à¸ĩà¸ģà¸¥', 'à¸±', 'à¸ļ', 'à¸¡à¸²à¸ĸà¸²à¸¡', 'à¸Ń', 'à¸µ', 'à¸ģà¸£à¸Ńà¸ļ', 'Ġà¹Ħ', 'à¸Ńà¸Ńà¸²à¸ģà¸²à¸£', 'à¹ģà¸ļà¸ļà¸Ļ', 'à¸µà¹ī', 'à¸Ħ', 'à¸·', 'à¸Ńà¸Ńà¸°à¹Ħà¸£à¸Ħà¸°', 'Ġ?', 'Ġà¹Ħà¸«à¸Ļ', 'à¹Ģà¸Ĥà¸²à¸ļà¸Ńà¸ģ', 'à¸Ńà¸¢à¸²à¸ģà¸Ńà¸¢', 'à¸¹à¹Ī', 'à¸Ħà¸Ļà¹Ģà¸Ķ', 'à¸µ', 'à¸¢à¸§', 'Ġà¹ģà¸ķ', 'à¹Ī', 'à¸ŀà¸Ń', 'à¹Ģà¸£à¸²à¸¡', 'à¸µ', 'à¸Ħà¸Ļà¹Ģà¸Ĥ', 'à¹ī', 'à¸²à¸¡à¸²', 'Ġà¸Ĺ', 'à¹į', 'à¸²à¹Ħà¸¡', 'à¹Ģà¸Ĥà¸²à¸ĸ', 'à¸¶', 'à¸ĩà¸¡', 'à¸µ', 'à¸Ńà¸²à¸ģà¸²à¸£à¹ģà¸ļà¸ļà¸Ļ', 'à¸µà¹ī', 'Ġà¸¡', 'à¸²à¸ĸà¸²à¸¡', 'à¹ģà¸ļà¸ļà¸Ļ', 'à¸µà¹ī', 'à¸ĭ', 'à¹īà¹į', 'à¸²à¹Ĩ', 'Ġà¸Ħ', 'à¸·', 'à¸Ńà¸Ńà¸°à¹Ħà¸£à¸ģ', 'à¸±', 'à¸Ļ', 'Ġà¹Ģà¸£à¸²à¹Ħà¸¡', 'à¹Ī', 'à¸Ńà¸¢à¸²à¸ģà¸Ħ', 'à¸´', 'à¸Ķà¸Ńà¸°à¹Ħà¸£', 'à¹Ħà¸Ľà¹Ģà¸Ńà¸ĩ', 'Ġà¹ĥà¸Ħà¸£', 'à¸ŀà¸Ńà¸Īà¸°', 'à¸ķà¸Ńà¸ļà¹Ħà¸Ķ', 'à¹ī', 'à¸ļ', 'à¹ī', 'à¸²à¸ĩà¸Ħà¸°', 'Ġà¸§', 'à¹Ī', 'à¸²', 'Ġà¹Ħà¸Ń', 'à¹ģà¸ļà¸ļà¸Ļ', 'à¸µà¹ī', 'à¸¡', 'à¸±', 'à¸Ļà¸Ħ', 'à¸·', 'à¸Ńà¸Ńà¸°à¹Ħà¸£', 'Ġà¸£', 'à¸¹à¹ī', 'à¸ª', 'à¸¶', 'à¸ģà¸Ńà¸°à¹Ħà¸£', 'à¸Ńà¸¢', 'à¸¹à¹Ī']\n",
      "[' อาการแบบน', 'ี้', 'ค', 'ื', 'อไร', 'ก', 'ั', 'น', '?', ' ', ' เขาค', 'ุ', 'ยก', 'ั', 'บเร', 'ามา', ' 5', '-', '6', ' เด', 'ื', 'อน', ' เร', 'าตาม', 'จ', 'ี', 'บเข', 'านะคะ', ' ก', '็', 'ค', 'ุ', 'ยก', 'ั', 'นมา', ' ใน', 'ระยะเวล', 'าเข', 'าบอกว', '่', 'า', ' ถ', '้', 'าเร', 'าลด', ' นน', ' เพ', 'ื่', 'อเข', 'าได', '้', ' เขาจะ', 'ยอม', 'เป', '็', 'นแฟน', 'เรา', ' ตร', 'รก', 'ะ', 'โง', '่', 'มาก', 'นะคะ', ' แต', '่', 'ถามว', '่', 'าท', 'ํ', 'าม', 'ั้', 'ย', ' ท', 'ํ', 'าค', '่', 'ะ', ' พอไป', 'ร', 'ู้', 'ว', '่', 'าเขาค', 'ุ', 'ยก', 'ั', 'บเพ', 'ื่', 'อน', ' เพ', 'ื่', 'อนเข', 'าถามว', '่', 'า', ' ร', 'ู้', 'ส', 'ึ', 'กย', 'ั', 'งไงก', 'ั', 'บเรา', ' เข', 'าตอบ', 'เพ', 'ื่', 'อนว', '่', 'า', ' เขาว', '่', 'าเข', 'าควร', 'อย', 'ู่', 'คนเด', 'ี', 'ยว', ' ย', 'ั', 'งไม', '่', 'พร', '้', 'อม', 'จะร', 'ั', 'กใคร', ' จนตอนน', 'ี้', 'เราเร', 'ิ่', 'มร', 'ู้', 'ส', 'ึ', 'กว', '่', 'า', ' ท', 'ํ', 'าไม', 'เราต', '้', 'องท', 'ํ', 'าขนาดน', 'ั้', 'น', ' ถ', '้', 'าเข', 'าจะร', 'ั', 'ก', ' ร', 'ั', 'กท', 'ี่', 'เป', '็', 'นต', 'ั', 'วเราไม', '่', 'ได', '้', 'หรอ', ' หล', 'ั', 'งๆ', 'เลยเร', 'ิ่', 'ม', 'สนใจ', 'เขาน', '้', 'อยลง', ' แต', '่', 'ย', 'ั', 'งค', 'ุ', 'ยก', 'ั', 'นเหม', 'ื', 'อนเด', 'ิ', 'ม', ' เร', 'าลอง', 'แกล', '้', 'งเง', 'ี', 'ยบไป', ' ไม', '่', 'ท', 'ั', 'กไป', 'คร', 'ึ่', 'งว', 'ั', 'น', ' ปกต', 'ิ', 'เราจะม', 'ี', 'การม', 'อน', 'ิ่', 'งก', 'ั', 'นตอน', 'เช', '้', 'าค', '่', 'ะ', ' พอ', 'เราไม', '่', 'ท', 'ั', 'กไป', ' เขาท', 'ํ', 'างาน', 'เสร', '็', 'จ', ' ถ', 'ึ', 'งเวล', 'าพ', 'ั', 'กของ', 'เขา', ' เขาก', '็', 'ท', 'ั', 'กม', 'าว', '่', 'า', ' ก', 'ิ', 'นข', '้', 'าวก', 'ั', 'น', ' เราก', '็', 'ย', 'ิ่', 'ง', ' งง', ' ก', '็', 'ค', 'ิ', 'ดว', '่', 'า', ' เข', 'าอาจ', 'จะช', 'ิ', 'นห', 'ั', 'บการค', 'ุ', 'ยก', 'ั', 'บเราท', 'ุ', 'กว', 'ั', 'น', 'เฉยๆ', ' น', 'ี่', 'เลยไม', '่', 'ได', '้', 'สนใจ', 'ในส', '่', 'วนน', 'ั้', 'น', ' เราก', '็', 'ตอบ', 'ตามปกต', 'ิ', ' จนเม', 'ื่', 'อค', 'ื', 'นม', 'ี', 'คน', 'มาท', 'ั', 'กเร', 'าจ', 'ี', 'บเรา', ' จะ', 'ไปส', '่', 'งเราท', 'ี่', 'บ', '้', 'าน', ' เราก', '็', 'เลยเล', '่', 'าให', '้', 'เข', 'าฟ', 'ั', 'งว', '่', 'า', ' ให', '้', 'ไลน', '์', 'ไป', ' ให', '้', 'เข', 'าไปส', '่', 'งอย', 'ู่', 'แต', '่', 'ไม', 'ไ', '่', 'ด', '้', 'น', 'ั่', 'งรถ', 'ค', 'ั', 'นเด', 'ี', 'ยวก', 'ั', 'น', ' เราก', '็', 'ข', 'ั', 'บของ', 'เรา', ' คนท', 'ี่', 'มาจ', 'ี', 'บเรา', 'เขาก', '็', 'ข', 'ั', 'บค', 'ั', 'นของ', 'เข', 'าแค', '่', 'มาส', '่', 'ง', 'เฉยๆ', ' พอ', 'เช', '้', 'าม', 'าเขาท', 'ั', 'กม', 'าม', 'อน', 'ิ่', 'ง', ' ก', '็', 'ถาม', 'เราเร', 'ื่', 'องเม', 'ื่', 'อค', 'ื', 'น', ' เราท', 'ํ', 'างานท', 'ี่', 'กล', 'ั', 'บด', 'ึ', 'กมากๆ', 'ไม', '่', 'ได', '้', 'ท', 'ั', 'กไป', 'บอก', 'เขา', 'ไว', '้', 'ว', '่', 'า', ' ถ', 'ึ', 'งบ', '้', 'านแล', '้', 'วนะ', ' เง', 'ี', 'ยบ', 'ไปท', 'ั้', 'งค', 'ื', 'นเลย', ' เขาก', '็', 'ถามเร', 'ื่', 'องเม', 'ื่', 'อค', 'ื', 'นว', '่', 'า', ' หน', 'ุ่', 'ม', 'ไปส', '่', 'งท', 'ี่', 'บ', '้', 'านเป', '็', 'นไงบ', '้', 'าง', ' ถาม', 'แต', '่', 'เร', 'ื่', 'องของ', 'ผ', 'ู้', 'ชายท', 'ี่', 'มาจ', 'ี', 'บเราท', 'ั้', 'งว', 'ั', 'น', ' จน', 'เราเปล', 'ี่', 'ยนเร', 'ื่', 'องก', '็', 'ย', 'ั', 'งกล', 'ั', 'บ', 'มาถาม', 'อ', 'ี', 'กรอบ', ' ไ', 'ออาการ', 'แบบน', 'ี้', 'ค', 'ื', 'ออะไรคะ', ' ?', ' ไหน', 'เขาบอก', 'อยากอย', 'ู่', 'คนเด', 'ี', 'ยว', ' แต', '่', 'พอ', 'เราม', 'ี', 'คนเข', '้', 'ามา', ' ท', 'ํ', 'าไม', 'เขาถ', 'ึ', 'งม', 'ี', 'อาการแบบน', 'ี้', ' ม', 'าถาม', 'แบบน', 'ี้', 'ซ', '้ํ', 'าๆ', ' ค', 'ื', 'ออะไรก', 'ั', 'น', ' เราไม', '่', 'อยากค', 'ิ', 'ดอะไร', 'ไปเอง', ' ใคร', 'พอจะ', 'ตอบได', '้', 'บ', '้', 'างคะ', ' ว', '่', 'า', ' ไอ', 'แบบน', 'ี้', 'ม', 'ั', 'นค', 'ื', 'ออะไร', ' ร', 'ู้', 'ส', 'ึ', 'กอะไร', 'อย', 'ู่']\n"
     ]
    }
   ],
   "source": [
    "text = \"อาการแบบนี้คือไรกัน?  เขาคุยกับเรามา 5-6 เดือน เราตามจีบเขานะคะ ก็คุยกันมา ในระยะเวลาเขาบอกว่า ถ้าเราลด นน เพื่อเขาได้ เขาจะยอมเป็นแฟนเรา ตรรกะโง่มากนะคะ แต่ถามว่าทำมั้ย ทำค่ะ พอไปรู้ว่าเขาคุยกับเพื่อน เพื่อนเขาถามว่า รู้สึกยังไงกับเรา เขาตอบเพื่อนว่า เขาว่าเขาควรอยู่คนเดียว ยังไม่พร้อมจะรักใคร จนตอนนี้เราเริ่มรู้สึกว่า ทำไมเราต้องทำขนาดนั้น ถ้าเขาจะรัก รักที่เป็นตัวเราไม่ได้หรอ หลังๆเลยเริ่มสนใจเขาน้อยลง แต่ยังคุยกันเหมือนเดิม เราลองแกล้งเงียบไป ไม่ทักไปครึ่งวัน ปกติเราจะมีการมอนิ่งกันตอนเช้าค่ะ พอเราไม่ทักไป เขาทำงานเสร็จ ถึงเวลาพักของเขา เขาก็ทักมาว่า กินข้าวกัน เราก็ยิ่ง งง ก็คิดว่า เขาอาจจะชินหับการคุยกับเราทุกวันเฉยๆ นี่เลยไม่ได้สนใจในส่วนนั้น เราก็ตอบตามปกติ จนเมื่อคืนมีคนมาทักเราจีบเรา จะไปส่งเราที่บ้าน เราก็เลยเล่าให้เขาฟังว่า ให้ไลน์ไป ให้เขาไปส่งอยู่แต่ไมไ่ด้นั่งรถคันเดียวกัน เราก็ขับของเรา คนที่มาจีบเราเขาก็ขับคันของเขาแค่มาส่งเฉยๆ พอเช้ามาเขาทักมามอนิ่ง ก็ถามเราเรื่องเมื่อคืน เราทำงานที่กลับดึกมากๆไม่ได้ทักไปบอกเขาไว้ว่า ถึงบ้านแล้วนะ เงียบไปทั้งคืนเลย เขาก็ถามเรื่องเมื่อคืนว่า หนุ่มไปส่งที่บ้านเป็นไงบ้าง ถามแต่เรื่องของผู้ชายที่มาจีบเราทั้งวัน จนเราเปลี่ยนเรื่องก็ยังกลับมาถามอีกรอบ ไออาการแบบนี้คืออะไรคะ ? ไหนเขาบอกอยากอยู่คนเดียว แต่พอเรามีคนเข้ามา ทำไมเขาถึงมีอาการแบบนี้ มาถามแบบนี้ซ้ำๆ คืออะไรกัน เราไม่อยากคิดอะไรไปเอง ใครพอจะตอบได้บ้างคะ ว่า ไอแบบนี้มันคืออะไร รู้สึกอะไรอยู่\"\n",
    "encoded = tokenizer.encode(text)\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)\n",
    "print(list(map(lambda x : tokenizer.decode([x]), encoded.ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we want to use it again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoding structure exposes multiple properties which are useful when working with transformers models\n",
    "\n",
    "- normalized_str: The input string after normalization (lower-casing, unicode, stripping, etc.)\n",
    "- original_str: The input string as it was provided\n",
    "- tokens: The generated tokens with their string representation\n",
    "- input_ids: The generated tokens with their integer representation\n",
    "- attention_mask: If your input has been padded by the tokenizer, then this would be a vector of 1 for any non padded token and 0 for padded ones.\n",
    "- special_token_mask: If your input contains special tokens such as [CLS], [SEP], [MASK], [PAD], then this would be a vector with 1 in places where a special token has been added.\n",
    "- type_ids: If your input was made of multiple \"parts\" such as (question, context), then this would be a vector with for each token the segment it belongs to.\n",
    "- overflowing: If your input has been truncated into multiple subparts because of a length limit (for BERT for example the sequence length is limited to 512), this will contain all the remaining overflowing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1125, 275, 566, 278, 333, 275, 282, 16456, 327, 2565, 368, 317, 830, 340, 274, 301, 334, 301, 13155, 302, 3555, 271, 1303, 1468, 278, 6036, 271, 279, 225, 407, 302, 283, 296, 1273, 444, 271, 19117, 785, 14369, 278, 333, 275, 282, 225, 1986, 271, 12879, 301, 11970, 1474, 793, 1050]\n",
      "['Ġà¸ªà¸§', 'à¸±', 'à¸ªà¸Ķ', 'à¸µ', 'à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġà¸ľà¸¡à¸Ĭ', 'à¸·à¹Ī', 'à¸Ńà¹Ħ', 'à¸Ļà¸Ĺ', 'à¹Į', 'Ġà¸ķà¸Ńà¸Ļà¸Ļ', 'à¸µà¹ī', 'à¸ģ', 'à¹ĩ', 'à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¹Ģà¸§à¸¥à¸²à¸Ĺ', 'à¸µà¹Ī', 'à¸ľà¸¡à¸ķ', 'à¹ī', 'à¸Ńà¸ĩà¹Ħà¸Ľ', 'à¹Ĥà¸£à¸ĩà¹Ģà¸£', 'à¸µ', 'à¸¢à¸Ļà¹ģà¸¥', 'à¹ī', 'à¸§', 'Ġ', 'Ġà¸Ļ', 'à¸µà¹Ī', 'à¸Ħ', 'à¸·', 'à¸Ńà¸ģà¸²à¸£', 'à¹Ģà¸§', 'à¹ī', 'à¸Ļà¸§à¸£', 'à¸£à¸Ħ', 'à¸ªà¸Ńà¸ĩà¸Ĺ', 'à¸µ', 'à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġ', 'Ġà¸Īà¸°à¹Ħà¸Ķ', 'à¹ī', 'à¸Ńà¸Ńà¸ģà¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¸ªà¸Ńà¸ĩ', 'Ġsp', 'ac', 'es']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"./all-data-bytebpe-30522.tokenizer.json\")\n",
    "encoded =  tokenizer.encode(u\"สวัสดีครับ ผมชื่อไนท์ ตอนนี้ก็เป็นเวลาที่ผมต้องไปโรงเรียนแล้ว  นี่คือการเว้นวรรคสองทีครับ  จะได้ออกเป็นสอง Spaces\")\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
