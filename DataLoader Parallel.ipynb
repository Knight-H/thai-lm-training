{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import CharBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "\n",
    "from transformers import RobertaTokenizerFast, RobertaTokenizer\n",
    "\n",
    "import random\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from filelock import FileLock\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Path file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT_DIR = \"/datadisk/data/raw_data_extraction_v2\"\n",
    "ROOT_DIR = \"/workdir/Code/bma_transformer_model/data/raw_data_extraction\"\n",
    "SELECTOR_DIR = [\"classification_dataset\"]\n",
    "# SELECTOR_DIR = [\"another_website\"]\n",
    "\n",
    "PATH_FILES_SAMPLE = []\n",
    "for selector_dir in SELECTOR_DIR:\n",
    "    for root, dirs, files in os.walk(os.path.join(ROOT_DIR, selector_dir)):\n",
    "        for file in files:\n",
    "            PATH_FILES_SAMPLE.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/siamrath_0.txt',\n",
       " '/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/dailynews_0.txt',\n",
       " '/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/prachachat_0.txt',\n",
       " '/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/naewna_0.txt',\n",
       " '/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/springnews_0.txt',\n",
       " '/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/pptv36_0.txt',\n",
       " '/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/prbangkok_0.txt',\n",
       " '/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/thaipbs_0.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_FILES_SAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader (Single Core)\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/siamrath_0.txt\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/dailynews_0.txt\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/prachachat_0.txt\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/naewna_0.txt\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/springnews_0.txt\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/pptv36_0.txt\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/prbangkok_0.txt\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/thaipbs_0.txt\n",
    "\n",
    "#### Processing Time: 71.71228837966919"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, sample_path: [], block_size: int, overwrite_cache=False,):\n",
    "        # assert os.path.isfile(file_path)\n",
    "        # For Loop MultiFile\n",
    "        self.examples = []\n",
    "        #cached_directory = \"/datadisk/cached_data\"\n",
    "        cached_directory = \"/workdir/Code/bma_transformer_model/data/cached_data\"\n",
    "        for file_path in sample_path:\n",
    "            print(\"file_path\", file_path)\n",
    "            block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n",
    "\n",
    "            directory, filename = os.path.split(file_path)\n",
    "            cached_features_file = os.path.join(\n",
    "                cached_directory, \"cached_lm_{}_{}_{}\".format(tokenizer.__class__.__name__, str(block_size), filename,),\n",
    "            )\n",
    "\n",
    "            # Make sure only the first process in distributed training processes the dataset,\n",
    "            # and the others will use the cache.\n",
    "            lock_path = cached_features_file + \".lock\"\n",
    "\n",
    "            with FileLock(lock_path):\n",
    "                if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                    start = time.time()\n",
    "                    with open(cached_features_file, \"rb\") as handle:\n",
    "                        self.examples = pickle.load(handle)\n",
    "                    logger.info(\n",
    "                        f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    with open(file_path, encoding=\"utf-8\") as f:\n",
    "                        text = f.read()\n",
    "\n",
    "                    tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "                    for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n",
    "                        self.examples.append(\n",
    "                            tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size])\n",
    "                        )\n",
    "                    # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n",
    "                    # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
    "                    # can change this behavior by adding (model specific) padding.\n",
    "\n",
    "                    start = time.time()\n",
    "                    with open(cached_features_file, \"wb\") as handle:\n",
    "                        pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    logger.info(\n",
    "                        \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                    )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        print(\"self.examples\", self.examples)\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/siamrath_0.txt\n",
      "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/dailynews_0.txt\n",
      "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/prachachat_0.txt\n",
      "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/naewna_0.txt\n",
      "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/springnews_0.txt\n",
      "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/pptv36_0.txt\n",
      "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/prbangkok_0.txt\n",
      "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/thaipbs_0.txt\n",
      "71.71228837966919\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "tic = time.time()\n",
    "dataset = TextDataset(tokenizer, sample_path=PATH_FILES_SAMPLE, block_size=512, overwrite_cache=True)\n",
    "print(time.time() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea (Multi Core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME MAP 0.0020973682403564453\n",
      "TIME LOOP 0.002908468246459961\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "pool = multiprocessing.Pool()\n",
    "tic = time.time()\n",
    "num_loop = 10**4\n",
    "pool.map(f, range(1000))\n",
    "# print(pool.map(f, range(1000)))\n",
    "print(\"TIME MAP\", time.time() - tic)\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(0, num_loop):\n",
    "    f(i)\n",
    "print(\"TIME LOOP\", time.time() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader (Multi Core)\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/siamrath_0.txt\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/dailynews_0.txt\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/prachachat_0.txt\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/naewna_0.txt\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/springnews_0.txt\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/pptv36_0.txt\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/prbangkok_0.txt\n",
    "file_path /workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/thaipbs_0.txt\n",
    "\n",
    "#### Processing Time: 41.26610994338989 >> pool = multiprocessing.Pool(processes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "class TextDatasetParallel(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"         \n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, sample_path: [], block_size: int, overwrite_cache=False,):\n",
    "        # assert os.path.isfile(file_path)\n",
    "        # For Loop MultiFile\n",
    "        self.examples = []\n",
    "        self.sample_path = sample_path\n",
    "        self.block_size = block_size\n",
    "        self.overwrite_cache = overwrite_cache\n",
    "        #print(\"CPU\", multiprocessing.cpu_count())\n",
    "        pool = multiprocessing.Pool(processes=8)\n",
    "        self.examples = pool.map(self.load_data_tokenized, self.sample_path)\n",
    "\n",
    "    def load_data_tokenized(self, file_path):\n",
    "        # print(\"TEST_\", self.block_size)\n",
    "        #cached_directory = \"/datadisk/cached_data\"\n",
    "        cached_directory = \"/workdir/Code/bma_transformer_model/data/cached_data\"\n",
    "        self.block_size = self.block_size - tokenizer.num_special_tokens_to_add(pair=False)\n",
    "\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            cached_directory, \"cached_lm_{}_{}_{}\".format(tokenizer.__class__.__name__, str(self.block_size), filename,),\n",
    "        )\n",
    "\n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "            if os.path.exists(cached_features_file) and not self.overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    self.examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "            else:\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "\n",
    "                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "                for i in range(0, len(tokenized_text) - self.block_size + 1, self.block_size):  # Truncate in block of block_size\n",
    "                    #print(\">>> APPEND <<<\")\n",
    "                    self.examples.append(\n",
    "                        tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + self.block_size])\n",
    "                    )\n",
    "                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n",
    "                # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
    "                # can change this behavior by adding (model specific) padding.\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "        return self.examples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        #print(\"self.examples\", self.examples)\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Path File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT_DIR = \"/datadisk/data/raw_data_extraction_v2\"\n",
    "ROOT_DIR = \"/workdir/Code/bma_transformer_model/data/raw_data_extraction\"\n",
    "SELECTOR_DIR = [\"classification_dataset\", \"another_website\"]\n",
    "# SELECTOR_DIR = [\"another_website\"]\n",
    "\n",
    "PATH_FILES_SAMPLE = []\n",
    "for selector_dir in SELECTOR_DIR:\n",
    "    for root, dirs, files in os.walk(os.path.join(ROOT_DIR, selector_dir)):\n",
    "        for file in files:\n",
    "            PATH_FILES_SAMPLE.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/siamrath_0.txt',\n",
       " '/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/dailynews_0.txt',\n",
       " '/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/prachachat_0.txt',\n",
       " '/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/naewna_0.txt',\n",
       " '/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/springnews_0.txt',\n",
       " '/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/pptv36_0.txt',\n",
       " '/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/prbangkok_0.txt',\n",
       " '/workdir/Code/bma_transformer_model/data/raw_data_extraction/classification_dataset/thaipbs_0.txt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_FILES_SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 8\n",
      "TEST_TEST_  512512\n",
      "\n",
      "TEST_ 512\n",
      "TEST_ 512\n",
      "TEST_ 512\n",
      "TEST_ 512\n",
      "TEST_ 512\n",
      "TEST_ 512\n",
      "50.652223348617554\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "tic = time.time()\n",
    "dataset = TextDatasetParallel(tokenizer, sample_path=PATH_FILES_SAMPLE, block_size=512, overwrite_cache=True)\n",
    "print(time.time() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 48759, 24107,  ..., 24107,  2469,     2],\n",
       "        [    0,    17,    46,  ...,  6800,  1437,     2],\n",
       "        [    0, 24107, 23133,  ...,  4034,     6,     2],\n",
       "        ...,\n",
       "        [    0, 24107,  4958,  ..., 24107,  6382,     2],\n",
       "        [    0, 42348, 10172,  ..., 23133, 24107,     2],\n",
       "        [    0, 12410, 42348,  ..., 42348,  8384,     2]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 24107, 15722,  ..., 18537, 24107,     2],\n",
       "        [    0,    27, 42348,  ..., 24107,  3726,     2],\n",
       "        [    0, 42348, 10172,  ..., 42348, 23171,     2],\n",
       "        ...,\n",
       "        [    0, 42348,  7471,  ..., 24107, 16948,     2],\n",
       "        [    0, 24107, 14292,  ...,  3726, 24107,     2],\n",
       "        [    0,  2469, 24107,  ..., 24107, 15389,     2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
