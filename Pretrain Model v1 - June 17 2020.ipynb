{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How to train a language model__\tNotebook to Highlight all the steps to effectively train Transformer model on custom data\n",
    "https://github.com/huggingface/transformers/tree/master/notebooks\n",
    "\n",
    "https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb\n",
    "\n",
    "__Language Modeling__\n",
    "https://github.com/huggingface/transformers/tree/master/examples/language-modeling\n",
    "https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import CharBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "\n",
    "from transformers import RobertaTokenizerFast, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Electra Model\n",
    "\n",
    "However, currently pretraining Electra is still inside PR stage  \n",
    "- Issue : When will ELECTRA pretraining from scratch will be available? #3878 https://github.com/huggingface/transformers/issues/3878  \n",
    "- Issue : BERT and other models pretraining from scratch example #4425 https://github.com/huggingface/transformers/issues/4425\n",
    "- PR : Electra training from scratch #4656 https://github.com/huggingface/transformers/pull/4656\n",
    "\n",
    "Combined model\n",
    "- Combines ElectraForMaskedLM and ElectraForPreTraining with embedding sharing + custom masking/replaced token detection\n",
    "\n",
    "Commit before Merge: https://github.com/huggingface/transformers/pull/4656/commits/30b2dbbba6918ac6540b6a1758b7ee19f0ac969c#diff-8a95e2bfb7da25648b5d12ffa69fd7a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"embedding_size\": 128,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1024,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"electra\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 20000\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ElectraModel, ElectraConfig\n",
    "\n",
    "# Initializing a ELECTRA electra-base-uncased style configuration\n",
    "configuration = ElectraConfig()\n",
    "configuration.vocab_size = 20000\n",
    "\n",
    "# Initializing a model from the electra-base-uncased style configuration\n",
    "model = ElectraModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraModel(\n",
       "  (embeddings): ElectraEmbeddings(\n",
       "    (word_embeddings): Embedding(20000, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12136192"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()\n",
    "# => 12 million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(20000, 128, padding_idx=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out Roberta per Notebook \n",
    "\n",
    "From __HuggingFace Notebooks__ https://huggingface.co/transformers/notebooks.html: \n",
    "\n",
    "How to train a language model\tHighlight all the steps to effectively train Transformer model on custom data\n",
    "- Colab (ipynb) version : https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb\n",
    "- MD version: https://github.com/huggingface/blog/blob/master/how-to-train.md\n",
    "\n",
    "Pretrain Longformer\tHow to build a \"long\" version of existing pretrained models\tIz Beltagy  \n",
    "https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "configuration = RobertaConfig(\n",
    "#     vocab_size=20000,\n",
    "    max_position_embeddings=514, # 512 + 2 more special tokens\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "# configuration.vocab_size = 20000\n",
    "\n",
    "model = RobertaForMaskedLM(config=configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110105658"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()\n",
    "# => 102 million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15906, 1204, 18091, 13184, 1257, 12938, 3426, 9773, 5391, 1005, 4462, 1397, 1096, 1751, 1364, 527, 7777, 5202, 4057, 10411, 1564, 1317, 18091, 527, 8768, 3127, 1564, 6927, 2113, 1739]\n",
      "['▁สวัส', 'ดี', 'ครับ', '▁ผม', 'ชื่อ', 'ไนท์', '▁ตอน', 'นี้ก็', 'เป็นเวลา', 'ที่', 'ผม', 'ต้อง', 'ไป', 'โรงเรียน', 'แล้ว', '▁', '▁นี่', 'คือการ', 'เว้น', 'วรรค', 'สอง', 'ที', 'ครับ', '▁', '▁จะได้', 'ออกเป็น', 'สอง', '▁Sp', 'ac', 'es']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"./thwiki-sentencepiecebpe.tokenizer.json\")\n",
    "encoded =  tokenizer.encode(u\"สวัสดีครับ ผมชื่อไนท์ ตอนนี้ก็เป็นเวลาที่ผมต้องไปโรงเรียนแล้ว  นี่คือการเว้นวรรคสองทีครับ  จะได้ออกเป็นสอง Spaces\")\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.enable_truncation(max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will not be over 128:  128 ['▁สวัส', 'ดี', 'ครับ', '▁ผม', 'ชื่อ', 'ไนท์', '▁ตอน', 'นี้ก็', 'เป็นเวลา', 'ที่', 'ผม', 'ต้อง', 'ไป', 'โรงเรียน', 'แล้ว', '▁', '▁นี่', 'คือการ', 'เว้น', 'วรรค', 'สอง', 'ที', 'ครับ', '▁', '▁จะได้', 'ออกเป็น', 'สอง', '▁Sp', 'ac', 'es', 'W', 'h', 'at', '▁is', '▁g', 'reat', '▁is', '▁th', 'at', '▁o', 'ur', '▁to', 'k', 'en', 'iz', 'er', '▁is', '▁o', 'p', 'tim', 'iz', 'ed', '▁for', '▁E', 's', 'per', 'ant', 'o', '.', '▁Comp', 'ar', 'ed', '▁to', '▁a', '▁g', 'ener', 'ic', '▁to', 'k', 'en', 'iz', 'er', '▁tr', 'ain', 'ed', '▁for', '▁Eng', 'l', 'ish', ',', '▁m', 'ore', '▁n', 'ative', '▁w', 'ord', 's', '▁', 'are', '▁re', 'p', 'res', 'ent', 'ed', '▁by', '▁a', '▁s', 'ing', 'le', ',', '▁un', 's', 'pl', 'it', '▁to', 'k', 'en', '.', '▁D', 'ia', 'c', 'ri', 'tic', 's,', '▁i', '.', 'e', '.', '▁ac', 'c', 'ent', 'ed', '▁ch', 'ar', 'ac', 'ter', 's', '▁']\n",
      "['us', 'ed', '▁in', '▁E', 's', 'per', 'ant', 'o', '▁–', '▁,', '▁,', '▁,', '▁,', '▁,', '▁and', '▁', '▁–', '▁', 'are', '▁', 'enc', 'od', 'ed', '▁n', 'ati', 'v', 'el', 'y', '.', '▁We', '▁al', 's', 'o', '▁re', 'p', 'res', 'ent', '▁s', 'e', 'qu', 'enc', 'es', '▁in', '▁a', '▁m', 'ore', '▁e', 'ff', 'ic', 'i', 'ent', '▁m', 'ann', 'er', '.', '▁H', 'ere', '▁on', '▁th', 'is', '▁cor', 'p', 'us', ',', '▁the', '▁a', 'ver', 'age', '▁le', 'ng', 'th', '▁of', '▁', 'enc', 'od', 'ed', '▁s', 'e', 'qu', 'enc', 'es', '▁is', '▁', '~', '30', '%', '▁s', 'm', 'al', 'ler', '▁as', '▁wh', 'en', '▁', 'us', 'ing', '▁the', '▁p', 're', 'tr', 'ain', 'ed', '▁G', 'P', 'T', '-2', '▁to', 'k', 'en', 'iz', 'er', '.']\n"
     ]
    }
   ],
   "source": [
    "encoded =  tokenizer.encode(u\"สวัสดีครับ ผมชื่อไนท์ ตอนนี้ก็เป็นเวลาที่ผมต้องไปโรงเรียนแล้ว  นี่คือการเว้นวรรคสองทีครับ  จะได้ออกเป็นสอง SpacesWhat is great is that our tokenizer is optimized for Esperanto. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. Diacritics, i.e. accented characters used in Esperanto – ĉ, ĝ, ĥ, ĵ, ŝ, and ŭ – are encoded natively. We also represent sequences in a more efficient manner. Here on this corpus, the average length of encoded sequences is ~30% smaller as when using the pretrained GPT-2 tokenizer.\")\n",
    "print(\"This will not be over 128: \", len(encoded.ids), encoded.tokens)\n",
    "print(encoded.overflowing[0].tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wrap tokenizers inside a PreTrainedTokenizerFast from transformers \n",
    "\n",
    "https://github.com/huggingface/tokenizers/issues/259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "class SentencePieceBPETokenizerFast(PreTrainedTokenizerFast):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        merges_file,\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        sep_token=\"</s>\",\n",
    "        cls_token=\"<s>\",\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        mask_token=\"<mask>\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            SentencePieceBPETokenizer(\n",
    "                vocab_file=vocab_file,\n",
    "                merges_file=merges_file,\n",
    "            ),\n",
    "             bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# with open(\"./thwiki-sentencepiecebpe.tokenizer.json\", 'r' ) as json_data:\n",
    "with open(\"./thwiki-charbpe-30522.tokenizer.json\", 'r' ) as json_data:\n",
    "     data = json.load(json_data)\n",
    "vocab = data['model']['vocab']\n",
    "merges = data['model']['merges']\n",
    "\n",
    "\n",
    "with open('vocab.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(vocab, json_file, ensure_ascii=False)\n",
    "with open('merges.txt', 'w', encoding='utf-8') as f:\n",
    "    for merge_string in merges:\n",
    "        f.write(f'{merge_string}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_tokenizer = SentencePieceBPETokenizerFast(vocab_file='vocab.json',merges_file ='merges.txt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_additional_special_tokens',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_from_pretrained',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_mask_token',\n",
       " '_maybe_update_backend',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_sep_token',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'clean_up_tokenization',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'special_tokens_map',\n",
       " 'tokenize',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pretrain_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.tokenization_utils_fast.PreTrainedTokenizerFast at 0x7f42f0f257d0>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "# from tokenizers import Tokenizer\n",
    "# from tokenizers.implementations import BaseTokenizer\n",
    "\n",
    "# tokenizer = Tokenizer.from_file(\"./thwiki-sentencepiecebpe.tokenizer.json\")\n",
    "# base_tokenizer = BaseTokenizer(tokenizer) # Wrapper!! to PretrainTokenizerFast Tokenizer should be an instance of a Tokenizer provided by HuggingFace tokenizers library.\n",
    "# # base_tokenizer = SentencePieceBPETokenizer()\n",
    "# pretrain_tokenizer = PreTrainedTokenizerFast(tokenizer=base_tokenizer)\n",
    "# pretrain_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.tokenization_utils_fast.PreTrainedTokenizerFast at 0x7f42f03a7610>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, CharBPETokenizer\n",
    "from tokenizers.implementations import BaseTokenizer\n",
    "\n",
    "# tokenizer = Tokenizer.from_file(\"./thwiki-charbpe-30522.tokenizer.json\")\n",
    "# base_tokenizer = BaseTokenizer(tokenizer) # Wrapper!! to PretrainTokenizerFast Tokenizer should be an instance of a Tokenizer provided by HuggingFace tokenizers library.\n",
    "base_tokenizer = CharBPETokenizer(vocab_file='vocab.json',merges_file ='merges.txt')\n",
    "pretrain_tokenizer = PreTrainedTokenizerFast(tokenizer=base_tokenizer)\n",
    "pretrain_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "pretrain_tokenizer = RobertaTokenizerFast(vocab_file='vocab.json',merges_file ='merges.txt', max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "pretrain_tokenizer = RobertaTokenizerFast.from_pretrained(\".\", max_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building our dataset\n",
    "\n",
    "Build it with `from torch.utils.data.dataset import Dataset` just like [TextDataset](https://github.com/huggingface/transformers/blob/448c467256332e4be8c122a159b482c1ef039b98/src/transformers/data/datasets/language_modeling.py) and [LineByLineTextDataset](https://github.com/huggingface/transformers/blob/448c467256332e4be8c122a159b482c1ef039b98/src/transformers/data/datasets/language_modeling.py)\n",
    "\n",
    "padding documentation [link](https://github.com/huggingface/tokenizers/blob/master/bindings/python/tokenizers/implementations/base_tokenizer.py#L52)\n",
    "\n",
    "Potential Improvements\n",
    "- การทำให้ Dataset นั้น dynamically tokenize + dynamically open file : ตอนนี้เวลาทำ Dataset จาก torch.utils.data.dataset จะทำการ tokenize เลยตอนอยู่ใน constructor  , กำลังคิดว่าถ้าเกิดว่า Data ใหญ่มากๆ อาจจะไม่เหมาะสมกับการทำแบบนี้  เพราะว่า Ram จะต้องมีขนาดเท่าๆกับ data ที่เราใส่เข้าไป  ซึ่งเป็นไปได้ยากหาก Data มีขนาดใหญ่มากๆ   ผมได้ทำการ Search ดูแล้วก็พบว่าจาก Discussion Forum ของ Pytorch: https://discuss.pytorch.org/t/how-to-use-a-huge-line-corpus-text-with-dataset-dataloader/30872 \n",
    "Option1: ใช้ pd.Dataframe ในการเปิด File แบบ small chunks of data https://discuss.pytorch.org/t/data-processing-as-a-batch-way/14154/4?u=ptrblck\n",
    "Option2: ใช้ byte Offsets จากไฟล์ใหญ่ๆเพื่อที่จะ lookup .seek(): https://github.com/pytorch/text/issues/130#issuecomment-510412877\n",
    "More Examples: https://github.com/pytorch/text/blob/master/torchtext/datasets/unsupervised_learning.py , https://github.com/pytorch/text/blob/a5880a3da7928dd7dd529507eec943a307204de7/examples/text_classification/iterable_train.py#L169-L214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class THWikiDataset(Dataset):\n",
    "    def __init__(self, evaluate = False, block_size=32):\n",
    "#         tokenizer = Tokenizer.from_file(\"./thwiki-charbpe-30522.tokenizer.json\")\n",
    "#         tokenizer = CharBPETokenizer(vocab_file='vocab.json',merges_file ='merges.txt' )\n",
    "        self.tokenizer = RobertaTokenizerFast(vocab_file='vocab.json',merges_file ='merges.txt', max_len=512)\n",
    "#         no_accent_strip = BertNormalizer(strip_accents=False)\n",
    "#         tokenizer._tokenizer.normalizer = no_accent_strip\n",
    "#         tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "#             (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "#             (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "#         )\n",
    "#         tokenizer.enable_truncation(max_length=512)\n",
    "#         tokenizer.enable_padding()\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "        self.examples = []\n",
    "        \n",
    "        # Use AA and AB as train, AC as test\n",
    "        src_files = Path(\"../data/text/AC/\").glob(\"wiki*\") if evaluate else Path(\"../data/text/\").glob(\"A[AB]/wiki*\")\n",
    "        for src_file in src_files:\n",
    "            print(\"🔥\", src_file)\n",
    "            lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "#             self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "            batch_encoding = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=block_size)\n",
    "            self.examples = batch_encoding[\"input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # We’ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_04\n",
      "🔥 ../data/text/AA/wiki_95\n",
      "🔥 ../data/text/AA/wiki_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_26\n",
      "🔥 ../data/text/AA/wiki_61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_34\n",
      "🔥 ../data/text/AA/wiki_78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_08\n",
      "🔥 ../data/text/AA/wiki_14\n",
      "🔥 ../data/text/AA/wiki_13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_02\n",
      "🔥 ../data/text/AA/wiki_37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_20\n",
      "🔥 ../data/text/AA/wiki_57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_15\n",
      "🔥 ../data/text/AA/wiki_03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_49\n",
      "🔥 ../data/text/AA/wiki_87\n",
      "🔥 ../data/text/AA/wiki_19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_99\n",
      "🔥 ../data/text/AA/wiki_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_31\n",
      "🔥 ../data/text/AA/wiki_91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_25\n",
      "🔥 ../data/text/AA/wiki_76\n",
      "🔥 ../data/text/AA/wiki_94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_77\n",
      "🔥 ../data/text/AA/wiki_35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_23\n",
      "🔥 ../data/text/AA/wiki_92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_58\n",
      "🔥 ../data/text/AA/wiki_72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_18\n",
      "🔥 ../data/text/AA/wiki_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_70\n",
      "🔥 ../data/text/AA/wiki_36\n",
      "🔥 ../data/text/AA/wiki_80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_51\n",
      "🔥 ../data/text/AA/wiki_32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_71\n",
      "🔥 ../data/text/AA/wiki_69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_40\n",
      "🔥 ../data/text/AA/wiki_52\n",
      "🔥 ../data/text/AA/wiki_42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_93\n",
      "🔥 ../data/text/AA/wiki_98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_22\n",
      "🔥 ../data/text/AA/wiki_07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_53\n",
      "🔥 ../data/text/AA/wiki_54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_28\n",
      "🔥 ../data/text/AA/wiki_48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_43\n",
      "🔥 ../data/text/AA/wiki_01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_62\n",
      "🔥 ../data/text/AA/wiki_83\n",
      "🔥 ../data/text/AA/wiki_56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_10\n",
      "🔥 ../data/text/AA/wiki_73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_86\n",
      "🔥 ../data/text/AA/wiki_39\n",
      "🔥 ../data/text/AA/wiki_47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_05\n",
      "🔥 ../data/text/AA/wiki_24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_97\n",
      "🔥 ../data/text/AA/wiki_38\n",
      "🔥 ../data/text/AA/wiki_81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_00\n",
      "🔥 ../data/text/AA/wiki_63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_64\n",
      "🔥 ../data/text/AA/wiki_85\n",
      "🔥 ../data/text/AA/wiki_65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_06\n",
      "🔥 ../data/text/AA/wiki_79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_17\n",
      "🔥 ../data/text/AA/wiki_66\n",
      "🔥 ../data/text/AA/wiki_90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_68\n",
      "🔥 ../data/text/AA/wiki_46\n",
      "🔥 ../data/text/AA/wiki_84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_60\n",
      "🔥 ../data/text/AA/wiki_45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_33\n",
      "🔥 ../data/text/AA/wiki_44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_30\n",
      "🔥 ../data/text/AA/wiki_27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_41\n",
      "🔥 ../data/text/AA/wiki_96\n",
      "🔥 ../data/text/AA/wiki_09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_67\n",
      "🔥 ../data/text/AA/wiki_89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_21\n",
      "🔥 ../data/text/AA/wiki_59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_74\n",
      "🔥 ../data/text/AA/wiki_55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_88\n",
      "🔥 ../data/text/AA/wiki_82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AA/wiki_29\n",
      "🔥 ../data/text/AA/wiki_50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_04\n",
      "🔥 ../data/text/AB/wiki_95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_12\n",
      "🔥 ../data/text/AB/wiki_26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_61\n",
      "🔥 ../data/text/AB/wiki_34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_78\n",
      "🔥 ../data/text/AB/wiki_08\n",
      "🔥 ../data/text/AB/wiki_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_13\n",
      "🔥 ../data/text/AB/wiki_02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_37\n",
      "🔥 ../data/text/AB/wiki_20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_57\n",
      "🔥 ../data/text/AB/wiki_15\n",
      "🔥 ../data/text/AB/wiki_03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_49\n",
      "🔥 ../data/text/AB/wiki_87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_19\n",
      "🔥 ../data/text/AB/wiki_99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_11\n",
      "🔥 ../data/text/AB/wiki_31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_91\n",
      "🔥 ../data/text/AB/wiki_25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_76\n",
      "🔥 ../data/text/AB/wiki_94\n",
      "🔥 ../data/text/AB/wiki_77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_35\n",
      "🔥 ../data/text/AB/wiki_23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_92\n",
      "🔥 ../data/text/AB/wiki_58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_72\n",
      "🔥 ../data/text/AB/wiki_18\n",
      "🔥 ../data/text/AB/wiki_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_70\n",
      "🔥 ../data/text/AB/wiki_36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_80\n",
      "🔥 ../data/text/AB/wiki_51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_32\n",
      "🔥 ../data/text/AB/wiki_71\n",
      "🔥 ../data/text/AB/wiki_69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_40\n",
      "🔥 ../data/text/AB/wiki_52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_42\n",
      "🔥 ../data/text/AB/wiki_93\n",
      "🔥 ../data/text/AB/wiki_98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_22\n",
      "🔥 ../data/text/AB/wiki_07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_53\n",
      "🔥 ../data/text/AB/wiki_54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_28\n",
      "🔥 ../data/text/AB/wiki_48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_43\n",
      "🔥 ../data/text/AB/wiki_01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_62\n",
      "🔥 ../data/text/AB/wiki_83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_56\n",
      "🔥 ../data/text/AB/wiki_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_73\n",
      "🔥 ../data/text/AB/wiki_86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_39\n",
      "🔥 ../data/text/AB/wiki_47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_05\n",
      "🔥 ../data/text/AB/wiki_24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_97\n",
      "🔥 ../data/text/AB/wiki_38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_81\n",
      "🔥 ../data/text/AB/wiki_00\n",
      "🔥 ../data/text/AB/wiki_63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_64\n",
      "🔥 ../data/text/AB/wiki_85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_06\n",
      "🔥 ../data/text/AB/wiki_79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_17\n",
      "🔥 ../data/text/AB/wiki_66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_90\n",
      "🔥 ../data/text/AB/wiki_75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_68\n",
      "🔥 ../data/text/AB/wiki_46\n",
      "🔥 ../data/text/AB/wiki_84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_60\n",
      "🔥 ../data/text/AB/wiki_45\n",
      "🔥 ../data/text/AB/wiki_33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_44\n",
      "🔥 ../data/text/AB/wiki_30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_27\n",
      "🔥 ../data/text/AB/wiki_41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_96\n",
      "🔥 ../data/text/AB/wiki_09\n",
      "🔥 ../data/text/AB/wiki_67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_89\n",
      "🔥 ../data/text/AB/wiki_21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_59\n",
      "🔥 ../data/text/AB/wiki_74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_55\n",
      "🔥 ../data/text/AB/wiki_88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_82\n",
      "🔥 ../data/text/AB/wiki_29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 ../data/text/AB/wiki_50\n"
     ]
    }
   ],
   "source": [
    "dataset = THWikiDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=data_collator,\n",
    "                        shuffle=True, num_workers=4) # Still cant make more batch size!! Need collate function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/data/data_collator.py\", line 79, in __call__\n    inputs, labels = self.mask_tokens(batch)\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/data/data_collator.py\", line 102, in mask_tokens\n    if self.tokenizer.mask_token is None:\nAttributeError: 'CharBPETokenizer' object has no attribute 'mask_token'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-275-661edc455c42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batched\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moumodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/data/data_collator.py\", line 79, in __call__\n    inputs, labels = self.mask_tokens(batch)\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/data/data_collator.py\", line 102, in mask_tokens\n    if self.tokenizer.mask_token is None:\nAttributeError: 'CharBPETokenizer' object has no attribute 'mask_token'\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched)\n",
    "    oumodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_parameters',\n",
       " '_tokenizer',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'decode',\n",
       " 'decode_batch',\n",
       " 'enable_padding',\n",
       " 'enable_truncation',\n",
       " 'encode',\n",
       " 'encode_batch',\n",
       " 'get_vocab',\n",
       " 'get_vocab_size',\n",
       " 'id_to_token',\n",
       " 'no_padding',\n",
       " 'no_truncation',\n",
       " 'normalize',\n",
       " 'num_special_tokens_to_add',\n",
       " 'padding',\n",
       " 'post_process',\n",
       " 'save',\n",
       " 'save_model',\n",
       " 'to_str',\n",
       " 'token_to_id',\n",
       " 'train',\n",
       " 'truncation']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 27045,  1038,   340,     3,  3819,  1223,  2492,   314,     3,\n",
      "          1561,  9379,  4160,  1009,  3819, 18348,  1524,  1016,   359,     3,\n",
      "          3691,  4045,  3580,  9717,  1404,  1275,  1038,   340,     3,  3038,\n",
      "          2891, 12364,     3, 15536,     3,     2]])\n",
      "(tensor(10.4711, grad_fn=<NllLossBackward>), tensor([[[-0.1541, -0.1424, -0.0832,  ...,  0.5985, -0.6984, -0.8883],\n",
      "         [ 0.2593, -0.3135, -0.0564,  ...,  1.0348, -0.1579, -0.4282],\n",
      "         [-0.4429, -0.2429,  0.3989,  ...,  0.6725, -0.0034, -0.2337],\n",
      "         ...,\n",
      "         [-0.0181, -0.6908, -0.4555,  ...,  1.0229, -1.0689, -0.6037],\n",
      "         [-0.6513, -0.3325,  0.3148,  ..., -0.0385, -0.4886, -0.4097],\n",
      "         [ 0.1321, -0.1464, -0.5289,  ...,  0.7893, -0.2908, -0.4264]]],\n",
      "       grad_fn=<AddBackward0>))\n",
      "tensor(10.4711, grad_fn=<NllLossBackward>) torch.Size([1, 36, 30522])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CharBPETokenizer(vocab_file='vocab.json',merges_file ='merges.txt' )\n",
    "no_accent_strip = BertNormalizer(strip_accents=False)\n",
    "tokenizer._tokenizer.normalizer = no_accent_strip\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(u\"สวัสดีครับ ผมชื่อไนท์ ตอนนี้ก็เป็นเวลาที่ผมต้องไปโรงเรียนแล้ว  นี่คือการเว้นวรรคสองทีครับ  จะได้ออกเป็นสอง Spaces\").ids).unsqueeze(0)\n",
    "print(input_ids)\n",
    "outputs = model(input_ids, labels=input_ids)\n",
    "print(outputs)\n",
    "loss, prediction_scores = outputs[:2]\n",
    "print(loss, prediction_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 1153, 3909, 1043,  333,    3,    2]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from transformers import TextDataset, LineByLineTextDataset\n",
    "\n",
    "# # dataset = LineByLineTextDataset(\n",
    "# #     tokenizer=pretrain_tokenizer,\n",
    "# #     file_path=\"../data/text/AA/wiki_01\",\n",
    "# #     block_size=128,\n",
    "# # )\n",
    "\n",
    "# dataset = TextDataset(\n",
    "#     tokenizer=pretrain_tokenizer,\n",
    "#     file_path=\"../data/text/AA/wiki_01\",\n",
    "#     block_size=128,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=1, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=1, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=103, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=1, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=154, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=1, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=189, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_doc = list(Path(\"../data/text/AA/\").glob(\"wiki*\"))[0].read_text(encoding=\"utf-8\").splitlines()\n",
    "tokenizer = Tokenizer.from_file(\"./thwiki-sentencepiecebpe.tokenizer.json\")\n",
    "tokenizer.encode_batch(one_doc[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 32, 1096, 1085, 33, 6, 5273, 6, 1211, 33, 6, 1207, 30, 19, 19, 1141, 18, 1210, 18, 1200, 19, 1095, 35, 1212, 33, 5273, 6, 1209, 33, 6, 86, 86, 72, 86, 76, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 6, 34, 2], [0, 86, 86, 72, 86, 76, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 2], [0, 2], [0, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 12, 31, 13, 86, 74, 86, 72, 86, 81, 86, 86, 86, 81, 86, 86, 86, 86, 78, 86, 86, 86, 78, 86, 72, 86, 73, 86, 86, 78, 86, 86, 78, 86, 86, 78, 86, 72, 86, 86, 72, 86, 77, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 12, 31, 13, 86, 86, 86, 86, 86, 86, 72, 86, 76, 86, 86, 86, 86, 86, 86, 86, 72, 86, 78, 86, 86, 86, 80, 86, 86, 86, 86, 86, 86, 74, 86, 86, 86, 86, 86, 86, 78, 86, 76, 86, 86, 86, 126, 86, 86, 86, 86, 126, 86, 78, 86, 86, 86, 86, 86, 86, 80, 86, 86, 86, 76, 86, 73, 86, 77, 86, 86, 86, 86, 86, 86, 124, 86, 80, 86, 86, 86, 86, 86, 124, 86, 86, 77, 86, 124, 86, 86, 86, 86, 86, 86, 86, 86, 86, 74, 86, 78, 86, 86, 86, 86, 86, 72, 86, 86, 86, 86, 86, 86, 86, 86, 86, 76, 86, 73, 86, 77, 86, 86, 86, 86, 86, 72, 86, 76, 86, 86, 86, 86, 86, 124, 86, 73, 86, 86, 86, 86, 78, 86, 76, 27, 16, 3117, 21, 86, 86, 86, 78, 86, 76, 86, 86, 86, 86, 86, 86, 77, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 126, 86, 86, 86, 86, 86, 78, 86, 86, 86, 72, 86, 86, 86, 74, 86, 86, 23, 86, 86, 86, 86, 86, 78, 86, 86, 86, 74, 86, 86, 81, 86, 86, 86, 72, 86, 86, 86, 86, 124, 86, 86, 86, 124, 86, 86, 86, 86, 86, 86, 86, 86, 86, 73, 86, 86, 86, 78, 86, 86, 77, 86, 86, 86, 86, 76, 86, 86, 86, 86, 86, 78, 86, 86, 86, 78, 86, 86, 86, 81, 86, 86, 86, 74, 86, 86, 73, 86, 86, 86, 86, 86, 86, 72, 86, 76, 86, 86, 86, 86, 86, 81, 86, 86, 86, 76, 86, 86, 86, 86, 78, 86, 86, 86, 73, 86, 86, 86, 86, 81, 86, 86, 86, 86, 80, 86, 86, 86, 80, 86, 86, 72, 86, 76, 86, 86, 78, 86, 86, 72, 86, 86, 78, 86, 86, 86, 80, 86, 86, 86, 86, 86, 86, 81, 86, 86, 86, 86, 72, 86, 86, 86, 86, 86, 86, 86, 77, 86, 124, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 73, 86, 86, 74, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 72, 86, 86, 76, 86, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 80, 86, 86, 78, 86, 86, 78, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 77, 86, 86, 86, 76, 86, 86, 86, 86, 80, 86, 86, 86, 86, 124, 86, 86, 78, 86, 86, 86, 86, 86, 86, 76, 86, 73, 86, 77, 86, 86, 86, 86, 86, 76, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 76, 86, 73, 86, 77, 86, 86, 86, 86, 86, 86, 86, 76, 86, 86, 76, 86, 86, 86, 86, 86, 86, 86, 86, 80, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 76, 86, 73, 86, 77, 86, 86, 86, 86, 86, 124, 86, 80, 86, 86, 86, 86, 86, 124, 86, 86, 86, 86, 86, 80, 86, 86, 72, 86, 86, 86, 86, 86, 86, 78, 86, 86, 86, 76, 86, 86, 86, 72, 86, 86, 73, 86, 86, 86, 77, 86, 86, 86, 86, 124, 86, 74, 86, 73, 86, 77, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 74, 86, 86, 81, 86, 86, 86, 78, 86, 86, 86, 78, 86, 86, 78, 86, 86, 86, 86, 86, 86, 76, 86, 73, 86, 77, 86, 86, 86, 86, 86, 86, 78, 86, 86, 86, 86, 86, 80, 86, 86, 86, 86, 76, 86, 86, 86, 86, 86, 86, 86, 80, 86, 86, 86, 80, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 86, 124, 86, 86, 86, 76, 86, 86, 73, 86, 80, 86, 86, 86, 86, 78, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 76, 86, 73, 86, 77, 86, 86, 86, 2], [0, 2], [0, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 77, 86, 124, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 73, 86, 86, 86, 74, 86, 73, 86, 86, 86, 74, 86, 86, 86, 86, 86, 86, 86, 76, 86, 86, 86, 86, 124, 86, 86, 77, 86, 86, 86, 86, 124, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 72, 86, 86, 79, 86, 86, 74, 86, 124, 86, 86, 80, 86, 86, 86, 73, 86, 86, 86, 124, 86, 86, 86, 86, 86, 86, 80, 86, 86, 86, 76, 86, 86, 72, 86, 76, 86, 86, 86, 126, 86, 77, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 74, 86, 73, 86, 86, 86, 76, 86, 86, 86, 124, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 79, 86, 86, 74, 86, 124, 86, 86, 80, 86, 86, 77, 86, 124, 86, 86, 86, 72, 86, 77, 86, 86, 86, 78, 86, 86, 72, 86, 86, 72, 86, 72, 86, 86, 86, 78, 86, 86, 86, 86, 80, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 72, 86, 86, 86, 86, 86, 76, 86, 86, 73, 86, 78, 86, 86, 74, 86, 86, 78, 86, 86, 74, 86, 86, 78, 86, 86, 86, 78, 86, 86, 86, 80, 86, 73, 86, 126, 86, 78, 86, 86, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 77, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 80, 86, 86, 86, 81, 86, 124, 86, 86, 86, 80, 86, 86, 86, 72, 86, 76, 86, 86, 78, 86, 4758, 16, 1930, 86, 86, 78, 86, 72, 86, 78, 86, 86, 86, 86, 86, 86, 86, 86, 86, 72, 12, 7959, 16, 28, 2848, 86, 86, 78, 86, 72, 86, 78, 86, 86, 86, 86, 86, 13, 86, 86, 86, 76, 86, 86, 80, 86, 86, 72, 86, 76, 86, 86, 78, 86, 86, 72, 86, 86, 72, 86, 76, 86, 86, 78, 86, 2625, 86, 86, 124, 86, 78, 86, 86, 86, 86, 86, 77, 86, 86, 86, 86, 86, 86, 86, 72, 86, 76, 86, 86, 86, 86, 86, 80, 86, 86, 86, 80, 86, 86, 72, 86, 76, 86, 86, 78, 86, 86, 72, 86, 86, 78, 86, 86, 86, 80, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 77, 86, 86, 86, 77, 86, 86, 86, 80, 86, 28, 86, 86, 86, 86, 86, 86, 86, 80, 86, 86, 86, 86, 76, 86, 86, 86, 86, 86, 86, 72, 86, 76, 86, 86, 86, 86, 86, 80, 86, 86, 86, 80, 86, 86, 72, 86, 76, 86, 86, 78, 86, 86, 72, 86, 86, 78, 86, 86, 86, 80, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 77, 86, 86, 86, 77, 86, 86, 86, 80, 86, 1562, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 78, 86, 86, 86, 80, 86, 124, 86, 86, 86, 80, 86, 18, 86, 18, 3842, 86, 86, 77, 86, 86, 86, 80, 86, 86, 78, 86, 73, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 80, 86, 86, 86, 72, 86, 76, 86, 86, 78, 86, 1350, 86, 86, 124, 86, 78, 86, 86, 86, 86, 86, 78, 86, 86, 77, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 78, 86, 86, 86, 72, 86, 76, 86, 86, 86, 86, 72, 86, 73, 86, 86, 86, 86, 124, 86, 73, 86, 86, 81, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 77, 86, 86, 86, 86, 86, 86, 86, 80, 86, 86, 86, 74, 86, 86, 86, 86, 80, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 74, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 73, 86, 78, 86, 86, 74, 86, 86, 78, 86, 86, 74, 86, 86, 78, 86, 86, 86, 78, 86, 86, 86, 78, 86, 86, 86, 86, 77, 86, 86, 86, 86, 86, 86, 86, 76, 86, 73, 86, 77, 86, 86, 86, 86, 72, 86, 72, 86, 86, 86, 72, 86, 78, 86, 86, 125, 86, 86, 74, 86, 124, 86, 86, 74, 86, 86, 86, 86, 86, 86, 86, 86, 77, 86, 124, 86, 86, 74, 86, 86, 86, 86, 86, 86, 78, 86, 76, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 72, 86, 76, 86, 73, 86, 77, 86, 86, 86, 86, 78, 86, 86, 86, 72, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 72, 86, 86, 86, 80, 86, 86, 86, 86, 124, 86, 78, 86, 86, 78, 86, 86, 77, 86, 124, 86, 86, 72, 86, 86, 72, 86, 78, 86, 86, 86, 86, 74, 86, 86, 86, 86, 86, 86, 78, 86, 76, 86, 86, 74, 86, 86, 86, 86, 80, 86, 124, 86, 86, 81, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 72, 86, 80, 86, 86, 86, 86, 78, 86, 86, 86, 78, 86, 86, 124, 86, 73, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 72, 86, 86, 80, 86, 86, 86, 80, 86, 86, 86, 86, 80, 86, 86, 86, 86, 86, 86, 86, 86, 124, 86, 78, 86, 86, 78, 86, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 86, 81, 86, 86, 86, 86, 86, 80, 86, 86, 78, 86, 72, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 80, 86, 86, 86, 86, 86, 77, 86, 86, 86, 78, 86, 73, 86, 86, 80, 86, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 86, 86, 80, 86, 86, 86, 86, 76, 86, 86, 78, 86, 74, 86, 72, 86, 77, 86, 86, 86, 78, 86, 86, 86, 77, 86, 124, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 72, 86, 72, 86, 77, 86, 86, 86, 78, 86, 86, 86, 76, 86, 86, 86, 86, 86, 124, 86, 86, 86, 78, 86, 74, 86, 86, 78, 86, 86, 86, 74, 86, 86, 86, 126, 86, 78, 86, 86, 86, 86, 124, 86, 86, 78, 86, 72, 86, 86, 86, 86, 72, 86, 86, 86, 86, 86, 86, 86, 78, 86, 86, 86, 86, 78, 86, 86, 77, 86, 86, 72, 86, 78, 86, 86, 78, 86, 74, 86, 72, 86, 81, 86, 86, 86, 86, 86, 86, 86, 78, 86, 2], [0, 2], [0, 86, 86, 86, 86, 72, 86, 86, 86, 86, 86, 78, 86, 86, 86, 86, 86, 78, 86, 86, 86, 86, 86, 77, 86, 86, 86, 86, 124, 86, 86, 78, 86, 86, 86, 124, 86, 86, 86, 77, 86, 86, 86, 86, 80, 86, 86, 86, 86, 78, 86, 76, 86, 86, 75, 86, 86, 86, 86, 86, 75, 86, 12, 86, 86, 86, 86, 124, 86, 86, 77, 86, 86, 86, 86, 78, 86, 76, 86, 86, 78, 86, 86, 78, 86, 72, 86, 13, 86, 86, 86, 86, 80, 86, 18, 86, 18, 1184, 3585, 12, 86, 18, 86, 18, 1488, 2214, 13, 86, 86, 77, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 72, 86, 86, 86, 86, 86, 124, 86, 86, 86, 86, 86, 86, 86, 86, 86, 74, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 76, 86, 86, 79, 86, 86, 78, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 80, 86, 18, 86, 18, 1184, 2870, 12, 86, 18, 86, 18, 1488, 3660, 13, 86, 86, 77, 86, 86, 86, 79, 86, 72, 86, 73, 86, 86, 86, 78, 86, 73, 86, 86, 86, 86, 86, 86, 81, 86, 86, 86, 72, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 86, 86, 124, 86, 86, 77, 86, 124, 86, 86, 86, 81, 86, 86, 86, 86, 86, 78, 86, 76, 86, 86, 78, 86, 86, 78, 86, 72, 86, 86, 86, 86, 76, 86, 86, 86, 86, 86, 72, 86, 73, 86, 86, 86, 77, 86, 86, 73, 86, 86, 78, 6, 86, 74, 86, 86, 86, 86, 86, 86, 78, 86, 76, 86, 86, 86, 86, 80, 86, 86, 6, 86, 74, 86, 72, 86, 81, 86, 6, 86, 86, 86, 86, 86, 77, 86, 86, 86, 86, 86, 86, 86, 86, 86, 77, 86, 6, 12, 13, 86, 86, 86, 81, 86, 86, 86, 86, 86, 86, 86, 86, 86, 80, 86, 86, 72, 86, 86, 86, 86, 86, 86, 86, 86, 124, 86, 78, 86, 86, 78, 86, 86, 86, 86, 86, 80, 86, 86, 86, 86, 74, 86, 86, 86, 86, 77, 86, 86, 86, 86, 86, 72, 86, 80, 86, 86, 12, 86, 86, 86, 86, 86, 78, 86, 86, 86, 86, 86, 77, 86, 86, 86, 86, 86, 86, 86, 86, 86, 77, 86, 86, 86, 86, 124, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 86, 81, 86, 86, 86, 86, 72, 86, 80, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 78, 86, 76, 86, 86, 77, 86, 124, 86, 86, 74, 86, 86, 13, 86, 86, 86, 86, 80, 86, 18, 86, 18, 22, 5822, 12, 86, 18, 86, 18, 1488, 2924, 13, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 77, 86, 86, 86, 80, 86, 86, 86, 124, 86, 86, 77, 86, 86, 86, 77, 86, 124, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 74, 86, 86, 86, 86, 72, 86, 86, 86, 86, 86, 86, 78, 86, 76, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 86, 86, 73, 86, 86, 74, 86, 86, 86, 86, 86, 86, 86, 86, 86, 77, 86, 86, 72, 86, 73, 86, 72, 86, 72, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 73, 86, 86, 78, 86, 86, 78, 86, 86, 86, 73, 86, 86, 78, 4758, 86, 86, 80, 86, 86, 86, 86, 86, 86, 86, 74, 86, 124, 86, 86, 78, 86, 86, 86, 78, 86, 86, 72, 86, 86, 86, 86, 86, 86, 86, 86, 78, 86, 86, 86, 72, 86, 86, 77, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 86, 78, 86, 86, 86, 78, 86, 74, 86, 86, 77, 86, 86, 86, 86, 86, 86, 86, 124, 86, 86, 86, 86, 86, 74, 86, 86, 86, 86, 86, 86, 78, 86, 76, 86, 86, 86, 86, 86, 73, 86, 86, 86, 73, 86, 86, 78, 86, 86, 80, 86, 124, 86, 86, 76, 86, 86, 86, 86, 78, 86, 86, 80, 86, 86, 78, 86, 86, 76, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 78, 86, 86, 86, 78, 86, 72, 86, 86, 72, 86, 86, 74, 86, 78, 86, 72, 86, 86, 86, 86, 86, 77, 86, 86, 72, 86, 73, 86, 72, 86, 72, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 80, 86, 86, 86, 86, 76, 86, 86, 77, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 78, 86, 86, 86, 78, 86, 86, 86, 86, 86, 86, 76, 86, 73, 86, 77, 86, 86, 86, 86, 86, 86, 86, 86, 78, 86, 72, 86, 86, 124, 86, 78, 86, 86, 124, 86, 78, 86, 86, 86, 74, 86, 78, 86, 86, 86, 86, 86, 72, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 81, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 80, 86, 86, 86, 86, 124, 86, 78, 86, 86, 77, 86, 86, 86, 86, 81, 86, 86, 86, 86, 78, 86, 86, 78, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 72, 86, 86, 86, 78, 86, 86, 86, 78, 86, 86, 86, 78, 86, 86, 86, 72, 86, 81, 86, 86, 86, 86, 86, 76, 86, 86, 86, 86, 78, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_doc = list(Path(\"../data/text/AA/\").glob(\"wiki*\"))[0].read_text(encoding=\"utf-8\").splitlines()\n",
    "tokenizer = RobertaTokenizerFast(vocab_file='vocab.json',merges_file ='merges.txt', max_len=512)\n",
    "tokenizer.batch_encode_plus(one_doc[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁ฟิลิปปินส์', 'ตั้งอยู่ใน', 'แถบ', 'วงแหวน', 'แห่ง', 'ไฟ', 'และ', 'ใกล้กับ', 'เส้น', 'ศูนย์สูตร', '▁ทําให้มี', 'แนวโน้ม', 'สูง', 'ที่จะ', 'ประสบ', 'ภัย', 'จาก', 'แผ่นดินไหว', 'และ', 'ไต้ฝุ่น', '▁แต่ก็', 'ทําให้', 'มีทั้ง', 'ทรัพยากรธรรมชาติ', 'ที่', 'อุดมสมบูรณ์', 'และความ', 'หลากหลาย', 'ทางชีวภาพ', 'อย่างยิ่ง', 'เช่นกัน', '▁ฟิลิปปินส์', 'มี', 'เนื้อที่', 'ประมาณ', '▁300', ',000', '▁ตารางกิโลเมตร', '▁(1', '15', ',8', '31', '▁ตาราง', 'ไมล์', ')', '▁และมี', 'ประชากร', 'ประมาณ', '▁100', '▁ล้านคน', '▁นับเป็น', 'ประเทศ', 'ที่มีประชากร', 'มากที่สุด', 'เป็นอันดับที่', '▁8', '▁ใน', 'เอเชีย', '▁และเป็น', 'ประเทศ', 'ที่มีประชากร', 'มากที่สุด', 'เป็นอันดับที่', '▁12', '▁ของโลก', '▁นอกจากนี้', '▁ณ', '▁ปี', '▁พ.ศ.', '▁2556', '▁ยังมี', 'ชาว', 'ฟิลิปปินส์', 'อีก', 'ประมาณ', '▁10', '▁ล้านคน', 'อาศัยอยู่ใน', 'ต่างประเทศ', '▁รวม', 'แล้ว', 'ถือเป็น', 'กลุ่มคน', 'พลัด', 'ถิ่น', 'ที่ใหญ่ที่สุด', 'กลุ่ม', 'หนึ่งของ', 'โลก', '▁ความ', 'หลากหลาย', 'ทาง', 'ชาติพันธุ์', 'และวัฒนธรรม', 'ปรากฏ', 'ให้เห็น', 'ตลอดทั้ง', 'หมู่เกาะ', '▁ในยุค', 'ก่อนประวัติศาสตร์', '▁มนุษย์', 'กลุ่ม', 'แรกที่', 'เข้ามา', 'ตั้ง', 'รกราก', 'ใน', 'หมู่เกาะ', 'แห่งนี้', 'คือ', 'กลุ่ม', 'ชน', 'นิก', 'รี', 'โต', '▁ตาม', 'มาด้วย', 'กลุ่ม', 'ชน', 'ออส', 'โตร', 'นี', 'เซียน', 'ที่', 'อพยพ', 'เข้ามา', 'อย่างต่อเนื่อง', '▁มีการ', 'ติดต่อ', 'แลกเปลี่ยน', 'กับ', 'ชาวจีน', '▁ม', 'ลายู', '▁อินเดีย', '▁และ', 'อาหรับ', '▁จากนั้นก็', 'เกิด', 'นคร', 'รัฐ', 'ทางทะเล', 'ขึ้นมา', 'หลายแห่ง', 'ภายใต้', 'การปกครองของ', 'ดา', 'ตู', '▁ล', 'ากัน', '▁', 'ราชา', '▁หรือ', 'สุลต่าน']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode_batch(one_doc[:8])[5].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<doc id=\"1990\" url=\"https://th.wikipedia.org/wiki?curid=1990\" title=\"ประเทศฟิลิปปินส์\">',\n",
       " 'ประเทศฟิลิปปินส์',\n",
       " '',\n",
       " 'ฟิลิปปินส์ (; ) หรือชื่อทางการว่า สาธารณรัฐฟิลิปปินส์ (; ) เป็นประเทศเอกราชที่เป็นหมู่เกาะในภูมิภาคเอเชียตะวันออกเฉียงใต้ ตั้งอยู่ในมหาสมุทรแปซิฟิกตะวันตก ประกอบด้วยเกาะ 7,641 เกาะ ซึ่งจัดอยู่ในเขตภูมิศาสตร์ใหญ่ 3 เขตจากเหนือจรดใต้ ได้แก่ ลูซอน วิซายัส และมินดาเนา เมืองหลวงของประเทศคือมะนิลา ส่วนเมืองที่มีประชากรมากที่สุดคือนครเกซอน ทั้งสองเป็นส่วนหนึ่งของเมโทรมะนิลา ฟิลิปปินส์มีอาณาเขตติดต่อกับทะเลจีนใต้ทางทิศตะวันตก ทะเลฟิลิปปินทางทิศตะวันออก และทะเลเซเลบีสทางทิศตะวันตกเฉียงใต้ โดยมีพรมแดนทางทะเลร่วมกับไต้หวันทางทิศเหนือ ปาเลาทางทิศตะวันออก มาเลเซียและอินโดนีเซียทางทิศใต้ และเวียดนามทางทิศตะวันตก ',\n",
       " '',\n",
       " 'ฟิลิปปินส์ตั้งอยู่ในแถบวงแหวนแห่งไฟและใกล้กับเส้นศูนย์สูตร ทำให้มีแนวโน้มสูงที่จะประสบภัยจากแผ่นดินไหวและไต้ฝุ่น แต่ก็ทำให้มีทั้งทรัพยากรธรรมชาติที่อุดมสมบูรณ์และความหลากหลายทางชีวภาพอย่างยิ่งเช่นกัน ฟิลิปปินส์มีเนื้อที่ประมาณ 300,000 ตารางกิโลเมตร (115,831 ตารางไมล์) และมีประชากรประมาณ 100 ล้านคน นับเป็นประเทศที่มีประชากรมากที่สุดเป็นอันดับที่ 8 ในเอเชีย และเป็นประเทศที่มีประชากรมากที่สุดเป็นอันดับที่ 12 ของโลก นอกจากนี้ ณ ปี พ.ศ. 2556 ยังมีชาวฟิลิปปินส์อีกประมาณ 10 ล้านคนอาศัยอยู่ในต่างประเทศ รวมแล้วถือเป็นกลุ่มคนพลัดถิ่นที่ใหญ่ที่สุดกลุ่มหนึ่งของโลก ความหลากหลายทางชาติพันธุ์และวัฒนธรรมปรากฏให้เห็นตลอดทั้งหมู่เกาะ ในยุคก่อนประวัติศาสตร์ มนุษย์กลุ่มแรกที่เข้ามาตั้งรกรากในหมู่เกาะแห่งนี้คือกลุ่มชนนิกรีโต ตามมาด้วยกลุ่มชนออสโตรนีเซียนที่อพยพเข้ามาอย่างต่อเนื่อง มีการติดต่อแลกเปลี่ยนกับชาวจีน มลายู อินเดีย และอาหรับ จากนั้นก็เกิดนครรัฐทางทะเลขึ้นมาหลายแห่งภายใต้การปกครองของดาตู ลากัน ราชา หรือสุลต่าน',\n",
       " '',\n",
       " 'เฟอร์ดินานด์ มาเจลลัน ได้มาขึ้นฝั่งที่เกาะโฮโมนโฮน (ใกล้กับเกาะซามาร์) ในปี พ.ศ. 2063 (ค.ศ. 1521) นับเป็นจุดเริ่มต้นของยุคแห่งอิทธิพลและอำนาจของสเปน ในปี พ.ศ. 2085 (ค.ศ. 1542) นักสำรวจชาวสเปนชื่อ รุย โลเปซ เด บิยาโลโบส ได้ตั้งชื่อเกาะซามาร์และเลเตรวมกันว่า \"หมู่เกาะเฟลีเป\" หรือ \"อิสลัสฟิลิปินัส\" () เพื่อเป็นเกียรติแด่เจ้าชายเฟลีเปแห่งอัสตูเรียส (ต่อมา อิสลัสฟิลิปินัสได้กลายเป็นชื่อเรียกกลุ่มเกาะทั้งหมด) ในปี พ.ศ. 2108 (ค.ศ. 1565) มิเกล โลเปซ เด เลกัซปี ได้จัดตั้งนิคมสเปนแห่งแรกบนเกาะเซบู ฟิลิปปินส์กลายเป็นส่วนหนึ่งของจักรวรรดิสเปนเป็นเวลานานกว่า 300 ปี ส่งผลให้ศาสนาคริสต์นิกายโรมันคาทอลิกกลายเป็นศาสนาหลักของผู้คนในหมู่เกาะ ในช่วงเวลานี้ มะนิลามีฐานะเป็นศูนย์กลางการบริหารของจักรวรรดิสเปนในเอเชีย และยังเป็นศูนย์กลางทางทิศตะวันตกของการค้าข้ามมหาสมุทรแปซิฟิก โดยเชื่อมโยงเอเชียเข้ากับเมืองอากาปุลโกในอเมริกาผ่านทางเรือใบมะนิลา']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_doc[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "tokenizer = RobertaTokenizer(vocab_file='vocab.json',merges_file ='merges.txt', max_len=512)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_additional_special_tokens',\n",
       " '_batch_encode_plus',\n",
       " '_batch_prepare_for_model',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_from_pretrained',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_mask_token',\n",
       " '_maybe_update_backend',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_prepare_for_model',\n",
       " '_sep_token',\n",
       " '_tokenize',\n",
       " '_unk_token',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'bpe',\n",
       " 'bpe_ranks',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'byte_decoder',\n",
       " 'byte_encoder',\n",
       " 'cache',\n",
       " 'clean_up_tokenization',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'encoder',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'errors',\n",
       " 'from_pretrained',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'pat',\n",
       " 'prepare_for_tokenization',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'special_tokens_map',\n",
       " 'tokenize',\n",
       " 'truncate_sequences',\n",
       " 'unique_added_tokens_encoder',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_RoBERTa\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=256,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970c8d60c5794b5ea8be7b6a9f7fb85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=20.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f94c9f211634f09a2beabf0ab2ab385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=17.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74fe093c593464ba898f2734f82f7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=17.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509503d6ecba43bd97736260a46e1311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=17.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8e78240e3d4229bbc014060add1415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=17.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3eda92bae04cc0b9cb31c68ca865cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=17.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a746e06f8442f3911d6feb06d3cf46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=17.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf86eeaae3cc46399fdf43582158c69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=17.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93992de64cb1438199481691f06050df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=17.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c396b7288a4d1c872078257d1c9d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=17.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./EsperBERTo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 100,\n",
       " 104,\n",
       " 107,\n",
       " 99,\n",
       " 104,\n",
       " 108,\n",
       " 107,\n",
       " 100,\n",
       " 105,\n",
       " 107,\n",
       " 99,\n",
       " 99,\n",
       " 107,\n",
       " 100,\n",
       " 107,\n",
       " 108,\n",
       " 105,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 107,\n",
       " 99,\n",
       " 104,\n",
       " 103,\n",
       " 9042,\n",
       " 2771,\n",
       " 3174,\n",
       " 2]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = pretrain_tokenizer.encode(u\"สวัสดีครับ ผมชื่อไนท์ ตอนนี้ก็เป็นเวลาที่ผมต้องไปโรงเรียนแล้ว  นี่คือการเว้นวรรคสองทีครับ  จะได้ออกเป็นสอง Spaces\")\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
