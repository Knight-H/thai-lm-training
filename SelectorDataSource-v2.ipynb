{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasource\n",
    "- [x] `classification_dataset` from Jung+Ninja\n",
    "- [x] `Another Website` include: amarintv, , ch3thailand, instagram, mgronline,pantip, pptv, sanook, sudsapda, thairath,       tnn2, workpointnews, babyandkids, dailynews\t, khaosod,  new18, postjung, praew, siamdara,thaich8,thansettakij, tvpoolonline, brighttv, facebook, matichon, one31, posttoday, ryt9, siamsport, thainews_prd, think_of_living, twitter\n",
    "- [ ] `social_listening` include: Facebook, Twitter and Instagram\n",
    "- [x] `Wiki TH`\n",
    "- [ ] `Pantip` of Knight (data_lm)\n",
    "\n",
    "## Stucture Files\n",
    "```\n",
    "/raw_data_extraction\n",
    "│   README.md\n",
    "└───classification_dataset\n",
    "│   └───scrapy_dailynews\n",
    "│   |   │   file111.txt\n",
    "│   └───scrapy_naewna\n",
    "│   \n",
    "└───Another Website\n",
    "│   └───amarintv\n",
    "│   |   │   file111.txt\n",
    "│   └───baanlaesuan\n",
    "│       │   file111.txt\n",
    "│   \n",
    "└───social_listening \n",
    "│   │   comment_data.txt\n",
    "│   │   pantip_post_data.txt\n",
    "│   \n",
    "└───Wiki TH\n",
    "│   └───AA\n",
    "│       │   file111.txt\n",
    "│   \n",
    "└───Pantip \n",
    "│   │   file011.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification_dataset from Jung + Ninja\n",
    "```\n",
    "print(data['title'])\n",
    "print(data['author'])\n",
    "print(data['publish'])\n",
    "print(data['content_id'])\n",
    "print(data['content'])\n",
    "print(data['url'])\n",
    "print(data['images'])\n",
    "print(data['content_class'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from enum import Enum\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear Files\n",
    "# !rm -rf /workdir/Code/bma_transformer_model/data/raw_data_extraction/*\n",
    "# --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/workdir/Code/bma_transformer_model/data/raw/\"\n",
    "EXPORT_DIR = \"/workdir/Code/bma_transformer_model/data/raw_data_extraction/\"\n",
    "\n",
    "SELECTOR_DIR = [\"classification_dataset\"]\n",
    "# SELECTOR_DIR = [\"another_website\"]\n",
    "# SELECTOR_DIR = [\"thwiki-20200601-extracted\"]\n",
    "# SELECTOR_DIR = [\"data_lm\"]\n",
    "# SELECTOR_DIR = [\"social_listening\"]\n",
    "\n",
    "MINIMUM_FILE_SIZE = 30000000 #size of file 30 MB \n",
    "OUTPUT_Filename = \"\"\n",
    "extension = \".txt\"\n",
    "count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file_from_json(filename, data):\n",
    "    f=open(filename,'a')\n",
    "    f.write(re.sub('[[|!\"_.”-]|'']', '', data['title']))\n",
    "    f.write('\\n')\n",
    "    f.write(re.sub('[[\\n\\r\\t“|!\"_.”-]|'']', '', data['content']))\n",
    "\n",
    "    # New Content\n",
    "    f.write('\\n')\n",
    "    f.write('\\n')\n",
    "    return f.tell()\n",
    "\n",
    "def write_file_from_text(filename, data, TYPE_DATA, idx=0):\n",
    "    f=open(filename, 'a')\n",
    "    if (TYPE_DATA == \"TEXT\"):\n",
    "        f.write(re.sub('[[|!\"_.”-]|'']', '', data))\n",
    "    elif (TYPE_DATA == \"WIKI\"):\n",
    "        data = re.sub('<doc.*', '', data)\n",
    "        data = re.sub('.*doc>', '', data)\n",
    "        data = re.sub('[[|!\"_.”-]|'']', '', data)\n",
    "        data = re.sub('[[\\n\\r\\t“|!\"_.”-]|'']', '', data)\n",
    "        f.write(data)\n",
    "    elif (TYPE_DATA == \"dataLM\"):\n",
    "        f.write(data['Title'][idx])\n",
    "        f.write('\\n')\n",
    "        data = re.sub('[[\\n\\r\\t“|!\"_.”-]|'']', '', data['Kratoo'][idx])\n",
    "        f.write(data)\n",
    "    elif (TYPE_DATA == \"social_listening\"):\n",
    "        f.write(data['Title'][idx])\n",
    "        f.write('\\n')\n",
    "        data = re.sub('[[\\n\\r\\t“|!\"_.”-]|'']', '', data['Kratoo'][idx])\n",
    "        f.write(data)\n",
    "\n",
    "    # New Content\n",
    "    f.write('\\n')\n",
    "    f.write('\\n')\n",
    "    return f.tell()\n",
    "\n",
    "def create_folder(pathFile, count):\n",
    "    if not os.path.exists(os.path.dirname(pathFile)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(pathFile))\n",
    "            count = 0\n",
    "            return count\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            raise\n",
    "    return count\n",
    "\n",
    "def readAndWriteFromJSONFile_LimitFileSize(pathFile, output_file, count):\n",
    "    # Read JSON File\n",
    "    with open(pathFile) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        \n",
    "        # Write TxT File\n",
    "        newSize = write_file_from_json(output_file + \"_\" + str(count) + \".txt\", data)\n",
    "        # IF size of file MINIMUM_FILE_SIZE \n",
    "        if(newSize > MINIMUM_FILE_SIZE):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def readAndWriteFromTEXTFile_LimitFileSize(pathFile, output_file, count, TYPE_DATA):\n",
    "    try:\n",
    "        fRead = open(pathFile, \"r\")\n",
    "        data = fRead.read()\n",
    "    except:\n",
    "        data = \"\"\n",
    "\n",
    "    # Write TxT File on Temp File\n",
    "    newSize = write_file_from_text(output_file + \"_\" + str(count) + '.txt', data, TYPE_DATA)\n",
    "\n",
    "    # IF size of file MINIMUM_FILE_SIZE\n",
    "    if(newSize > MINIMUM_FILE_SIZE):\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def readAndWriteCsvFile_LimitFileSize(pathFile, output_file, count, dataFrame, TYPE_DATA):\n",
    "    for i in range(0, dataFrame.shape[0]):\n",
    "        newSize = write_file_from_text(output_file + \"_\" + str(count) + '.txt', dataFrame, TYPE_DATA, idx=i)\n",
    "\n",
    "        # IF size of file MINIMUM_FILE_SIZE\n",
    "        if(newSize > MINIMUM_FILE_SIZE):\n",
    "            count += 1\n",
    "    return count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Time\n",
    "Export `30GB = 10 Min`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pathFile /workdir/Code/bma_transformer_model/data/raw/classification_dataset/scrapy_pptv36/pptv36/pptv36_55520.json\n",
      "pathFile /workdir/Code/bma_transformer_model/data/raw/classification_dataset/scrapy_thaipbs/thaipbs/thaipbs_108355.json\n",
      "pathFile /workdir/Code/bma_transformer_model/data/raw/classification_dataset/scrapy_springnews/springnews/springnews_542304.json\n",
      "pathFile /workdir/Code/bma_transformer_model/data/raw/classification_dataset/scrapy_siamrath/siamrath/siamrath_156220.json\n",
      "pathFile /workdir/Code/bma_transformer_model/data/raw/classification_dataset/scrapy_dailynews/dailynews/dailynews_774189.json\n",
      "pathFile /workdir/Code/bma_transformer_model/data/raw/classification_dataset/scrapy_naewna/naewna/naewna_485187.json\n",
      "pathFile /workdir/Code/bma_transformer_model/data/raw/classification_dataset/scrapy_prbangkok/prbangkok/prbangkok_MDY1cDBzNnM0NHIyb3Ezc3E2NnEyNDk0cDRyOTQzcjQ1NzIz.json\n",
      "pathFile /workdir/Code/bma_transformer_model/data/raw/classification_dataset/scrapy_prachachat/prachachat/prachachat_news-196305.json\n"
     ]
    }
   ],
   "source": [
    "for selector_dir in SELECTOR_DIR:\n",
    "    old_folder_file = \"\"\n",
    "    temp_filename = \"\"\n",
    "    for root, dirs, files in os.walk(os.path.join(ROOT_DIR, selector_dir)):\n",
    "        for file in files:\n",
    "            folder = os.path.basename(root)\n",
    "\n",
    "            # Type: \"classification_dataset\"\n",
    "            if (selector_dir == \"classification_dataset\" and folder != \"full\"):\n",
    "                # Input\n",
    "                pathFile = os.path.join(root, file)\n",
    "                print(\"pathFile\", pathFile)\n",
    "                break\n",
    "                filename, file_extension = os.path.splitext(pathFile)\n",
    "                folder_file = os.path.basename(root)\n",
    "\n",
    "                # Output\n",
    "                output_file = os.path.join(EXPORT_DIR, selector_dir, folder_file)\n",
    "                if(file_extension != '.jpg' or file_extension != '.xml' or file_extension != '.py'):\n",
    "                    count = create_folder(output_file, count)\n",
    "                    \n",
    "                    if(old_folder_file != folder_file):\n",
    "                        count = 0\n",
    "                        old_folder_file = folder_file\n",
    "                        \n",
    "                    #Read & Write New File\n",
    "                    count = readAndWriteFromJSONFile_LimitFileSize(pathFile, output_file, count)\n",
    "                    \n",
    "            elif (selector_dir == \"another_website\"):\n",
    "                # Input\n",
    "                pathFile = os.path.join(root, file)\n",
    "                folder_file = pathFile.split(\"/\")[-3]\n",
    "\n",
    "                # Output\n",
    "                output_file = os.path.join(EXPORT_DIR, selector_dir, folder_file)\n",
    "\n",
    "                if(old_folder_file != folder_file):\n",
    "                    count = 0\n",
    "                    old_folder_file = folder_file\n",
    "\n",
    "                count = create_folder(output_file, count)\n",
    "                count = readAndWriteFromTEXTFile_LimitFileSize(pathFile, output_file, count, \"TEXT\")\n",
    "\n",
    "            elif (selector_dir == \"thwiki-20200601-extracted\"):\n",
    "                # Input\n",
    "                pathFile = os.path.join(root, file)\n",
    "                folder_file = pathFile.split(\"/\")[-2]\n",
    "\n",
    "                # Output\n",
    "                output_file = os.path.join(EXPORT_DIR, selector_dir, \"Wiki\"+folder_file)\n",
    "                \n",
    "                if(old_folder_file != folder_file):\n",
    "                    count = 0\n",
    "                    old_folder_file = folder_file\n",
    "\n",
    "                count = create_folder(output_file, count)\n",
    "                count = readAndWriteFromTEXTFile_LimitFileSize(pathFile, output_file, count, \"WIKI\")\n",
    "            elif (selector_dir == \"data_lm\"):\n",
    "                # Input\n",
    "                pathFile = os.path.join(root, file)\n",
    "                folder_file = pathFile.split(\"/\")[-1]\n",
    "\n",
    "                # Output\n",
    "                output_file = os.path.join(EXPORT_DIR, selector_dir, \"Pantip\"+folder_file)\n",
    "                \n",
    "                if(old_folder_file != folder_file):\n",
    "                    count = 0\n",
    "                    old_folder_file = folder_file\n",
    "                count = create_folder(output_file, count)\n",
    "                dataFrame = pd.read_csv(pathFile, delimiter=',', encoding='utf-8')\n",
    "                count = readAndWriteCsvFile_LimitFileSize(pathFile, output_file, count, dataFrame, \"dataLM\")\n",
    "            elif (selector_dir == \"social_listening\"):\n",
    "                pathFile = os.path.join(root, file)\n",
    "                print(\">>\", pathFile)\n",
    "                folder_file = pathFile.split(\"/\")[-1]\n",
    "\n",
    "                # Output\n",
    "                output_file = os.path.join(EXPORT_DIR, selector_dir, \"SocialListening\"+folder_file)\n",
    "                \n",
    "                if(old_folder_file != folder_file):\n",
    "                    count = 0\n",
    "                    old_folder_file = folder_file\n",
    "                count = create_folder(output_file, count)\n",
    "                pathFile = \"/workdir/Code/bma_transformer_model/data/raw/social_listening/pantip_post_data.csv\"\n",
    "                dataFrame = pd.read_csv(pathFile, encoding='utf-8', delimiter=\"\\t\", error_bad_lines=False, names=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"])\n",
    "                print(dataFrame[\"D\"])\n",
    "                \n",
    "\n",
    "#                 count = readAndWriteCsvFile_LimitFileSize(pathFile, output_file, TEMP_Filename, count, dataFrame, \"social_listening\")\n",
    "#             break\n",
    "#         break\n",
    "#     break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
