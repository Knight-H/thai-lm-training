{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 subword algorithms help to improve your NLP model performance\n",
    "- Byte Pair Encoding (BPE)\n",
    "- WordPiece\n",
    "- Unigram Language Model\n",
    "- SentencePiece  \n",
    "\n",
    "Subword balances vocabulary size and footprint. Extreme case is we can only use 26 token (i.e. character) to present all English word. 16k or 32k subwords are recommended vocabulary size to have a good result.\n",
    "\n",
    "Many Asian language word cannot be separated by space. Therefore, the initial vocabulary is larger than English a lot. You may need to prepare over 10k initial word to kick start the word segmentation. From Schuster and Nakajima research, they propose to use 22k word and 11k word for Japanese and Korean respectively.  \n",
    "https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform subword tokenization, BPE is slightly modified in its implementation such that the frequently occurring subword pairs are merged together instead of being replaced by another byte to enable compression. This would basically lead the rare word athazagoraphobia to be split up into more frequent subwords such as ['▁ath', 'az', 'agor', 'aphobia'].\n",
    "https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers: How machines read - 28 JANUARY 2020\n",
    "Recommended read on tokenization  \n",
    "\n",
    "- **BPE**: Just uses the frequency of occurrences to identify the best match at every iteration until it reaches the predefined vocabulary size.\n",
    "- **WordPiece**: Similar to BPE and uses frequency occurrences to identify potential merges but makes the final decision based on the likelihood of the merged token\n",
    "- **Unigram**: A fully probabilistic model which does not use frequency occurrences. Instead, it trains a LM using a probabilistic model, removing the token which improves the overall likelihood the least and then starting over until it reaches the final token limit.\n",
    "- **SentencePiece** basically tries to bring all the subword tokenization tools and techniques under one banner. _\" SentencePiece is a re-implementation of sub-word units, an effective way to alleviate the open vocabulary problems in neural machine translation. SentencePiece supports two segmentation algorithms, byte-pair-encoding (BPE) [Sennrich et al.] and unigram language model [Kudo.]. \"_ (BPE and Unigram are reimplemented with improvements).\n",
    "    - __All other models assume input is already tokenized__: BPE and Unigram are great models but they share one big disadvantage- they both need to have their input already tokenized. BPE needs to have the input tokenized so that every character (including word-boundary characters) are tokenized. Only then can BPE count frequencies and start to merge tokens. Usually this is done by simply doing word level tokenization but, as we discussed earlier, this is a problem with tokenization since not all languages are space segmented. Similarly, the unigram model needs to have its input tokenized before it can start discarding tokens based on their probability distribution. SentencePiece deals with this by simply taking in an input in raw text and then doing everything (which we will discuss below) needed on that input to perform subword tokenization.\n",
    "    - __Encode everything as unicode ...__: SentencePiece first converts all the input into unicode characters. This means it doesn’t have to worry about different languages or characters or symbols. If it uses unicode it can just treat all input in the same way, which allows it to be language agnostic\n",
    "    - __… including  the spaces__: To get around the word segmenting issues, SentencePiece simply encodes spaces as a unicode symbol. Specifically it encodes it as unicode value U+2581 (underscore ‘_’ to those of us who don’t speak unicode). This helps with the language agnostic issues and the decoding issue. Since spaces are unicode encoded then they can be easily reversed or decoded and treated (i.e learned) like a normal language character. It sounds like a simple approach and I guess it is, but the best ideas tend to seem that way in the end\n",
    "\n",
    "\n",
    "https://blog.floydhub.com/tokenization-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Huggingface `tokenizers`__ : \n",
    "Provided Tokenizers\n",
    "- CharBPETokenizer: The original BPE\n",
    "- ByteLevelBPETokenizer: The byte level version of the BPE\n",
    "- SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
    "- BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece  \n",
    " \n",
    "We designed the library so that it provides all the required blocks to create end-to-end tokenizers in an interchangeable way. In that sense, we provide\n",
    "these various components: \n",
    "\n",
    "- **Normalizer**: Executes all the initial transformations over the initial input string. For example when you need to\n",
    "lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer. \n",
    "- **PreTokenizer**: In charge of splitting the initial input string. That's the component that decides where and how to\n",
    "pre-segment the origin string. The simplest example would be like we saw before, to simply split on spaces.\n",
    "- **Model**: Handles all the sub-token discovery and generation, this part is trainable and really dependant\n",
    " of your input data.\n",
    "- **Post-Processor**: Provides advanced construction features to be compatible with some of the Transformers-based SoTA\n",
    "models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens.\n",
    "- **Decoder**: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according\n",
    "to the `PreTokenizer` we used previously.\n",
    "- **Trainer**: Provides training capabilities to each model. \n",
    "\n",
    "Notebook for Tokenizers: https://github.com/huggingface/transformers/blob/master/notebooks/01-training-tokenizers.ipynb  \n",
    "Github Link for Python Binding: https://github.com/huggingface/tokenizers/tree/master/bindings/python\n",
    "\n",
    "Implementation: https://github.com/huggingface/tokenizers/tree/master/bindings/python/tokenizers/implementations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (0.8.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw_data_extraction/thwiki-20200601-extracted/WikiAD_2.txt\n",
      "../data/raw_data_extraction/thwiki-20200601-extracted/WikiAC_2.txt\n",
      "../data/raw_data_extraction/thwiki-20200601-extracted/WikiAF_2.txt\n",
      "../data/raw_data_extraction/thwiki-20200601-extracted/WikiAE_1.txt\n",
      "../data/raw_data_extraction/thwiki-20200601-extracted/WikiAB_1.txt\n",
      "../data/raw_data_extraction/classification_dataset/siamrath_0.txt\n",
      "../data/raw_data_extraction/classification_dataset/dailynews_0.txt\n",
      "../data/raw_data_extraction/classification_dataset/prachachat_0.txt\n",
      "../data/raw_data_extraction/classification_dataset/naewna_0.txt\n",
      "../data/raw_data_extraction/classification_dataset/springnews_0.txt\n",
      "../data/raw_data_extraction/another_website/pantip_87.txt\n",
      "../data/raw_data_extraction/another_website/pantip_535.txt\n",
      "../data/raw_data_extraction/another_website/dailynews_15.txt\n",
      "../data/raw_data_extraction/another_website/pantip_219.txt\n",
      "../data/raw_data_extraction/another_website/pantip_549.txt\n",
      "../data/raw_data_extraction/data_lm/Pantipdata_train.csv_209.txt\n",
      "../data/raw_data_extraction/data_lm/Pantipdata_train.csv_270.txt\n",
      "../data/raw_data_extraction/data_lm/Pantipdata_train.csv_23.txt\n",
      "../data/raw_data_extraction/data_lm/Pantipdata_train.csv_74.txt\n",
      "../data/raw_data_extraction/data_lm/Pantipdata_train.csv_356.txt\n",
      "../data/raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_5.txt\n",
      "../data/raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_1.txt\n",
      "../data/raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_2.txt\n",
      "../data/raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_0.txt\n",
      "../data/raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_3.txt\n",
      "\n",
      "I have a total of 1409 files!\n",
      "Amounts to a total of 41284.40 MB\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(\"../data\")\n",
    "\n",
    "# DATA_RAW_PATH = DATA_PATH/\"raw\"\n",
    "DATA_RAW_EXTRACTED_PATH = DATA_PATH/\"raw_data_extraction\"\n",
    "\n",
    "# 1. The data from thwiki\n",
    "THWIKI_FOLDER = Path(\"thwiki-20200601-extracted\")\n",
    "WIKI_FILES = list((DATA_RAW_EXTRACTED_PATH/THWIKI_FOLDER).glob(\"Wiki*.txt\"))\n",
    "list(map(print , WIKI_FILES[:5]))\n",
    "\n",
    "\n",
    "# 2. The classification data from jung and ninja\n",
    "CLASSIFICATION_JUNG_NINJA_FOLDER = Path(\"classification_dataset\")\n",
    "CLASSIFICATION_FILES = list((DATA_RAW_EXTRACTED_PATH/CLASSIFICATION_JUNG_NINJA_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , CLASSIFICATION_FILES[:5]))\n",
    "\n",
    "# 3. The Data from p'Moo Crawlers\n",
    "ANOTHER_WEBSITE_MOO_FOLDER = Path(\"another_website\")\n",
    "ANOTHER_WEBSITE_FILES = list((DATA_RAW_EXTRACTED_PATH/ANOTHER_WEBSITE_MOO_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , ANOTHER_WEBSITE_FILES[:5]))\n",
    "\n",
    "\n",
    "# 4. Senior Project Files\n",
    "SENIOR_PROJ_FOLDER = Path(\"data_lm\")\n",
    "SENIOR_PROJ_FILES = list((DATA_RAW_EXTRACTED_PATH/SENIOR_PROJ_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , SENIOR_PROJ_FILES[:5]))\n",
    "\n",
    "# 5. Guru Crawler Files\n",
    "GURU_CRAWLER_FOLDER = Path(\"social_listening\")\n",
    "GURU_CRAWLER_FILES = list((DATA_RAW_EXTRACTED_PATH/GURU_CRAWLER_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , GURU_CRAWLER_FILES[:5]))\n",
    "\n",
    "ALL_FILES = WIKI_FILES + CLASSIFICATION_FILES + ANOTHER_WEBSITE_FILES + SENIOR_PROJ_FILES + GURU_CRAWLER_FILES\n",
    "print(f\"\\nI have a total of {len(ALL_FILES)} files!\")\n",
    "\n",
    "\n",
    "# Output is in bytes - helper from Pathlib Path https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python\n",
    "def getStat(prev_value, cur_value):\n",
    "    if isinstance(prev_value, int):\n",
    "        return prev_value + cur_value.stat().st_size\n",
    "    return prev_value.stat().st_size + cur_value.stat().st_size\n",
    "\n",
    "from functools import reduce\n",
    "print(f\"Amounts to a total of {reduce(getStat, ALL_FILES)/1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[file for file in list(map(str, ALL_FILES)) if os.path.isdir(file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chardet\n",
    "# for filename in list(map(str, ALL_FILES))[::-1]:\n",
    "#     with open(filename, 'rb') as f:\n",
    "#         content_bytes = f.read()\n",
    "#     detected = chardet.detect(content_bytes)\n",
    "#     encoding = detected['encoding']\n",
    "#     print(f\"{filename}: detected as {encoding}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../data/raw_data_extraction/classification_dataset/siamrath_0.txt'),\n",
       " PosixPath('../data/raw_data_extraction/classification_dataset/dailynews_0.txt'),\n",
       " PosixPath('../data/raw_data_extraction/classification_dataset/prachachat_0.txt'),\n",
       " PosixPath('../data/raw_data_extraction/classification_dataset/naewna_0.txt'),\n",
       " PosixPath('../data/raw_data_extraction/classification_dataset/springnews_0.txt'),\n",
       " PosixPath('../data/raw_data_extraction/classification_dataset/pptv36_0.txt'),\n",
       " PosixPath('../data/raw_data_extraction/classification_dataset/prbangkok_0.txt'),\n",
       " PosixPath('../data/raw_data_extraction/classification_dataset/thaipbs_0.txt')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list((DATA_RAW_EXTRACTED_PATH/CLASSIFICATION_JUNG_NINJA_FOLDER).glob(\"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train `SeniorProjectTokenizer` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from senior_project_util import ThaiTokenizer, pre_rules_th, post_rules_th\n",
    "from fastai.text.transform import BaseTokenizer, Tokenizer\n",
    "from fastai.text.data import TokenizeProcessor, NumericalizeProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ม่าย', 'เอา', 'เปง', 'ไง', 'บ้าง', 'น่ารัก', 'จุงเบย']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='ม่ายเอาเปงไงบ้างน่ารักจุงเบย'\n",
    "pyThai_tt = ThaiTokenizer()\n",
    "a = pyThai_tt.tokenizer(text)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = Tokenizer(tok_func = ThaiTokenizer, lang = 'th', pre_rules = pre_rules_th, post_rules=post_rules_th, n_cpus=10)\n",
    "tt.process_all([text[:1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_processor = TokenizeProcessor(tokenizer=tt, chunksize=300000, mark_fields=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxbos', ' ', 'เวลา', 'xxnum', ' ', 'น.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_processor.process_one(\"เวลา 12.00น.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_tokenized(file_path):\n",
    "    print(f\"I AM DOING {file_path}\")\n",
    "    directory, filename = os.path.split(file_path)\n",
    "#     cached_features_file = os.path.join(\n",
    "#         self.cached_directory, f\"cached_lm_{tokenizer.__class__.__name__}_{str(self.block_size)}_{filename}\",\n",
    "#     )\n",
    "\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    tokens = tokenizer_processor.process_one(text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1409 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I AM DOING ../data/raw_data_extraction/thwiki-20200601-extracted/WikiAC_2.txtI AM DOING ../data/raw_data_extraction/thwiki-20200601-extracted/WikiAF_2.txtI AM DOING ../data/raw_data_extraction/thwiki-20200601-extracted/WikiAE_1.txtI AM DOING ../data/raw_data_extraction/thwiki-20200601-extracted/WikiAD_2.txt\n",
      "I AM DOING ../data/raw_data_extraction/thwiki-20200601-extracted/WikiAB_1.txtI AM DOING ../data/raw_data_extraction/thwiki-20200601-extracted/WikiAA_3.txt\n",
      "\n",
      "I AM DOING ../data/raw_data_extraction/thwiki-20200601-extracted/WikiAE_0.txt\n",
      "I AM DOING ../data/raw_data_extraction/thwiki-20200601-extracted/WikiAE_3.txtI AM DOING ../data/raw_data_extraction/thwiki-20200601-extracted/WikiAC_0.txtI AM DOING ../data/raw_data_extraction/thwiki-20200601-extracted/WikiAB_2.txt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I AM DOING ../data/raw_data_extraction/thwiki-20200601-extracted/WikiAF_0.txt\n",
      "I AM DOING ../data/raw_data_extraction/thwiki-20200601-extracted/WikiAD_3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-91:\n",
      "Process ForkPoolWorker-84:\n",
      "Process ForkPoolWorker-92:\n",
      "Process ForkPoolWorker-88:\n",
      "Process ForkPoolWorker-89:\n",
      "Process ForkPoolWorker-86:\n",
      "Process ForkPoolWorker-90:\n",
      "Process ForkPoolWorker-85:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-81-070cad943125>\", line 10, in load_data_tokenized\n",
      "    tokens = tokenizer_processor.process_one(text)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/data.py\", line 291, in process_one\n",
      "    return self.tokenizer._process_all_1(_join_texts([item], self.mark_fields, self.include_bos, self.include_eos))[0]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in _process_all_1\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in <listcomp>\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-81-070cad943125>\", line 10, in load_data_tokenized\n",
      "    tokens = tokenizer_processor.process_one(text)\n",
      "Process ForkPoolWorker-87:\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 106, in process_text\n",
      "    toks = tok.tokenizer(t)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-81-070cad943125>\", line 10, in load_data_tokenized\n",
      "    tokens = tokenizer_processor.process_one(text)\n",
      "  File \"<ipython-input-81-070cad943125>\", line 10, in load_data_tokenized\n",
      "    tokens = tokenizer_processor.process_one(text)\n",
      "  File \"/workdir/Code/bma_transformer_model/thai-lm-training/senior_project_util.py\", line 84, in tokenizer\n",
      "    return self.pyengine.word_tokenize(t)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/data.py\", line 291, in process_one\n",
      "    return self.tokenizer._process_all_1(_join_texts([item], self.mark_fields, self.include_bos, self.include_eos))[0]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/core.py\", line 493, in word_tokenize\n",
      "    keep_whitespace=self.__keep_whitespace,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in _process_all_1\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/core.py\", line 115, in word_tokenize\n",
      "    segments = segment(text, custom_dict)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in <listcomp>\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/data.py\", line 291, in process_one\n",
      "    return self.tokenizer._process_all_1(_join_texts([item], self.mark_fields, self.include_bos, self.include_eos))[0]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 106, in process_text\n",
      "    toks = tok.tokenizer(t)\n",
      "  File \"<ipython-input-81-070cad943125>\", line 10, in load_data_tokenized\n",
      "    tokens = tokenizer_processor.process_one(text)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/newmm.py\", line 159, in segment\n",
      "    return list(_onecut(text, custom_dict))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in _process_all_1\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/workdir/Code/bma_transformer_model/thai-lm-training/senior_project_util.py\", line 84, in tokenizer\n",
      "    return self.pyengine.word_tokenize(t)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/newmm.py\", line 75, in _onecut\n",
      "    valid_poss = tcc_pos(text)  # breaking positions that are TCC-valid\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/data.py\", line 291, in process_one\n",
      "    return self.tokenizer._process_all_1(_join_texts([item], self.mark_fields, self.include_bos, self.include_eos))[0]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in <listcomp>\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/core.py\", line 493, in word_tokenize\n",
      "    keep_whitespace=self.__keep_whitespace,\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 106, in process_text\n",
      "    toks = tok.tokenizer(t)\n",
      "Process ForkPoolWorker-83:\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/core.py\", line 115, in word_tokenize\n",
      "    segments = segment(text, custom_dict)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-81-070cad943125>\", line 10, in load_data_tokenized\n",
      "    tokens = tokenizer_processor.process_one(text)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in _process_all_1\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/tcc.py\", line 88, in tcc_pos\n",
      "    for w in tcc(text):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in <listcomp>\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/workdir/Code/bma_transformer_model/thai-lm-training/senior_project_util.py\", line 84, in tokenizer\n",
      "    return self.pyengine.word_tokenize(t)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/newmm.py\", line 159, in segment\n",
      "    return list(_onecut(text, custom_dict))\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/data.py\", line 291, in process_one\n",
      "    return self.tokenizer._process_all_1(_join_texts([item], self.mark_fields, self.include_bos, self.include_eos))[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/tcc.py\", line 66, in tcc\n",
      "    m = _PAT_TCC.match(text[p:])\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/newmm.py\", line 75, in _onecut\n",
      "    valid_poss = tcc_pos(text)  # breaking positions that are TCC-valid\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 106, in process_text\n",
      "    toks = tok.tokenizer(t)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/data.py\", line 291, in process_one\n",
      "    return self.tokenizer._process_all_1(_join_texts([item], self.mark_fields, self.include_bos, self.include_eos))[0]\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/tcc.py\", line 88, in tcc_pos\n",
      "    for w in tcc(text):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/core.py\", line 493, in word_tokenize\n",
      "    keep_whitespace=self.__keep_whitespace,\n",
      "  File \"<ipython-input-81-070cad943125>\", line 10, in load_data_tokenized\n",
      "    tokens = tokenizer_processor.process_one(text)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/tcc.py\", line 67, in tcc\n",
      "    if m:\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/data.py\", line 291, in process_one\n",
      "    return self.tokenizer._process_all_1(_join_texts([item], self.mark_fields, self.include_bos, self.include_eos))[0]\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-81-070cad943125>\", line 10, in load_data_tokenized\n",
      "    tokens = tokenizer_processor.process_one(text)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-81-070cad943125>\", line 10, in load_data_tokenized\n",
      "    tokens = tokenizer_processor.process_one(text)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in _process_all_1\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/data.py\", line 291, in process_one\n",
      "    return self.tokenizer._process_all_1(_join_texts([item], self.mark_fields, self.include_bos, self.include_eos))[0]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in <listcomp>\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/data.py\", line 291, in process_one\n",
      "    return self.tokenizer._process_all_1(_join_texts([item], self.mark_fields, self.include_bos, self.include_eos))[0]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in _process_all_1\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in _process_all_1\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in _process_all_1\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in <listcomp>\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 106, in process_text\n",
      "    toks = tok.tokenizer(t)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in <listcomp>\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in <listcomp>\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/workdir/Code/bma_transformer_model/thai-lm-training/senior_project_util.py\", line 84, in tokenizer\n",
      "    return self.pyengine.word_tokenize(t)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 106, in process_text\n",
      "    toks = tok.tokenizer(t)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 106, in process_text\n",
      "    toks = tok.tokenizer(t)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/core.py\", line 493, in word_tokenize\n",
      "    keep_whitespace=self.__keep_whitespace,\n",
      "  File \"/workdir/Code/bma_transformer_model/thai-lm-training/senior_project_util.py\", line 84, in tokenizer\n",
      "    return self.pyengine.word_tokenize(t)\n",
      "  File \"/workdir/Code/bma_transformer_model/thai-lm-training/senior_project_util.py\", line 84, in tokenizer\n",
      "    return self.pyengine.word_tokenize(t)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/core.py\", line 493, in word_tokenize\n",
      "    keep_whitespace=self.__keep_whitespace,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/core.py\", line 115, in word_tokenize\n",
      "    segments = segment(text, custom_dict)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 106, in process_text\n",
      "    toks = tok.tokenizer(t)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/newmm.py\", line 159, in segment\n",
      "    return list(_onecut(text, custom_dict))\n",
      "  File \"/workdir/Code/bma_transformer_model/thai-lm-training/senior_project_util.py\", line 84, in tokenizer\n",
      "    return self.pyengine.word_tokenize(t)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/newmm.py\", line 75, in _onecut\n",
      "    valid_poss = tcc_pos(text)  # breaking positions that are TCC-valid\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/core.py\", line 493, in word_tokenize\n",
      "    keep_whitespace=self.__keep_whitespace,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/core.py\", line 115, in word_tokenize\n",
      "    segments = segment(text, custom_dict)\n",
      "  File \"/workdir/Code/bma_transformer_model/thai-lm-training/senior_project_util.py\", line 84, in tokenizer\n",
      "    return self.pyengine.word_tokenize(t)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/core.py\", line 493, in word_tokenize\n",
      "    keep_whitespace=self.__keep_whitespace,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/newmm.py\", line 159, in segment\n",
      "    return list(_onecut(text, custom_dict))\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in _process_all_1\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/core.py\", line 115, in word_tokenize\n",
      "    segments = segment(text, custom_dict)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/newmm.py\", line 75, in _onecut\n",
      "    valid_poss = tcc_pos(text)  # breaking positions that are TCC-valid\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 114, in <listcomp>\n",
      "    return [self.process_text(str(t), tok) for t in texts]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/newmm.py\", line 159, in segment\n",
      "    return list(_onecut(text, custom_dict))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/newmm.py\", line 75, in _onecut\n",
      "    valid_poss = tcc_pos(text)  # breaking positions that are TCC-valid\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/tcc.py\", line 88, in tcc_pos\n",
      "    for w in tcc(text):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/core.py\", line 115, in word_tokenize\n",
      "    segments = segment(text, custom_dict)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/tcc.py\", line 88, in tcc_pos\n",
      "    for w in tcc(text):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/newmm.py\", line 159, in segment\n",
      "    return list(_onecut(text, custom_dict))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/conda/lib/python3.7/site-packages/fastai/text/transform.py\", line 106, in process_text\n",
      "    toks = tok.tokenizer(t)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/tcc.py\", line 66, in tcc\n",
      "    m = _PAT_TCC.match(text[p:])\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/tcc.py\", line 66, in tcc\n",
      "    m = _PAT_TCC.match(text[p:])\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/workdir/Code/bma_transformer_model/thai-lm-training/senior_project_util.py\", line 84, in tokenizer\n",
      "    return self.pyengine.word_tokenize(t)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/core.py\", line 493, in word_tokenize\n",
      "    keep_whitespace=self.__keep_whitespace,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/tcc.py\", line 88, in tcc_pos\n",
      "    for w in tcc(text):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pythainlp/tokenize/core.py\", line 493, in word_tokenize\n",
      "    keep_whitespace=self.__keep_whitespace,\n",
      "  File \"<ipython-input-81-070cad943125>\", line 10, in load_data_tokenized\n",
      "    tokens = tokenizer_processor.process_one(text)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-eb8d7a746494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    735\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "\n",
    "num_processes = 10\n",
    "sample_path = ALL_FILES\n",
    "\n",
    "with Pool(processes=num_processes) as p:\n",
    "    tokens = list(tqdm.tqdm(p.imap(load_data_tokenized, sample_path), total=len(sample_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I AM DOING ../data/raw_data_extraction/thwiki-20200601-extracted/WikiAD_2.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xxbos', ' ', 'ขั้นตอนวิธี', 'ของ', 'ค', 'ริ', 'ส', 'โต', 'ไฟ', 'ด์']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data_tokenized(ALL_FILES[0])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Vocabs\n",
    "copied from Fastai [`Vocab.create()`](https://github.com/fastai/fastai/blob/d418294f0f17382f7a33bb72b93f5055a7768b14/fastai/text/transform.py#L149)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 50000\n",
    "min_freq = 3\n",
    "\n",
    "BOS,EOS,FLD,UNK,PAD = 'xxbos','xxeos','xxfld','xxunk','xxpad'\n",
    "TK_REP,TK_WREP, TK_NUM, TK_LAUGH = 'xxrep','xxwrep', 'xxnum', 'xxlaugh'\n",
    "text_spec_tok = [UNK,PAD,BOS,EOS,FLD,TK_REP,TK_WREP, TK_NUM, TK_LAUGH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Counter(p for o in tokens for p in o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 56922, 'ใน': 4840, 'และ': 4410, '\\n': 4062, 'ที่': 3813, 'เป็น': 3394, 'ของ': 3071, 'ได้': 2150, 'มี': 1895, '(': 1654, ')': 1569, '\"': 1534, 'ปี': 1273, 'ให้': 1194, 'โดย': 1182, 'พ.ศ.': 1159, 'จาก': 1104, 'กับ': 1023, 'ซึ่ง': 1001, 'ทํา': 994}\n"
     ]
    }
   ],
   "source": [
    "print({i:v for i,v in freq.most_common(20)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'ใน', 'และ', '\\n', 'ที่', 'เป็น', 'ของ', 'ได้', 'มี', '(', ')', '\"', 'ปี', 'ให้', 'โดย', 'พ.ศ.', 'จาก', 'กับ', 'ซึ่ง', 'ทํา', 'การ', 'จะ', 'เมื่อ', 'ว่า', 'นี้', 'ไป', 'มา', ',', '-', 'หรือ', 'วันที่', 'ด้วย', 'คือ', 'ก็', 'คน', 'พระ', 'แต่', 'ได้รับ', 'เพื่อ', 'เรื่อง', 'ใช้', 'เขา', 'เมือง', 'ถูก', 'น้ํา', 'จํา', 'ํา', 'ยัง', 'จึง', 'อยู่', '2', 'ค.ศ.', 'ไม่', 'นํา', 'สํา', '1', 'ทาง', 'ผู้', 'นั้น', 'แห่ง', 'ขึ้น', 'ที่มี', '์', 'เกิด', 'สามารถ', 'หนึ่ง', 'แล้ว', '3', 'ถึง', 'คํา', 'ภาพยนตร์', 'ความ', 'ทั้ง', 'หรับ', 'ชื่อ', 'ส', 'แบบ', 'กา', 'ภาษา', 'ต่อมา', '()', 'อีก', 'กัน', 'ปัจจุบัน', 'ระหว่าง', 'เช่น', 'ทรง', 'เธอ', 'อร', 'ตัว', 'ไทย', '.', 'ตาม', 'ประเทศ', 'ด้าน', 'เพลง', 'มาก', 'จัด', 'อยู่ใน', 'เข้า']\n"
     ]
    }
   ],
   "source": [
    "itos = [o for o,c in freq.most_common(max_vocab) if c >= min_freq]\n",
    "print(itos[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITOS ['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxrep', 'xxwrep', 'xxnum', 'xxlaugh', ' ', 'ใน', 'และ', '\\n', 'ที่', 'เป็น', 'ของ', 'ได้', 'มี', '(', ')', '\"', 'ปี', 'ให้', 'โดย', 'พ.ศ.', 'จาก', 'กับ', 'ซึ่ง', 'ทํา', 'การ', 'จะ', 'เมื่อ', 'ว่า', 'นี้', 'ไป', 'มา', ',', '-', 'หรือ', 'วันที่', 'ด้วย', 'คือ', 'ก็', 'คน', 'พระ', 'แต่', 'ได้รับ', 'เพื่อ', 'เรื่อง', 'ใช้', 'เขา', 'เมือง', 'ถูก', 'น้ํา', 'จํา', 'ํา', 'ยัง', 'จึง', 'อยู่', '2', 'ค.ศ.', 'ไม่', 'นํา', 'สํา', '1', 'ทาง', 'ผู้', 'นั้น', 'แห่ง', 'ขึ้น', 'ที่มี', '์', 'เกิด', 'สามารถ', 'หนึ่ง', 'แล้ว', '3', 'ถึง', 'คํา', 'ภาพยนตร์', 'ความ', 'ทั้ง', 'หรับ', 'ชื่อ', 'ส', 'แบบ', 'กา', 'ภาษา', 'ต่อมา', '()', 'อีก', 'กัน', 'ปัจจุบัน', 'ระหว่าง', 'เช่น', 'ทรง', 'เธอ', 'อร', 'ตัว', 'ไทย', '.', 'ตาม', 'ประเทศ', 'ด้าน', 'เพลง', 'มาก', 'จัด', 'อยู่ใน', 'เข้า', 'นา', 'ต่อ', 'แสดง', 'จํานวน', 'วัด', 'สร้าง', 'ริ', 'กว่า', 'ช่วง', ':', 'หลาย', 'ร์', 'รา', 'ส่วน', 'ชุด', 'แรก', 'ๆ', 'โรงเรียน', 'อําเภอ', '4', 'นัก', 'ระดับ', 'ครั้ง', 'ตั้งแต่', 'นี', 'ทีม', 'โรค', 'ไว้', 'จังหวัด', 'ออก', 'เริ่ม', 'ณ', '5', 'อย่าง', 'กลุ่ม', 'ต้อง', 'ประ', 'งาน', 'บน', 'สอง', 'ประเทศไทย', 'มาจาก', 'สมเด็จ', 'กี', 'ใหม่', '6', 'เพราะ', 'รี', 'พบ', 'แม่', 'การศึกษา', 'โลก', 'สําคัญ', 'จน', 'ท่าน', 'ลง', '/', 'ดี', 'บิ', 'ยาง', 'บริษัท', 'สําเร็จ', 'อันดับ', 'ระบบ', 'นาย', 'เนิน', 'มัน', 'ตําแหน่ง', 'ถนน', 'ล', 'เขต', 'ใหญ่', 'ปลา', 'อาจ', 'ศึกษา', 'ประมาณ', 'ที่สุด', 'เนื่องจาก', 'เดิม', 'น', '“', 'กําลัง', 'เร', 'ลํา', '\\u200b', 'รางวัล', 'ที่จะ', 'ผลงาน', 'เคย', 'พระเจ้า', 'ตา', 'โร', 'เด', 'ผ่าน', 'อา', 'หญิง', 'อัลบั้ม', 'เอ', 'มหา', 'โอ', 'โน', '7', 'ชาว', 'อํา', 'เจ้า', 'มหาวิทยาลัย', 'หลังจาก', 'เพียง', 'เม', '10', '”', 'สมัย', 'โดยมี', 'ตําบล', 'การแข่งขัน', 'สาขา', 'มัก', 'พระองค์', 'วัคซีน', 'ทั้งหมด', 'บริเวณ', 'โก', 'หลัก', 'ลี', 'อดีต', 'ประเภท', 'ราช', 'the', 'พื้นที่', 'ก่อน', 'ร่วม', 'ชนิด', 'กํา', 'ต่าง ๆ', 'จีน', 'ไม่มี', 'รัฐ', 'ล้าน', 'วงศ์', 'ซี', 'มิ', 'รวม', 'คา', 'ซา', 'วัน', 'นิ', 'ตน', 'ฟุตบอล', 'อาการ', 'ลา', 'ยา', 'of', 'ฝ่าย', 'ครั้งแรก', 'เวลา', 'แก่', 'ตั้ง', 'โซ', 'ของเขา', 'ภาพ', 'วิ', 'กลาง', 'ง', 'ได้แก่', 'บาง', 'นอกจากนี้', 'แขวง', 'กระแส', 'ขนาด', 'ค', 'หลัง', 's', 'เรียกว่า', 'ทอง', 'คนไข้', 'ส์', 'ดํา', 'เซ', 'เค', 'เกิดขึ้น', 'สูง', 'ยาน', 'ละคร', 'เล่น', 'นักแสดง', 'ภาค', 'ลิ', 'ลีก', 'ทํางาน', 'ยังมี', 'โค', 'ดิ', 'อัน', 'ย์', 'ดับ', 'ไม่ได้', 'เก', 'ต่างๆ', 'วา', '9', '–', 'ซิ', 'รอบ', 'ให้กับ', 'เดียวกัน', 'น้อย', 'รุ่น', 'รูปแบบ', 'เทียน', 'เรียก', 'บ้าน', '20', 'สโมสร', 'เปิด', 'ใต้', 'ผล', 'ประกอบด้วย', '%', '8', 'มากกว่า', 'ดา', 'ข้อมูล', 'หน่าย', 'เต', 'ร่วมกับ', 'ชื่อว่า', 'ชั้น', 'กลายเป็น', 'ยุค', 'ส่ง', 'รูป', 'ประกาศ', 'เขียน', 'ตอน', 'อีกครั้ง', 'ตั้งอยู่', 'เกี่ยวกับ', 'องค์', 'เมตร', 'การแสดง', 'วง', 'แอ', 'กรุงธนบุรี', 'จากนั้น', 'ช่อง', 'พฤศจิกายน', 'ช่วย', 'สหรัฐอเมริกา', 'บท', 'กองทัพ', 'พัฒนา', 'ดังกล่าว', 'นาง', 'อายุ', 'ธันวาคม', 'ติ', 'เจริญ', 'เดือน', 'ฟ', 'อารมณ์', 'จุด', 'กัด', 'เหนือ', 'รัฐบาล', 'แมน', 'กําหนด', 'รวมถึง', 'ก็ได้', 'สาย', 'สงคราม', 'ฟุ้ง', 'พล่าน', 'เดียว', 'เลือก', 'อื่น', 'ฮา', 'บุคคล', 'ตุลาคม', 'โม', 'ออกมา', 'ปรากฏ', 'ชาติ', \"'\", 'รวมทั้ง', 'ยาว', 'พระราชทาน', 'พฤษภาคม', '12', 'ประกอบ', 'รับ', 'ออกจาก', 'พม่า', 'แนว', 'สี', 'กันยายน', 'เกาะ', 'รน', 'กลับมา', 'จนถึง', 'รับรางวัล', 'ทุก', 'ขอ', 'พระยา', 'แก้', 'พวก', 'สอด', 'ที่ใช้', 'ลักษณะ', 'สนาม', 'เจ', 'ไปยัง', 'ยู', 'รายการ', 'ออ', 'มกราคม', 'โครงการ', 'อื่น ๆ', 'ผลิต', 'อี', 'บี', 'อีกด้วย', '16', 'สาม', 'ประชากร', 'นคร', 'เกม', 'ว่าจะ', 'หมายถึง', 'แต่ละ', 'แก้ว', 'ศรี', 'คณะกรรมการ', 'แดง', '2557', 'กลับ', 'แทน', 'เป็นการ', '30', 'ภายใน', 'จบ', 'ส่วนใหญ่', 'ทั่วไป', 'อธิบาย', 'เป็นต้น', 'ในฐานะ', 'ว่าย', 'หน้าที่', 'มีนาคม', 'ลาย', 'บาท', '15', 'ตัน', 'ก', 'พรรค', 'รด', 'ระ', 'รุ', 'เสียง', 'ฝรั่งเศส', 'โด', 'ท', 'แห่งชาติ', 'อาหาร', 'เปลี่ยน', 'อ', '),', 'ทั้งสอง', 'เข้าร่วม', 'สิงหาคม', 'กฎหมาย', 'จ', 'การผลิต', 'จิ', 'ภาวะ', 'โบ', 'โต', 'วิธี', 'เป็นหนึ่ง', 'เพชร', 'ร้าน', 'อาจารย์', 'ย้าย', 'ชิ', 'ซู', 'ถู', 'การสร้าง', 'เครื่อง', 'นิยม', 'คลอง', 'รพะ', 'หนัก', 'คิด', 'พนม', 'ละ', 'ฉาย', 'ตาก', 'เข้ามา', 'สิ่ง', 'จา', '2554', 'สถาบัน', 'เกือบ', 'สติปัฏฐาน', 'แผ่น', 'ยะ', 'สติ', 'เป็นที่รู้จัก', 'หนัง', 'สายการบิน', 'เกียน', 'หา', 'เส้นทาง', 'เป็นที่', 'ชีวิต', 'กุมภาพันธ์', 'ต', 'เท่านั้น', 'หัว', '(;', 'รับบท', 'บร', 'เพิ่ม', 'การใช้', 'ประชาชน', 'มากที่สุด', 'ดู', 'อาชีพ', 'แบ่ง', 'นาน', 'ชาย', 'หนังสือ', 'สมาคม', 'นิตยสาร', 'เอเชีย', '2014', 'เขมร', 'พล', 'ชิง', 'ก่อตั้ง', 'เอา', 'โปรดเกล้าฯ', 'หมายเลข', 'ฝั่ง', 'เครื่องบิน', 'สัญญาณ', 'เหล่านี้', 'เรียน', 'ครู', 'กรกฎาคม', 'อาคาร', 'เด็ก', 'สุดท้าย', 'ขนาดใหญ่', 'ออกแบบ', 'ร', 'ของเธอ', 'ใบ', 'คณะ', 'กรรมฐาน', 'การพัฒนา', 'ตัด', 'ตํา', 'คล้าย', 'การอ้างอิง', 'หน้า', 'ประสบ', 'สูงสุด', 'ฮิ', 'อังกฤษ', 'อะ', 'ค่า', 'ว', 'ป่า', 'ต้น', 'ชนะ', 'วงการ', 'กิโลเมตร', '11', 'มากมาย', 'กอง', 'สถานี', 'ซิงเกิล', 'ดังนั้น', 'จริง', 'ถ้า', 'ที่อยู่', 'กรุงเทพมหานคร', 'ด', '\"\"', 'ชิ้น', 'ป', 'อาศัย', 'แยก', 'ก่อ', 'ใส่', 'สมาชิก', 'เดอะ', 'ดังนี้', 'ลด', 'เข้าสู่', 'กล่าว', 'นับ', 'เดินทาง', 'ยก', 'ทั้งนี้', 'เมษายน', 'แผ่นดิน', 'ตี', 'วาง', 'ตัวละคร', 'อรรถกถา', 'เส้น', '24', 'ลิง', 'and', 'ธรรม', 'โดยเฉพาะ', 'นวย', 'เสด็จ', 'ทหาร', 'เห็น', 'ดารา', 'เท', 'ไฮ', 'ต่อต้าน', 'วอลเลย์บอล', 'กระ', 'เลย', 'สาร', 'หัวหน้า', 'เอง', 'เงิน', 'สู่', 'ต์', 'สุ', 'เทพ', 'อาร์', 'สูตร', 'รส', 'บุตร', 'ดํารง', 'ทา', '17', 'ซ', '25', 'การก', 'มักจะ', 'รักษา', 'คัดเลือก', 'ยอดเยี่ยม', 'เชิง', 'เอ็ดดี้', 'ยังคง', 'อื่นๆ', 'คู่', 'รู', 'สํานักงาน', 'เฉพาะ', 'วิทยาศาสตร์', 'บรม', 'ผู้หญิง', 'อินเดีย', 'โล', 'ขาย', 'ลอง', 'มนุษย์', 'บิดา', 'ต่าง', 'กุล', 'เรื่องราว', 'จิน', 'ซ์', 'formula', '_', '(\"', '19', '23', 'รอ', 'หมู่บ้าน', 'ไพร', 'มิถุนายน', 'รอง', 'ติด', 'ปฏิกิริยา', 'ทั่วโลก', 'อิน', 'ben', 'การบินไทย', 'ปัญหา', 'เสนอ', 'อย่างไรก็ตาม', 'กีฬา', 'บ', 'เพื่อน', 'วี', '2553', 'พิเศษ', 'ฉบับ', 'กันใน', 'เบ', 'รัก', 'วิจัย', 'แปล', 'อิตาลี', 'เหตุการณ์', 'เหล่า', 'ญี่ปุ่น', 'ศาสตราจารย์', 'หม่อมเจ้า', 'ศิลปิน', 'แฮ', 'ปุ', 'ซึมเศร้า', '22', 'ภายใต้', 'วัฒนธรรม', 'ขนาดเล็ก', 'พูด', 'เพื่อให้', 'มากขึ้น', 'พงษ์', 'เนอ', 'เบอร์', 'ปลาย', 'ในเมือง', 'มะ', 'โท', 'ตัวเอง', 'ปกติ', 'ดร.', 'ปา', 'ธุรกิจ', 'กรุง', '18', 'ต่ํา', '29', 'พบว่า', 'พอ', 'เสียชีวิต', 'มีอาการ', 'ที่สอง', 'มาร', 'ตะวันออก', 'wwe', 'ตะวันตก', 'ราว', 'ซีรีส์', 'เล็ก', 'ร้อง', 'ไฟ', '\")', 'เรา', '28', 'กน', 'พยายาม', 'ต่อไป', 'แถบ', 'ม', 'ําเภอ', 'นักเรียน', 'พร้อมกับ', 'ถือเป็น', 'สังคม', 'ขัต', 'นัด', 'จี', 'บทบาท', 'กษัตริย์', 'ขั้ว', 'แม่เหล็ก', 'ขั้นตอนวิธี', '14', 'ออกไป', 'เริ่มต้น', 'ไม่ใช่', 'ลูก', 'พา', 'บา', 'ชัย', 'น.', 'วิชา', 'แม้', 'ง่าย', 'หลวง', 'เสีย', 'อักษร', 'ระยะ', 'ประธาน', 'ภูเขา', 'ในขณะที่', 'รัสเซีย', 'ลาว', '’', 'วิธีการ', 'ไม่สามารถ', 'สถานะ', 'ท่า', 'ราม', 'อเมริกา', 'สี่', 'ฐาน', 'นักร้อง', '2559', '2558', 'ทางหลวง', 'a', '2551', 'ป้องกัน', ';', 'ดอลลาร์สหรัฐ', 'แนะ', 'มีลักษณะ', 'หลังจากนั้น', '2008', '2009', 'การวิจัย', 'เหมือน', 'รัฐมนตรี', 'เล่ม', 'ร้อยละ', 'สา', 'อุตสาหกรรม', 'ทะเล', 'ประเทศญี่ปุ่น', '34', '2011', 'พวกเขา', 'ดีเด่น', 'เจ้าพระยา', 'แซ็ค', 'ข้อ', 'จะต้อง', 'หัวใจ', 'ช้าง', '13', 'พราน', 'หมู่', 'ขณะ', 'ที', 'หลังจากที่', 'มีชื่อ', 'กว้าง', 'การออกแบบ', 'ชิงแชมป์', 'ดินแดน', '2019', 'กค', '2549', 'ถ้วย', 'ห่าง', 'การให้', 'ศูนย์', 'กล่าวว่า', 'อากาศ', 'รบ', 'บทความ', 'ยึด', 'เช่นกัน', 'ในประเทศ', '2556', 'ชาวอเมริกัน', 'กะ', 'ล้ํา', 'ว์', 'กําเนิด', 'อิ', 'อุทยาน', 'ครั้งนี้', 'ใด', 'พิจารณา', 'วิน', 'กจ', 'อุปกรณ์', 'การบิน', 'สังกัด', 'ผ้า', 'ฯ', 'บํา', 'พระบาท', 'พร้อม', 'สภาพ', 'พิมพ์', 'ตลอด', 'รัชกาล', 'ยิง', 'ใกล้', 'ออสเตรเลีย', 'สัญลักษณ์', 'เพ', 'เลอ', 'ชา', '2016', 'ช', 'กัมพูชา', 'โภช', 'อุดมศึกษา', 'บิต', 'ความสามารถ', 'อัตรา', 'แข่งขัน', 'เกิน', 'ตัวอย่าง', 'นุ', 'คราว', '21', '27', 'ดัง', '26', '40', '200', 'ย', 'แตก', 'พลังงาน', 'อย่างเป็นทางการ', 'โบราณ', 'คุ', 'เอ็ม', 'ศ', '2010', 'ร่างกาย', 'เอเชียนเกมส์', 'คอ', 'สมบูรณ์', 'ตรงกับ', 'ที่วัด', 'โรงพยาบาล', 'บุญ', '100', 'หิน', 'อวกาศ', 'บุรี', 'สถานที่', 'จิต', 'กรุณา', 'หน่วย', 'ตนเอง', 'ถือ', 'ไต', 'ปล้ํา', 'นาม', 'สาว', 'ติดกับ', 'จุฬาลงกรณ์มหาวิทยาลัย', 'เยอรมัน', 'ไร', 'จักรวรรดิ', 'กล่าวถึง', 'ชาร์ต', 'in', 'ยูไนเต็ด', 'ย่าน', 'ขั้น', 'สุด', 'ชอบ', 'ไพ', 'แอน', 'ราชา', 'ยอด', '2555', 'แรง', 'ดนตรี', 'บรรดา', 'สหรัฐ', 'ติดต่อ', 'ปัญญา', 'สั่ง', 'โรง', 'จนกระทั่ง', 'แกน', 'เข้าไป', 'ดอก', 'ตลาด', 'เปิดตัว', 'สิบ', 'นอ', 'แต่ง', 'วัยรุ่น', 'ประวัติศาสตร์', 'วัสดุ', 'นาซี', 'คัส', 'สตาร์', 'ไปสู่', 'ถือว่า', 'นอกจาก', 'เอฟ', 'ศักดิ์', 'ศึก', 'ครอบคลุม', 'โครงสร้าง', 'ควบคุม']\n"
     ]
    }
   ],
   "source": [
    "for o in reversed(text_spec_tok):\n",
    "    if o in itos: itos.remove(o)\n",
    "    itos.insert(0, o)\n",
    "itos = itos[:max_vocab]\n",
    "if len(itos) < max_vocab: #Make sure vocab size is a multiple of 8 for fast mixed precision training\n",
    "    while len(itos)%8 !=0: itos.append('xxfake')\n",
    "print(\"ITOS\", itos[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8064"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITE_OUT_FILE = \"senior_project_vocab.txt\"\n",
    "with open(WRITE_OUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(itos)\n",
    "print(f\"Successfully written vocabulary itos in {WRITE_OUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ขั้นตอนวิธีของคริสโตไฟด์\\nขั้นตอนวิธีของคริสโตไฟด์ () ตั้งชื่อตาม นิคอส คริสโตฟิลด์ เป็นขั้นตอนวิธีในการแก้ปัญหาบางกลุ่มของปัญหาการเดินทางของพนักงานขาย ที่มีเส้นเชื่อมถ่วงน้ําหนักเป็นไปตามความไม่เสมอภาคของสามเหลี่ยม ซึ่งได้คําตอบที่มีอัตราส่วนการประมาณ เป็น 1.5 เท่าของคําตอบดีที่สุด\\nขั้นตอนที่ 1: สร้าง ต้นไม้ทอดข้ามที่น้อยที่สุด formula_6 จาก formula_2\\nขั้นตอนที่ 2: ให้ formula_8 เป็นเซตของจุดยอดที่มี ระดับขั้น เป็นจํานวนคี่ ใน formula_6 และหา การจับคู่สมบูรณ์ formula_10 ซึ่งมีน้ําหนักน้อยที่สุดใน กราฟบริบูรณ์ บนจุดยอดใน formula_8\\nขั้นตอนที่ 3: รวมเส้นเชื่อมของ formula_10 และ formula_6 เป็น มัลติกราฟ formula_14\\nขั้นตอนที่ 4: สร้างวงจรออยเลอร์ ใน formula_14\\nขั้นตอนที่ 5: สร้างวงจรแฮมิลตัน จากขึั้นตอนที่แล้วโดยข้ามจุดยอดที่เยี่ยมชมแล้วออกไป (\"shortcutting\")\\nผลลัพธ์ของขั้นตอนวิธีนี้มีค่าเป็น 1.5 เท่าของของคําตอบดีที่สุด\\nพิสูจน์ได้ดังนี้:\\nให้ formula_16 แทนเซตของเส้นเชื่อมของคําตอบดีสุดของปัญหาการเดินทางของพนักงานขาย สําหรับformula_2, เนื่องจากformula_18 เชื่อมต่อกันบริบูรณ์ จึงมีต้นไม้ทอดข'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Tokens\n",
    "> SentencePiece reserves vocabulary ids for special meta symbols, e.g., unknown symbol (<unk\\>), BOS (<s\\>), EOS (</s\\>) and padding (<pad\\>). Their actual ids are configured with command line flags. We can also define custom meta symbols to encode contextual information as virtual tokens. Examples include the language- indicators, <2ja> and <2de>, for multilingual models\n",
    "\n",
    "-- From SentencePiece Paper\n",
    "\n",
    "Bert uses the special tokens `[UNK] [CLS] [SEP] [PAD] [MASK]`\n",
    "\n",
    "- Unknown: `[UNK]` `<unk>`\n",
    "- Beginning of Sentence (BOS): `[CLS]` `<s>`\n",
    "- Ending of Sentence (EOS): `[SEP]` `</s>`\n",
    "- Padding: `[PAD]`  `<pad>`\n",
    "- Mask: `[MASK]` `<mask>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=list(map(str, ALL_FILES)), \n",
    "                vocab_size=30522, \n",
    "                min_frequency=2,\n",
    "                show_progress=True,\n",
    "                special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"],\n",
    "               )\n",
    "print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir all-data-wordpiece-30522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-79dc60f251b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# And finally save it somewhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ! mkdir all-data-bytebpe-30522\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all-data-wordpiece-30522\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./all-data-wordpiece-30522.tokenizer.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# And finally save it somewhere\n",
    "# ! mkdir all-data-bytebpe-30522\n",
    "tokenizer.save_model(\"all-data-wordpiece-30522\")\n",
    "tokenizer.save(\"./all-data-wordpiece-30522.tokenizer.json\", pretty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tokenize on Pantip Sample ใครเคยมีแฟนที่กินอาหารไม่ถูกปากกันแล้วรู้สึกเสียความสุขไปอย่างนึงบ้างมั้ยครับ\n",
    "https://pantip.com/topic/40006922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(u\"สวัสดีครับ ผมชื่อไนท์ ตอนนี้ก็เป็นเวลาที่ผมต้องไปโรงเรียนแล้ว  นี่คือการเว้นวรรคสองทีครับ  จะได้ออกเป็นสอง Spaces\")\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)\n",
    "print(list(map(lambda x : tokenizer.decode([x]), encoded.ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(u\"Hello Thisis a test in English. How is this algorithm learning?? I dunno as well.\")\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tokenize on Pantip Sample ใครเคยมีแฟนที่กินอาหารไม่ถูกปากกันแล้วรู้สึกเสียความสุขไปอย่างนึงบ้างมั้ยครับ\n",
    "https://pantip.com/topic/40006922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"ใครเคยมีแฟนที่กินอาหารไม่ถูกปากกันแล้วรู้สึกเสียความสุขไปอย่างนึงบ้างมั้ยครับ  ก่อนอื่นผมต้องบอกก่อนเลยว่าคนเราจะเลือกกินอาหารแบบไหนชอบแบบไหนเป็นเรื่องของความชอบส่วนตัวนะครับทุกคนมีสิทธิในการเลือกของที่ชอบและไม่ชอบอยู่แล้ว แต่ผมรู้สึกว่าตอนนี้ผมกำลังประสบปัญหาที่ดูเหมือนจะเล็กแต่กลายเป็นว่ามันค่อนข้างใหญ่ ผมคบกับแฟนมา6ปีแล้วครับ ผมเป็นคนชอบกินอาหารญี่ปุ่นและปลาดิบแต่แฟนผมไม่กินปลาดิบเลย ผมอยากกินบุฟเฟ่เนื้อแต่แฟนผมก็ไม่กินเนื้อ เราเลยไม่ได้เข้าทานร้านบุฟเฟ่เนื้อและบุฟเฟ่อาหารญี่ปุ่นกันเพราะรู้สึกลัวแฟนผมทานไม่คุ้ม และเรื่องใหญ่เลยคือผมเป็นคนชอบทานอาหารรสจัดและรสเผ็ดมาก แต่แฟนผมทานเผ็ดไม่ได้เลยเวลาเราไปกินส้มตำกันก็จะสั่ง ส้มตำไม่ใส่พริก ต้มแซ่บไม่ใส่พริก ลาบไม่ใส่พริก ร้านกับข้าวอื่นๆก็เช่นกันแฟนผมจะไม่ชอบกินผักไม่ค่อยสั่งกับข้าวที่เป็นผักแล้วผมชอบผักบุ้งทอดกรอบ เห็ดหอมสดทอดมาก แต่ก็ไม่ได้สั่งเพราะว่าเธอไม่กินถึงเค้าจะบอกให้สั่งเลยๆก็เถอะแต่ผมก็ยังเกรงใจเธออยู่ดีอ่ะครับ ผมรู้สึกกินอาหารไม่มีความสุขเลยชีวิตผมขาดรสเผ็ดไปเหมือนจะขาดใจเหมือนมันทำให้ขาดความสุขไปอย่างนึงเลยอ่ะครับ ยิ่งถ้าเราแต่งงานกันแล้วผมก็อาจจะต้องมีปัญหาเรื่องนี้มากขึ้น พอผมเห็นคู่ที่ชอบทานอาหารเหมือนๆกันเห็นเค้ากินอาหารกันอย่างมีความสุขแล้วผมรู้สึกอิจฉามากๆเลย มีใครเคยมีปัญหาแบบผมมั้ยครับแล้วจะแก้ปัญหานี้ยังไงดีครับ\"\n",
    "encoded = tokenizer.encode(text)\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)\n",
    "print(list(map(lambda x : tokenizer.decode([x]), encoded.ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tokenize on Pantip Sample อาการแบบนี้คือไรกัน?\n",
    "https://pantip.com/topic/40009518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"อาการแบบนี้คือไรกัน?  เขาคุยกับเรามา 5-6 เดือน เราตามจีบเขานะคะ ก็คุยกันมา ในระยะเวลาเขาบอกว่า ถ้าเราลด นน เพื่อเขาได้ เขาจะยอมเป็นแฟนเรา ตรรกะโง่มากนะคะ แต่ถามว่าทำมั้ย ทำค่ะ พอไปรู้ว่าเขาคุยกับเพื่อน เพื่อนเขาถามว่า รู้สึกยังไงกับเรา เขาตอบเพื่อนว่า เขาว่าเขาควรอยู่คนเดียว ยังไม่พร้อมจะรักใคร จนตอนนี้เราเริ่มรู้สึกว่า ทำไมเราต้องทำขนาดนั้น ถ้าเขาจะรัก รักที่เป็นตัวเราไม่ได้หรอ หลังๆเลยเริ่มสนใจเขาน้อยลง แต่ยังคุยกันเหมือนเดิม เราลองแกล้งเงียบไป ไม่ทักไปครึ่งวัน ปกติเราจะมีการมอนิ่งกันตอนเช้าค่ะ พอเราไม่ทักไป เขาทำงานเสร็จ ถึงเวลาพักของเขา เขาก็ทักมาว่า กินข้าวกัน เราก็ยิ่ง งง ก็คิดว่า เขาอาจจะชินหับการคุยกับเราทุกวันเฉยๆ นี่เลยไม่ได้สนใจในส่วนนั้น เราก็ตอบตามปกติ จนเมื่อคืนมีคนมาทักเราจีบเรา จะไปส่งเราที่บ้าน เราก็เลยเล่าให้เขาฟังว่า ให้ไลน์ไป ให้เขาไปส่งอยู่แต่ไมไ่ด้นั่งรถคันเดียวกัน เราก็ขับของเรา คนที่มาจีบเราเขาก็ขับคันของเขาแค่มาส่งเฉยๆ พอเช้ามาเขาทักมามอนิ่ง ก็ถามเราเรื่องเมื่อคืน เราทำงานที่กลับดึกมากๆไม่ได้ทักไปบอกเขาไว้ว่า ถึงบ้านแล้วนะ เงียบไปทั้งคืนเลย เขาก็ถามเรื่องเมื่อคืนว่า หนุ่มไปส่งที่บ้านเป็นไงบ้าง ถามแต่เรื่องของผู้ชายที่มาจีบเราทั้งวัน จนเราเปลี่ยนเรื่องก็ยังกลับมาถามอีกรอบ ไออาการแบบนี้คืออะไรคะ ? ไหนเขาบอกอยากอยู่คนเดียว แต่พอเรามีคนเข้ามา ทำไมเขาถึงมีอาการแบบนี้ มาถามแบบนี้ซ้ำๆ คืออะไรกัน เราไม่อยากคิดอะไรไปเอง ใครพอจะตอบได้บ้างคะ ว่า ไอแบบนี้มันคืออะไร รู้สึกอะไรอยู่\"\n",
    "encoded = tokenizer.encode(text)\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)\n",
    "print(list(map(lambda x : tokenizer.decode([x]), encoded.ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we want to use it again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoding structure exposes multiple properties which are useful when working with transformers models\n",
    "\n",
    "- normalized_str: The input string after normalization (lower-casing, unicode, stripping, etc.)\n",
    "- original_str: The input string as it was provided\n",
    "- tokens: The generated tokens with their string representation\n",
    "- input_ids: The generated tokens with their integer representation\n",
    "- attention_mask: If your input has been padded by the tokenizer, then this would be a vector of 1 for any non padded token and 0 for padded ones.\n",
    "- special_token_mask: If your input contains special tokens such as [CLS], [SEP], [MASK], [PAD], then this would be a vector with 1 in places where a special token has been added.\n",
    "- type_ids: If your input was made of multiple \"parts\" such as (question, context), then this would be a vector with for each token the segment it belongs to.\n",
    "- overflowing: If your input has been truncated into multiple subparts because of a length limit (for BERT for example the sequence length is limited to 512), this will contain all the remaining overflowing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1125, 275, 566, 278, 333, 275, 282, 16456, 327, 2565, 368, 317, 830, 340, 274, 301, 334, 301, 13155, 302, 3555, 271, 1303, 1468, 278, 6036, 271, 279, 225, 407, 302, 283, 296, 1273, 444, 271, 19117, 785, 14369, 278, 333, 275, 282, 225, 1986, 271, 12879, 301, 11970, 1474, 793, 1050]\n",
      "['Ġà¸ªà¸§', 'à¸±', 'à¸ªà¸Ķ', 'à¸µ', 'à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġà¸ľà¸¡à¸Ĭ', 'à¸·à¹Ī', 'à¸Ńà¹Ħ', 'à¸Ļà¸Ĺ', 'à¹Į', 'Ġà¸ķà¸Ńà¸Ļà¸Ļ', 'à¸µà¹ī', 'à¸ģ', 'à¹ĩ', 'à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¹Ģà¸§à¸¥à¸²à¸Ĺ', 'à¸µà¹Ī', 'à¸ľà¸¡à¸ķ', 'à¹ī', 'à¸Ńà¸ĩà¹Ħà¸Ľ', 'à¹Ĥà¸£à¸ĩà¹Ģà¸£', 'à¸µ', 'à¸¢à¸Ļà¹ģà¸¥', 'à¹ī', 'à¸§', 'Ġ', 'Ġà¸Ļ', 'à¸µà¹Ī', 'à¸Ħ', 'à¸·', 'à¸Ńà¸ģà¸²à¸£', 'à¹Ģà¸§', 'à¹ī', 'à¸Ļà¸§à¸£', 'à¸£à¸Ħ', 'à¸ªà¸Ńà¸ĩà¸Ĺ', 'à¸µ', 'à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġ', 'Ġà¸Īà¸°à¹Ħà¸Ķ', 'à¹ī', 'à¸Ńà¸Ńà¸ģà¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¸ªà¸Ńà¸ĩ', 'Ġsp', 'ac', 'es']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"./all-data-wordpiece-30522.tokenizer.json\")\n",
    "encoded =  tokenizer.encode(u\"สวัสดีครับ ผมชื่อไนท์ ตอนนี้ก็เป็นเวลาที่ผมต้องไปโรงเรียนแล้ว  นี่คือการเว้นวรรคสองทีครับ  จะได้ออกเป็นสอง Spaces\")\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
