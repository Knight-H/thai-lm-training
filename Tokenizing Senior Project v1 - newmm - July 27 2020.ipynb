{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 subword algorithms help to improve your NLP model performance\n",
    "- Byte Pair Encoding (BPE)\n",
    "- WordPiece\n",
    "- Unigram Language Model\n",
    "- SentencePiece  \n",
    "\n",
    "Subword balances vocabulary size and footprint. Extreme case is we can only use 26 token (i.e. character) to present all English word. 16k or 32k subwords are recommended vocabulary size to have a good result.\n",
    "\n",
    "Many Asian language word cannot be separated by space. Therefore, the initial vocabulary is larger than English a lot. You may need to prepare over 10k initial word to kick start the word segmentation. From Schuster and Nakajima research, they propose to use 22k word and 11k word for Japanese and Korean respectively.  \n",
    "https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform subword tokenization, BPE is slightly modified in its implementation such that the frequently occurring subword pairs are merged together instead of being replaced by another byte to enable compression. This would basically lead the rare word athazagoraphobia to be split up into more frequent subwords such as ['▁ath', 'az', 'agor', 'aphobia'].\n",
    "https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers: How machines read - 28 JANUARY 2020\n",
    "Recommended read on tokenization  \n",
    "\n",
    "- **BPE**: Just uses the frequency of occurrences to identify the best match at every iteration until it reaches the predefined vocabulary size.\n",
    "- **WordPiece**: Similar to BPE and uses frequency occurrences to identify potential merges but makes the final decision based on the likelihood of the merged token\n",
    "- **Unigram**: A fully probabilistic model which does not use frequency occurrences. Instead, it trains a LM using a probabilistic model, removing the token which improves the overall likelihood the least and then starting over until it reaches the final token limit.\n",
    "- **SentencePiece** basically tries to bring all the subword tokenization tools and techniques under one banner. _\" SentencePiece is a re-implementation of sub-word units, an effective way to alleviate the open vocabulary problems in neural machine translation. SentencePiece supports two segmentation algorithms, byte-pair-encoding (BPE) [Sennrich et al.] and unigram language model [Kudo.]. \"_ (BPE and Unigram are reimplemented with improvements).\n",
    "    - __All other models assume input is already tokenized__: BPE and Unigram are great models but they share one big disadvantage- they both need to have their input already tokenized. BPE needs to have the input tokenized so that every character (including word-boundary characters) are tokenized. Only then can BPE count frequencies and start to merge tokens. Usually this is done by simply doing word level tokenization but, as we discussed earlier, this is a problem with tokenization since not all languages are space segmented. Similarly, the unigram model needs to have its input tokenized before it can start discarding tokens based on their probability distribution. SentencePiece deals with this by simply taking in an input in raw text and then doing everything (which we will discuss below) needed on that input to perform subword tokenization.\n",
    "    - __Encode everything as unicode ...__: SentencePiece first converts all the input into unicode characters. This means it doesn’t have to worry about different languages or characters or symbols. If it uses unicode it can just treat all input in the same way, which allows it to be language agnostic\n",
    "    - __… including  the spaces__: To get around the word segmenting issues, SentencePiece simply encodes spaces as a unicode symbol. Specifically it encodes it as unicode value U+2581 (underscore ‘_’ to those of us who don’t speak unicode). This helps with the language agnostic issues and the decoding issue. Since spaces are unicode encoded then they can be easily reversed or decoded and treated (i.e learned) like a normal language character. It sounds like a simple approach and I guess it is, but the best ideas tend to seem that way in the end\n",
    "\n",
    "\n",
    "https://blog.floydhub.com/tokenization-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Huggingface `tokenizers`__ : \n",
    "Provided Tokenizers\n",
    "- CharBPETokenizer: The original BPE\n",
    "- ByteLevelBPETokenizer: The byte level version of the BPE\n",
    "- SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
    "- BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece  \n",
    " \n",
    "We designed the library so that it provides all the required blocks to create end-to-end tokenizers in an interchangeable way. In that sense, we provide\n",
    "these various components: \n",
    "\n",
    "- **Normalizer**: Executes all the initial transformations over the initial input string. For example when you need to\n",
    "lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer. \n",
    "- **PreTokenizer**: In charge of splitting the initial input string. That's the component that decides where and how to\n",
    "pre-segment the origin string. The simplest example would be like we saw before, to simply split on spaces.\n",
    "- **Model**: Handles all the sub-token discovery and generation, this part is trainable and really dependant\n",
    " of your input data.\n",
    "- **Post-Processor**: Provides advanced construction features to be compatible with some of the Transformers-based SoTA\n",
    "models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens.\n",
    "- **Decoder**: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according\n",
    "to the `PreTokenizer` we used previously.\n",
    "- **Trainer**: Provides training capabilities to each model. \n",
    "\n",
    "Notebook for Tokenizers: https://github.com/huggingface/transformers/blob/master/notebooks/01-training-tokenizers.ipynb  \n",
    "Github Link for Python Binding: https://github.com/huggingface/tokenizers/tree/master/bindings/python\n",
    "\n",
    "Implementation: https://github.com/huggingface/tokenizers/tree/master/bindings/python/tokenizers/implementations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAD_2.txt\n",
      "/datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAD_3.txt\n",
      "/datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAC_0.txt\n",
      "/datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAA_2.txt\n",
      "/datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAA_0.txt\n",
      "/datadisk/raw_data_extraction/classification_dataset/dailynews_0.txt\n",
      "/datadisk/raw_data_extraction/classification_dataset/prachachat_0.txt\n",
      "/datadisk/raw_data_extraction/classification_dataset/thaipbs_0.txt\n",
      "/datadisk/raw_data_extraction/classification_dataset/pptv36_0.txt\n",
      "/datadisk/raw_data_extraction/classification_dataset/siamrath_0.txt\n",
      "/datadisk/raw_data_extraction/another_website/pantip_430.txt\n",
      "/datadisk/raw_data_extraction/another_website/new18_2.txt\n",
      "/datadisk/raw_data_extraction/another_website/pantip_237.txt\n",
      "/datadisk/raw_data_extraction/another_website/pantip_217.txt\n",
      "/datadisk/raw_data_extraction/another_website/instagram_23.txt\n",
      "/datadisk/raw_data_extraction/data_lm/Pantipdata_train.csv_111.txt\n",
      "/datadisk/raw_data_extraction/data_lm/Pantipdata_train.csv_330.txt\n",
      "/datadisk/raw_data_extraction/data_lm/Pantipdata_train.csv_269.txt\n",
      "/datadisk/raw_data_extraction/data_lm/Pantipdata_train.csv_90.txt\n",
      "/datadisk/raw_data_extraction/data_lm/Pantipdata_train.csv_212.txt\n",
      "/datadisk/raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_1.txt\n",
      "/datadisk/raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_4.txt\n",
      "/datadisk/raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_0.txt\n",
      "/datadisk/raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_2.txt\n",
      "/datadisk/raw_data_extraction/social_listening/SocialListeningpantip_post_data.csv_3.txt\n",
      "\n",
      "I have a total of 1410 files!\n",
      "Amounts to a total of 41289.13 MB\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(\"/datadisk\")\n",
    "\n",
    "# DATA_RAW_PATH = DATA_PATH/\"raw\"\n",
    "DATA_RAW_EXTRACTED_PATH = DATA_PATH/\"raw_data_extraction\"\n",
    "\n",
    "# 1. The data from thwiki\n",
    "THWIKI_FOLDER = Path(\"thwiki-20200601-extracted\")\n",
    "WIKI_FILES = list((DATA_RAW_EXTRACTED_PATH/THWIKI_FOLDER).glob(\"Wiki*.txt\"))\n",
    "list(map(print , WIKI_FILES[:5]))\n",
    "\n",
    "\n",
    "# 2. The classification data from jung and ninja\n",
    "CLASSIFICATION_JUNG_NINJA_FOLDER = Path(\"classification_dataset\")\n",
    "CLASSIFICATION_FILES = list((DATA_RAW_EXTRACTED_PATH/CLASSIFICATION_JUNG_NINJA_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , CLASSIFICATION_FILES[:5]))\n",
    "\n",
    "# 3. The Data from p'Moo Crawlers\n",
    "ANOTHER_WEBSITE_MOO_FOLDER = Path(\"another_website\")\n",
    "ANOTHER_WEBSITE_FILES = list((DATA_RAW_EXTRACTED_PATH/ANOTHER_WEBSITE_MOO_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , ANOTHER_WEBSITE_FILES[:5]))\n",
    "\n",
    "\n",
    "# 4. Senior Project Files\n",
    "SENIOR_PROJ_FOLDER = Path(\"data_lm\")\n",
    "SENIOR_PROJ_FILES = list((DATA_RAW_EXTRACTED_PATH/SENIOR_PROJ_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , SENIOR_PROJ_FILES[:5]))\n",
    "\n",
    "# 5. Guru Crawler Files\n",
    "GURU_CRAWLER_FOLDER = Path(\"social_listening\")\n",
    "GURU_CRAWLER_FILES = list((DATA_RAW_EXTRACTED_PATH/GURU_CRAWLER_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , GURU_CRAWLER_FILES[:5]))\n",
    "\n",
    "ALL_FILES = WIKI_FILES + CLASSIFICATION_FILES + ANOTHER_WEBSITE_FILES + SENIOR_PROJ_FILES + GURU_CRAWLER_FILES\n",
    "print(f\"\\nI have a total of {len(ALL_FILES)} files!\")\n",
    "\n",
    "\n",
    "# Output is in bytes - helper from Pathlib Path https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python\n",
    "def getStat(prev_value, cur_value):\n",
    "    if isinstance(prev_value, int):\n",
    "        return prev_value + cur_value.stat().st_size\n",
    "    return prev_value.stat().st_size + cur_value.stat().st_size\n",
    "\n",
    "from functools import reduce\n",
    "print(f\"Amounts to a total of {reduce(getStat, ALL_FILES)/1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[file for file in list(map(str, ALL_FILES)) if os.path.isdir(file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chardet\n",
    "# for filename in list(map(str, ALL_FILES))[::-1]:\n",
    "#     with open(filename, 'rb') as f:\n",
    "#         content_bytes = f.read()\n",
    "#     detected = chardet.detect(content_bytes)\n",
    "#     encoding = detected['encoding']\n",
    "#     print(f\"{filename}: detected as {encoding}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/datadisk/raw_data_extraction/classification_dataset/cached_lm_RobertaTokenizer_510_siamrath_0.txt.lock'),\n",
       " PosixPath('/datadisk/raw_data_extraction/classification_dataset/dailynews_0.txt'),\n",
       " PosixPath('/datadisk/raw_data_extraction/classification_dataset/prachachat_0.txt'),\n",
       " PosixPath('/datadisk/raw_data_extraction/classification_dataset/thaipbs_0.txt'),\n",
       " PosixPath('/datadisk/raw_data_extraction/classification_dataset/pptv36_0.txt'),\n",
       " PosixPath('/datadisk/raw_data_extraction/classification_dataset/siamrath_0.txt'),\n",
       " PosixPath('/datadisk/raw_data_extraction/classification_dataset/naewna_0.txt'),\n",
       " PosixPath('/datadisk/raw_data_extraction/classification_dataset/springnews_0.txt'),\n",
       " PosixPath('/datadisk/raw_data_extraction/classification_dataset/cached_lm_RobertaTokenizer_510_siamrath_0.txt'),\n",
       " PosixPath('/datadisk/raw_data_extraction/classification_dataset/prbangkok_0.txt')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list((DATA_RAW_EXTRACTED_PATH/CLASSIFICATION_JUNG_NINJA_FOLDER).glob(\"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train `SeniorProjectTokenizer` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: wiki_lm_lstm\n",
      "- Downloading: wiki_lm_lstm 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1050919089/1050919089 [01:30<00:00, 11648911.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: wiki_itos_lstm\n",
      "- Downloading: wiki_itos_lstm 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1530484/1530484 [00:01<00:00, 920075.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from senior_project_util import ThaiTokenizer, pre_rules_th, post_rules_th\n",
    "from fastai.text.transform import BaseTokenizer, Tokenizer\n",
    "from fastai.text.data import TokenizeProcessor, NumericalizeProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ม่าย', 'เอา', 'เปง', 'ไง', 'บ้าง', 'น่ารัก', 'จุงเบย']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='ม่ายเอาเปงไงบ้างน่ารักจุงเบย'\n",
    "pyThai_tt = ThaiTokenizer()\n",
    "a = pyThai_tt.tokenizer(text)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ไม่', 'เอา', 'เปง', 'ไง', 'บ้าง', 'น่ารัก', 'จังเลย']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = Tokenizer(tok_func = ThaiTokenizer, lang = 'th', pre_rules = pre_rules_th, post_rules=post_rules_th, n_cpus=60)\n",
    "tt.process_all([text[:1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_processor = TokenizeProcessor(tokenizer=tt, chunksize=300000, mark_fields=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxbos', ' ', 'เวลา', 'xxnum', ' ', 'น.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_processor.process_one(\"เวลา 12.00น.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_tokenized(file_path):\n",
    "    print(f\"I AM DOING {file_path}\")\n",
    "    directory, filename = os.path.split(file_path)\n",
    "#     cached_features_file = os.path.join(\n",
    "#         self.cached_directory, f\"cached_lm_{tokenizer.__class__.__name__}_{str(self.block_size)}_{filename}\",\n",
    "#     )\n",
    "\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    tokens = tokenizer_processor.process_one(text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1410 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAD_2.txt\n",
      "I AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAA_2.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAD_3.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAC_0.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAF_2.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAA_0.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAA_3.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAA_1.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAC_2.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAB_3.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAE_0.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAC_3.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAC_1.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAB_1.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAE_2.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAD_1.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAE_3.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAB_2.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAD_0.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAF_0.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAF_1.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAB_0.txtI AM DOING /datadisk/raw_data_extraction/classification_dataset/dailynews_0.txtI AM DOING /datadisk/raw_data_extraction/classification_dataset/prachachat_0.txtI AM DOING /datadisk/raw_data_extraction/classification_dataset/siamrath_0.txtI AM DOING /datadisk/raw_data_extraction/thwiki-20200601-extracted/WikiAE_1.txtI AM DOING /datadisk/raw_data_extraction/classification_dataset/pptv36_0.txtI AM DOING /datadisk/raw_data_extraction/classification_dataset/naewna_0.txtI AM DOING /datadisk/raw_data_extraction/classification_dataset/thaipbs_0.txtI AM DOING /datadisk/raw_data_extraction/classification_dataset/cached_lm_RobertaTokenizer_510_siamrath_0.txtI AM DOING /datadisk/raw_data_extraction/classification_dataset/springnews_0.txtI AM DOING /datadisk/raw_data_extraction/another_website/pantip_430.txtI AM DOING /datadisk/raw_data_extraction/another_website/new18_2.txtI AM DOING /datadisk/raw_data_extraction/classification_dataset/prbangkok_0.txtI AM DOING /datadisk/raw_data_extraction/another_website/pantip_237.txtI AM DOING /datadisk/raw_data_extraction/another_website/pantip_217.txtI AM DOING /datadisk/raw_data_extraction/another_website/khaosod_11.txtI AM DOING /datadisk/raw_data_extraction/another_website/instagram_23.txtI AM DOING /datadisk/raw_data_extraction/another_website/pantip_307.txtI AM DOING /datadisk/raw_data_extraction/another_website/pantip_392.txtI AM DOING /datadisk/raw_data_extraction/another_website/matichon_1.txtI AM DOING /datadisk/raw_data_extraction/another_website/pantip_184.txtI AM DOING /datadisk/raw_data_extraction/another_website/mgronline_4.txtI AM DOING /datadisk/raw_data_extraction/another_website/pantip_527.txtI AM DOING /datadisk/raw_data_extraction/another_website/khaosod_6.txtI AM DOING /datadisk/raw_data_extraction/another_website/pantip_7.txtI AM DOING /datadisk/raw_data_extraction/another_website/thairath_21.txtI AM DOING /datadisk/raw_data_extraction/another_website/pantip_558.txtI AM DOING /datadisk/raw_data_extraction/another_website/amarintv_3.txtI AM DOING /datadisk/raw_data_extraction/another_website/pantip_451.txtI AM DOING /datadisk/raw_data_extraction/another_website/pantip_324.txtI AM DOING /datadisk/raw_data_extraction/another_website/mgronline_3.txtI AM DOING /datadisk/raw_data_extraction/another_website/pantip_22.txtI AM DOING /datadisk/raw_data_extraction/another_website/pantip_504.txt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I AM DOING /datadisk/raw_data_extraction/another_website/twitter_24.txtI AM DOING /datadisk/raw_data_extraction/another_website/pantip_306.txtI AM DOING /datadisk/raw_data_extraction/another_website/mgronline_15.txtI AM DOING /datadisk/raw_data_extraction/another_website/pantip_397.txt\n",
      "\n",
      "I AM DOING /datadisk/raw_data_extraction/another_website/pantip_171.txt\n",
      "I AM DOING /datadisk/raw_data_extraction/another_website/pantip_222.txt\n",
      "\n",
      "\n",
      "I AM DOING /datadisk/raw_data_extraction/another_website/pantip_275.txt\n",
      "I AM DOING /datadisk/raw_data_extraction/another_website/pantip_32.txt\n",
      "I AM DOING /datadisk/raw_data_extraction/another_website/pantip_515.txt\n",
      "I AM DOING /datadisk/raw_data_extraction/another_website/pantip_50.txt\n",
      "I AM DOING /datadisk/raw_data_extraction/another_website/bangkokbiznews_2.txt\n",
      "I AM DOING /datadisk/raw_data_extraction/another_website/pantip_539.txt\n",
      "I AM DOING /datadisk/raw_data_extraction/another_website/postjung_12.txt\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "\n",
    "num_processes = 60\n",
    "sample_path = ALL_FILES\n",
    "\n",
    "with Pool(processes=num_processes) as p:\n",
    "    tokens = list(tqdm.tqdm(p.imap(load_data_tokenized, sample_path), total=len(sample_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_tokenized(ALL_FILES[0])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Vocabs\n",
    "copied from Fastai [`Vocab.create()`](https://github.com/fastai/fastai/blob/d418294f0f17382f7a33bb72b93f5055a7768b14/fastai/text/transform.py#L149)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 50000\n",
    "min_freq = 3\n",
    "\n",
    "BOS,EOS,FLD,UNK,PAD = 'xxbos','xxeos','xxfld','xxunk','xxpad'\n",
    "TK_REP,TK_WREP, TK_NUM, TK_LAUGH = 'xxrep','xxwrep', 'xxnum', 'xxlaugh'\n",
    "text_spec_tok = [UNK,PAD,BOS,EOS,FLD,TK_REP,TK_WREP, TK_NUM, TK_LAUGH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Counter(p for o in tokens for p in o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print({i:v for i,v in freq.most_common(20)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "itos = [o for o,c in freq.most_common(max_vocab) if c >= min_freq]\n",
    "print(itos[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for o in reversed(text_spec_tok):\n",
    "    if o in itos: itos.remove(o)\n",
    "    itos.insert(0, o)\n",
    "itos = itos[:max_vocab]\n",
    "if len(itos) < max_vocab: #Make sure vocab size is a multiple of 8 for fast mixed precision training\n",
    "    while len(itos)%8 !=0: itos.append('xxfake')\n",
    "print(\"ITOS\", itos[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITE_OUT_FILE = \"senior_project_vocab.txt\"\n",
    "with open(WRITE_OUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(itos)\n",
    "print(f\"Successfully written vocabulary itos in {WRITE_OUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ขั้นตอนวิธีของคริสโตไฟด์\\nขั้นตอนวิธีของคริสโตไฟด์ () ตั้งชื่อตาม นิคอส คริสโตฟิลด์ เป็นขั้นตอนวิธีในการแก้ปัญหาบางกลุ่มของปัญหาการเดินทางของพนักงานขาย ที่มีเส้นเชื่อมถ่วงน้ําหนักเป็นไปตามความไม่เสมอภาคของสามเหลี่ยม ซึ่งได้คําตอบที่มีอัตราส่วนการประมาณ เป็น 1.5 เท่าของคําตอบดีที่สุด\\nขั้นตอนที่ 1: สร้าง ต้นไม้ทอดข้ามที่น้อยที่สุด formula_6 จาก formula_2\\nขั้นตอนที่ 2: ให้ formula_8 เป็นเซตของจุดยอดที่มี ระดับขั้น เป็นจํานวนคี่ ใน formula_6 และหา การจับคู่สมบูรณ์ formula_10 ซึ่งมีน้ําหนักน้อยที่สุดใน กราฟบริบูรณ์ บนจุดยอดใน formula_8\\nขั้นตอนที่ 3: รวมเส้นเชื่อมของ formula_10 และ formula_6 เป็น มัลติกราฟ formula_14\\nขั้นตอนที่ 4: สร้างวงจรออยเลอร์ ใน formula_14\\nขั้นตอนที่ 5: สร้างวงจรแฮมิลตัน จากขึั้นตอนที่แล้วโดยข้ามจุดยอดที่เยี่ยมชมแล้วออกไป (\"shortcutting\")\\nผลลัพธ์ของขั้นตอนวิธีนี้มีค่าเป็น 1.5 เท่าของของคําตอบดีที่สุด\\nพิสูจน์ได้ดังนี้:\\nให้ formula_16 แทนเซตของเส้นเชื่อมของคําตอบดีสุดของปัญหาการเดินทางของพนักงานขาย สําหรับformula_2, เนื่องจากformula_18 เชื่อมต่อกันบริบูรณ์ จึงมีต้นไม้ทอดข'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Tokens\n",
    "> SentencePiece reserves vocabulary ids for special meta symbols, e.g., unknown symbol (<unk\\>), BOS (<s\\>), EOS (</s\\>) and padding (<pad\\>). Their actual ids are configured with command line flags. We can also define custom meta symbols to encode contextual information as virtual tokens. Examples include the language- indicators, <2ja> and <2de>, for multilingual models\n",
    "\n",
    "-- From SentencePiece Paper\n",
    "\n",
    "Bert uses the special tokens `[UNK] [CLS] [SEP] [PAD] [MASK]`\n",
    "\n",
    "- Unknown: `[UNK]` `<unk>`\n",
    "- Beginning of Sentence (BOS): `[CLS]` `<s>`\n",
    "- Ending of Sentence (EOS): `[SEP]` `</s>`\n",
    "- Padding: `[PAD]`  `<pad>`\n",
    "- Mask: `[MASK]` `<mask>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=list(map(str, ALL_FILES)), \n",
    "                vocab_size=30522, \n",
    "                min_frequency=2,\n",
    "                show_progress=True,\n",
    "                special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"],\n",
    "               )\n",
    "print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir all-data-wordpiece-30522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-79dc60f251b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# And finally save it somewhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ! mkdir all-data-bytebpe-30522\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all-data-wordpiece-30522\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./all-data-wordpiece-30522.tokenizer.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# And finally save it somewhere\n",
    "# ! mkdir all-data-bytebpe-30522\n",
    "tokenizer.save_model(\"all-data-wordpiece-30522\")\n",
    "tokenizer.save(\"./all-data-wordpiece-30522.tokenizer.json\", pretty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tokenize on Pantip Sample ใครเคยมีแฟนที่กินอาหารไม่ถูกปากกันแล้วรู้สึกเสียความสุขไปอย่างนึงบ้างมั้ยครับ\n",
    "https://pantip.com/topic/40006922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(u\"สวัสดีครับ ผมชื่อไนท์ ตอนนี้ก็เป็นเวลาที่ผมต้องไปโรงเรียนแล้ว  นี่คือการเว้นวรรคสองทีครับ  จะได้ออกเป็นสอง Spaces\")\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)\n",
    "print(list(map(lambda x : tokenizer.decode([x]), encoded.ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(u\"Hello Thisis a test in English. How is this algorithm learning?? I dunno as well.\")\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tokenize on Pantip Sample ใครเคยมีแฟนที่กินอาหารไม่ถูกปากกันแล้วรู้สึกเสียความสุขไปอย่างนึงบ้างมั้ยครับ\n",
    "https://pantip.com/topic/40006922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"ใครเคยมีแฟนที่กินอาหารไม่ถูกปากกันแล้วรู้สึกเสียความสุขไปอย่างนึงบ้างมั้ยครับ  ก่อนอื่นผมต้องบอกก่อนเลยว่าคนเราจะเลือกกินอาหารแบบไหนชอบแบบไหนเป็นเรื่องของความชอบส่วนตัวนะครับทุกคนมีสิทธิในการเลือกของที่ชอบและไม่ชอบอยู่แล้ว แต่ผมรู้สึกว่าตอนนี้ผมกำลังประสบปัญหาที่ดูเหมือนจะเล็กแต่กลายเป็นว่ามันค่อนข้างใหญ่ ผมคบกับแฟนมา6ปีแล้วครับ ผมเป็นคนชอบกินอาหารญี่ปุ่นและปลาดิบแต่แฟนผมไม่กินปลาดิบเลย ผมอยากกินบุฟเฟ่เนื้อแต่แฟนผมก็ไม่กินเนื้อ เราเลยไม่ได้เข้าทานร้านบุฟเฟ่เนื้อและบุฟเฟ่อาหารญี่ปุ่นกันเพราะรู้สึกลัวแฟนผมทานไม่คุ้ม และเรื่องใหญ่เลยคือผมเป็นคนชอบทานอาหารรสจัดและรสเผ็ดมาก แต่แฟนผมทานเผ็ดไม่ได้เลยเวลาเราไปกินส้มตำกันก็จะสั่ง ส้มตำไม่ใส่พริก ต้มแซ่บไม่ใส่พริก ลาบไม่ใส่พริก ร้านกับข้าวอื่นๆก็เช่นกันแฟนผมจะไม่ชอบกินผักไม่ค่อยสั่งกับข้าวที่เป็นผักแล้วผมชอบผักบุ้งทอดกรอบ เห็ดหอมสดทอดมาก แต่ก็ไม่ได้สั่งเพราะว่าเธอไม่กินถึงเค้าจะบอกให้สั่งเลยๆก็เถอะแต่ผมก็ยังเกรงใจเธออยู่ดีอ่ะครับ ผมรู้สึกกินอาหารไม่มีความสุขเลยชีวิตผมขาดรสเผ็ดไปเหมือนจะขาดใจเหมือนมันทำให้ขาดความสุขไปอย่างนึงเลยอ่ะครับ ยิ่งถ้าเราแต่งงานกันแล้วผมก็อาจจะต้องมีปัญหาเรื่องนี้มากขึ้น พอผมเห็นคู่ที่ชอบทานอาหารเหมือนๆกันเห็นเค้ากินอาหารกันอย่างมีความสุขแล้วผมรู้สึกอิจฉามากๆเลย มีใครเคยมีปัญหาแบบผมมั้ยครับแล้วจะแก้ปัญหานี้ยังไงดีครับ\"\n",
    "encoded = tokenizer.encode(text)\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)\n",
    "print(list(map(lambda x : tokenizer.decode([x]), encoded.ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tokenize on Pantip Sample อาการแบบนี้คือไรกัน?\n",
    "https://pantip.com/topic/40009518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"อาการแบบนี้คือไรกัน?  เขาคุยกับเรามา 5-6 เดือน เราตามจีบเขานะคะ ก็คุยกันมา ในระยะเวลาเขาบอกว่า ถ้าเราลด นน เพื่อเขาได้ เขาจะยอมเป็นแฟนเรา ตรรกะโง่มากนะคะ แต่ถามว่าทำมั้ย ทำค่ะ พอไปรู้ว่าเขาคุยกับเพื่อน เพื่อนเขาถามว่า รู้สึกยังไงกับเรา เขาตอบเพื่อนว่า เขาว่าเขาควรอยู่คนเดียว ยังไม่พร้อมจะรักใคร จนตอนนี้เราเริ่มรู้สึกว่า ทำไมเราต้องทำขนาดนั้น ถ้าเขาจะรัก รักที่เป็นตัวเราไม่ได้หรอ หลังๆเลยเริ่มสนใจเขาน้อยลง แต่ยังคุยกันเหมือนเดิม เราลองแกล้งเงียบไป ไม่ทักไปครึ่งวัน ปกติเราจะมีการมอนิ่งกันตอนเช้าค่ะ พอเราไม่ทักไป เขาทำงานเสร็จ ถึงเวลาพักของเขา เขาก็ทักมาว่า กินข้าวกัน เราก็ยิ่ง งง ก็คิดว่า เขาอาจจะชินหับการคุยกับเราทุกวันเฉยๆ นี่เลยไม่ได้สนใจในส่วนนั้น เราก็ตอบตามปกติ จนเมื่อคืนมีคนมาทักเราจีบเรา จะไปส่งเราที่บ้าน เราก็เลยเล่าให้เขาฟังว่า ให้ไลน์ไป ให้เขาไปส่งอยู่แต่ไมไ่ด้นั่งรถคันเดียวกัน เราก็ขับของเรา คนที่มาจีบเราเขาก็ขับคันของเขาแค่มาส่งเฉยๆ พอเช้ามาเขาทักมามอนิ่ง ก็ถามเราเรื่องเมื่อคืน เราทำงานที่กลับดึกมากๆไม่ได้ทักไปบอกเขาไว้ว่า ถึงบ้านแล้วนะ เงียบไปทั้งคืนเลย เขาก็ถามเรื่องเมื่อคืนว่า หนุ่มไปส่งที่บ้านเป็นไงบ้าง ถามแต่เรื่องของผู้ชายที่มาจีบเราทั้งวัน จนเราเปลี่ยนเรื่องก็ยังกลับมาถามอีกรอบ ไออาการแบบนี้คืออะไรคะ ? ไหนเขาบอกอยากอยู่คนเดียว แต่พอเรามีคนเข้ามา ทำไมเขาถึงมีอาการแบบนี้ มาถามแบบนี้ซ้ำๆ คืออะไรกัน เราไม่อยากคิดอะไรไปเอง ใครพอจะตอบได้บ้างคะ ว่า ไอแบบนี้มันคืออะไร รู้สึกอะไรอยู่\"\n",
    "encoded = tokenizer.encode(text)\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)\n",
    "print(list(map(lambda x : tokenizer.decode([x]), encoded.ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we want to use it again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoding structure exposes multiple properties which are useful when working with transformers models\n",
    "\n",
    "- normalized_str: The input string after normalization (lower-casing, unicode, stripping, etc.)\n",
    "- original_str: The input string as it was provided\n",
    "- tokens: The generated tokens with their string representation\n",
    "- input_ids: The generated tokens with their integer representation\n",
    "- attention_mask: If your input has been padded by the tokenizer, then this would be a vector of 1 for any non padded token and 0 for padded ones.\n",
    "- special_token_mask: If your input contains special tokens such as [CLS], [SEP], [MASK], [PAD], then this would be a vector with 1 in places where a special token has been added.\n",
    "- type_ids: If your input was made of multiple \"parts\" such as (question, context), then this would be a vector with for each token the segment it belongs to.\n",
    "- overflowing: If your input has been truncated into multiple subparts because of a length limit (for BERT for example the sequence length is limited to 512), this will contain all the remaining overflowing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1125, 275, 566, 278, 333, 275, 282, 16456, 327, 2565, 368, 317, 830, 340, 274, 301, 334, 301, 13155, 302, 3555, 271, 1303, 1468, 278, 6036, 271, 279, 225, 407, 302, 283, 296, 1273, 444, 271, 19117, 785, 14369, 278, 333, 275, 282, 225, 1986, 271, 12879, 301, 11970, 1474, 793, 1050]\n",
      "['Ġà¸ªà¸§', 'à¸±', 'à¸ªà¸Ķ', 'à¸µ', 'à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġà¸ľà¸¡à¸Ĭ', 'à¸·à¹Ī', 'à¸Ńà¹Ħ', 'à¸Ļà¸Ĺ', 'à¹Į', 'Ġà¸ķà¸Ńà¸Ļà¸Ļ', 'à¸µà¹ī', 'à¸ģ', 'à¹ĩ', 'à¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¹Ģà¸§à¸¥à¸²à¸Ĺ', 'à¸µà¹Ī', 'à¸ľà¸¡à¸ķ', 'à¹ī', 'à¸Ńà¸ĩà¹Ħà¸Ľ', 'à¹Ĥà¸£à¸ĩà¹Ģà¸£', 'à¸µ', 'à¸¢à¸Ļà¹ģà¸¥', 'à¹ī', 'à¸§', 'Ġ', 'Ġà¸Ļ', 'à¸µà¹Ī', 'à¸Ħ', 'à¸·', 'à¸Ńà¸ģà¸²à¸£', 'à¹Ģà¸§', 'à¹ī', 'à¸Ļà¸§à¸£', 'à¸£à¸Ħ', 'à¸ªà¸Ńà¸ĩà¸Ĺ', 'à¸µ', 'à¸Ħà¸£', 'à¸±', 'à¸ļ', 'Ġ', 'Ġà¸Īà¸°à¹Ħà¸Ķ', 'à¹ī', 'à¸Ńà¸Ńà¸ģà¹Ģà¸Ľ', 'à¹ĩ', 'à¸Ļà¸ªà¸Ńà¸ĩ', 'Ġsp', 'ac', 'es']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"./all-data-wordpiece-30522.tokenizer.json\")\n",
    "encoded =  tokenizer.encode(u\"สวัสดีครับ ผมชื่อไนท์ ตอนนี้ก็เป็นเวลาที่ผมต้องไปโรงเรียนแล้ว  นี่คือการเว้นวรรคสองทีครับ  จะได้ออกเป็นสอง Spaces\")\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
