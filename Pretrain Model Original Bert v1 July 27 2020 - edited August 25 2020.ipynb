{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How to train a language model__\tNotebook to Highlight all the steps to effectively train Transformer model on custom data\n",
    "https://github.com/huggingface/transformers/tree/master/notebooks\n",
    "\n",
    "https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb\n",
    "\n",
    "__Language Modeling__\n",
    "https://github.com/huggingface/transformers/tree/master/examples/language-modeling\n",
    "https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import CharBPETokenizer, Tokenizer, ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "# from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "import random\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast , AutoTokenizer,RobertaTokenizerFast, RobertaTokenizer\n",
    "from filelock import FileLock\n",
    "import logging\n",
    "import time\n",
    "import tqdm\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "# from concurrent.futures import ProcessPoolExecutor as Pool\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:1\")\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 25 09:12:16 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:21:01.0 Off |                    0 |\n",
      "| N/A   31C    P0    52W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:21:02.0 Off |                    0 |\n",
      "| N/A   33C    P0    54W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  Off  | 00000000:21:03.0 Off |                    0 |\n",
      "| N/A   30C    P0    52W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  Off  | 00000000:21:04.0 Off |                    0 |\n",
      "| N/A   29C    P0    52W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  Off  | 00000000:41:01.0 Off |                    0 |\n",
      "| N/A   32C    P0    55W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  Off  | 00000000:41:02.0 Off |                    0 |\n",
      "| N/A   29C    P0    49W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  Off  | 00000000:41:03.0 Off |                    0 |\n",
      "| N/A   30C    P0    55W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  Off  | 00000000:41:04.0 Off |                    0 |\n",
      "| N/A   31C    P0    55W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Check that PyTorch sees it\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datadisk/data/raw_data_extraction_v2/thwiki-20200601-extracted/WikiAA_0.txt\n",
      "/datadisk/data/raw_data_extraction_v2/thwiki-20200601-extracted/WikiAB_2.txt\n",
      "/datadisk/data/raw_data_extraction_v2/thwiki-20200601-extracted/WikiAD_2.txt\n",
      "/datadisk/data/raw_data_extraction_v2/thwiki-20200601-extracted/WikiAD_0.txt\n",
      "/datadisk/data/raw_data_extraction_v2/thwiki-20200601-extracted/WikiAE_0.txt\n",
      "thwiki-20200601-extracted Amounts to a total of 566.79 MB\n",
      "/datadisk/data/raw_data_extraction_v2/classification_dataset/thaipbs_0.txt\n",
      "/datadisk/data/raw_data_extraction_v2/classification_dataset/naewna_0.txt\n",
      "/datadisk/data/raw_data_extraction_v2/classification_dataset/dailynews_0.txt\n",
      "/datadisk/data/raw_data_extraction_v2/classification_dataset/prbangkok_0.txt\n",
      "/datadisk/data/raw_data_extraction_v2/classification_dataset/pptv36_0.txt\n",
      "classification_dataset Amounts to a total of 50.79 MB\n",
      "/datadisk/data/raw_data_extraction_v2/another_website/pantip_275.txt\n",
      "/datadisk/data/raw_data_extraction_v2/another_website/praew_2.txt\n",
      "/datadisk/data/raw_data_extraction_v2/another_website/facebook_3.txt\n",
      "/datadisk/data/raw_data_extraction_v2/another_website/facebook_27.txt\n",
      "/datadisk/data/raw_data_extraction_v2/another_website/ch7hd_0.txt\n",
      "another_website Amounts to a total of 29552.82 MB\n",
      "/datadisk/data/raw_data_extraction_v2/data_lm/Pantipdata_train.csv_311.txt\n",
      "/datadisk/data/raw_data_extraction_v2/data_lm/Pantipdata_train.csv_100.txt\n",
      "/datadisk/data/raw_data_extraction_v2/data_lm/Pantipdata_train.csv_271.txt\n",
      "/datadisk/data/raw_data_extraction_v2/data_lm/Pantipdata_train.csv_87.txt\n",
      "/datadisk/data/raw_data_extraction_v2/data_lm/Pantipdata_train.csv_333.txt\n",
      "Senior Project Amounts to a total of 10942.78 MB\n",
      "/datadisk/data/raw_data_extraction_v2/social_listening/SocialListeningpantip_post_data.csv_4.txt\n",
      "/datadisk/data/raw_data_extraction_v2/social_listening/SocialListeningpantip_post_data.csv_0.txt\n",
      "/datadisk/data/raw_data_extraction_v2/social_listening/SocialListeningpantip_post_data.csv_3.txt\n",
      "/datadisk/data/raw_data_extraction_v2/social_listening/SocialListeningpantip_post_data.csv_5.txt\n",
      "/datadisk/data/raw_data_extraction_v2/social_listening/SocialListeningpantip_post_data.csv_1.txt\n",
      "GuruCrawler Amounts to a total of 171.21 MB\n",
      "\n",
      "I have a total of 1409 files!\n",
      "Amounts to a total of 41284.40 MB\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(\"/datadisk/data\")\n",
    "\n",
    "# DATA_RAW_PATH = DATA_PATH/\"raw\"\n",
    "DATA_RAW_EXTRACTED_PATH = DATA_PATH/\"raw_data_extraction_v2\"\n",
    "\n",
    "# Output is in bytes - helper from Pathlib Path https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python\n",
    "def getStat(prev_value, cur_value):\n",
    "    if isinstance(prev_value, int):\n",
    "        return prev_value + cur_value.stat().st_size\n",
    "    return prev_value.stat().st_size + cur_value.stat().st_size\n",
    "\n",
    "# 1. The data from thwiki\n",
    "THWIKI_FOLDER = Path(\"thwiki-20200601-extracted\")\n",
    "WIKI_FILES = list((DATA_RAW_EXTRACTED_PATH/THWIKI_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , WIKI_FILES[:5]))\n",
    "print(f\"thwiki-20200601-extracted Amounts to a total of {reduce(getStat, WIKI_FILES)/1e6:.2f} MB\")\n",
    "\n",
    "# 2. The classification data from jung and ninja\n",
    "CLASSIFICATION_JUNG_NINJA_FOLDER = Path(\"classification_dataset\")\n",
    "CLASSIFICATION_FILES = list((DATA_RAW_EXTRACTED_PATH/CLASSIFICATION_JUNG_NINJA_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , CLASSIFICATION_FILES[:5]))\n",
    "print(f\"classification_dataset Amounts to a total of {reduce(getStat, CLASSIFICATION_FILES)/1e6:.2f} MB\")\n",
    "\n",
    "# 3. The Data from p'Moo Crawlers\n",
    "ANOTHER_WEBSITE_MOO_FOLDER = Path(\"another_website\")\n",
    "ANOTHER_WEBSITE_FILES = list((DATA_RAW_EXTRACTED_PATH/ANOTHER_WEBSITE_MOO_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , ANOTHER_WEBSITE_FILES[:5]))\n",
    "print(f\"another_website Amounts to a total of {reduce(getStat, ANOTHER_WEBSITE_FILES)/1e6:.2f} MB\")\n",
    "\n",
    "# 4. Senior Project Files\n",
    "SENIOR_PROJ_FOLDER = Path(\"data_lm\")\n",
    "SENIOR_PROJ_FILES = list((DATA_RAW_EXTRACTED_PATH/SENIOR_PROJ_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , SENIOR_PROJ_FILES[:5]))\n",
    "print(f\"Senior Project Amounts to a total of {reduce(getStat, SENIOR_PROJ_FILES)/1e6:.2f} MB\")\n",
    "\n",
    "# 5. Guru Crawler Files\n",
    "GURU_CRAWLER_FOLDER = Path(\"social_listening\")\n",
    "GURU_CRAWLER_FILES = list((DATA_RAW_EXTRACTED_PATH/GURU_CRAWLER_FOLDER).glob(\"*.txt\"))\n",
    "list(map(print , GURU_CRAWLER_FILES[:5]))\n",
    "print(f\"GuruCrawler Amounts to a total of {reduce(getStat, GURU_CRAWLER_FILES)/1e6:.2f} MB\")\n",
    "\n",
    "ALL_FILES = WIKI_FILES + CLASSIFICATION_FILES + ANOTHER_WEBSITE_FILES + SENIOR_PROJ_FILES + GURU_CRAWLER_FILES\n",
    "print(f\"\\nI have a total of {len(ALL_FILES)} files!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Amounts to a total of {reduce(getStat, ALL_FILES)/1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out BERT per Notebook \n",
    "\n",
    "From __HuggingFace Notebooks__ https://huggingface.co/transformers/notebooks.html: \n",
    "\n",
    "How to train a language model\tHighlight all the steps to effectively train Transformer model on custom data\n",
    "- Colab (ipynb) version : https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb\n",
    "- MD version: https://github.com/huggingface/blog/blob/master/how-to-train.md\n",
    "\n",
    "Pretrain Longformer\tHow to build a \"long\" version of existing pretrained models\tIz Beltagy  \n",
    "https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 80000\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertConfig\n",
    "\n",
    "configuration = BertConfig(\n",
    "    vocab_size=80000,\n",
    "#     max_position_embeddings=512, # 512 + 2 more special tokens\n",
    "#     num_attention_heads=12,\n",
    "#     num_hidden_layers=12,\n",
    "#     type_vocab_size=1,\n",
    ")\n",
    "# configuration.vocab_size = 20000\n",
    "\n",
    "model = BertForMaskedLM(config=configuration)\n",
    "# model = RobertaForMaskedLM.from_pretrained('./Roberta/checkpoint-200000')\n",
    "\n",
    "# Accessing the model configuration\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148153472"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()\n",
    "# => 102 million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(80000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=80000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite Tokenizer of bert_itos_80k with special tokens in front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from senior_project_util import ThaiTokenizer, pre_rules_th, post_rules_th\n",
    "from fastai.text.transform import BaseTokenizer, Tokenizer, Vocab\n",
    "from fastai.text.data import TokenizeProcessor, NumericalizeProcessor\n",
    "\n",
    "TOK_PATH = Path('./senior_proj_itos')\n",
    "\n",
    "max_vocab = 80000\n",
    "\n",
    "BOS,EOS,FLD,UNK,PAD = 'xxbos','xxeos','xxfld','xxunk','xxpad'\n",
    "TK_REP,TK_WREP, TK_NUM, TK_LAUGH = 'xxrep','xxwrep', 'xxnum', 'xxlaugh'\n",
    "text_spec_tok = [UNK,PAD,BOS,EOS,FLD,TK_REP,TK_WREP, TK_NUM, TK_LAUGH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.61\n"
     ]
    }
   ],
   "source": [
    "import fastai\n",
    "print(fastai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(TOK_PATH/\"bert_itos_80k.pkl\", 'rb') as f:\n",
    "#     itos = pickle.load(f)\n",
    "# len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for o in reversed(text_spec_tok):\n",
    "#     if o in itos: itos.remove(o)\n",
    "#     itos.insert(0, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itos = itos[:max_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(TOK_PATH/\"bert_itos_80k_cleaned.pkl\", 'wb') as f:\n",
    "#     pickle.dump(itos, f, pickle.HIGHEST_PROTOCOL)\n",
    "# print(f\"Successfully written vocabulary itos in {TOK_PATH/'bert_itos_80k_cleaned.pkl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(TOK_PATH/\"bert_itos_80k_cleaned.pkl\", 'rb') as f:\n",
    "    itos = pickle.load(f)\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'itos': ['xxunk',\n",
       "  'xxpad',\n",
       "  'xxbos',\n",
       "  'xxeos',\n",
       "  'xxfld',\n",
       "  'xxrep',\n",
       "  'xxwrep',\n",
       "  'xxnum',\n",
       "  'xxlaugh',\n",
       "  ' ',\n",
       "  'ที่',\n",
       "  'ก็',\n",
       "  'เรา',\n",
       "  'จะ',\n",
       "  'ไป',\n",
       "  'ครับ',\n",
       "  'มี',\n",
       "  'มา',\n",
       "  'ได้',\n",
       "  'แต่',\n",
       "  'ว่า',\n",
       "  'เป็น',\n",
       "  'เลย',\n",
       "  'ค่ะ',\n",
       "  'ไม่',\n",
       "  'ผม',\n",
       "  'แล้ว',\n",
       "  'และ',\n",
       "  'ให้',\n",
       "  'ๆ',\n",
       "  'ใน',\n",
       "  'ของ',\n",
       "  'คน',\n",
       "  'กับ',\n",
       "  '(',\n",
       "  ')',\n",
       "  'หรือ',\n",
       "  'มัน',\n",
       "  'นี้',\n",
       "  'กัน',\n",
       "  'มาก',\n",
       "  'อยาก',\n",
       "  'คือ',\n",
       "  'ต้อง',\n",
       "  'ด้วย',\n",
       "  'อยู่',\n",
       "  'ทำ',\n",
       "  '.',\n",
       "  'เขา',\n",
       "  '-',\n",
       "  'จาก',\n",
       "  'ถ้า',\n",
       "  'เพราะ',\n",
       "  'อะไร',\n",
       "  '3',\n",
       "  'ไม่ได้',\n",
       "  'เค้า',\n",
       "  'คะ',\n",
       "  'แบบ',\n",
       "  'ยัง',\n",
       "  '\"',\n",
       "  'ดี',\n",
       "  'เรื่อง',\n",
       "  'ดู',\n",
       "  'กว่า',\n",
       "  'ใช้',\n",
       "  'นะ',\n",
       "  'บ้าง',\n",
       "  'อีก',\n",
       "  'พอ',\n",
       "  'บาท',\n",
       "  'ไหม',\n",
       "  'เพื่อน',\n",
       "  'ขอ',\n",
       "  'ใคร',\n",
       "  'ไหน',\n",
       "  'นะคะ',\n",
       "  'บอก',\n",
       "  'เอา',\n",
       "  'ซื้อ',\n",
       "  'แฟน',\n",
       "  ':',\n",
       "  'ปี',\n",
       "  'ถึง',\n",
       "  'การ',\n",
       "  '/',\n",
       "  'ชอบ',\n",
       "  '?',\n",
       "  'ตัว',\n",
       "  'ตอนนี้',\n",
       "  'ช่วย',\n",
       "  'นั้น',\n",
       "  'หน่อย',\n",
       "  'ยังไง',\n",
       "  'วัน',\n",
       "  'แค่',\n",
       "  'ราคา',\n",
       "  'ซึ่ง',\n",
       "  'พี่',\n",
       "  'ขึ้น',\n",
       "  '5',\n",
       "  'ขอบคุณ',\n",
       "  'เคย',\n",
       "  'คุณ',\n",
       "  'ไม่มี',\n",
       "  ',',\n",
       "  'เวลา',\n",
       "  'เห็น',\n",
       "  'ไว้',\n",
       "  'ใหม่',\n",
       "  'บอ',\n",
       "  'ก่อน',\n",
       "  'แล้',\n",
       "  'เพื่อ',\n",
       "  'จน',\n",
       "  'นี่',\n",
       "  'เข้า',\n",
       "  'ประมาณ',\n",
       "  'แบบนี้',\n",
       "  'ทาง',\n",
       "  'เหมือน',\n",
       "  'ตาม',\n",
       "  'ถาม',\n",
       "  'ทำให้',\n",
       "  'มั้ย',\n",
       "  'แนะนำ',\n",
       "  'หลาย',\n",
       "  'บ้าน',\n",
       "  'อย่าง',\n",
       "  'ต่อ',\n",
       "  'เมื่อ',\n",
       "  'ขาย',\n",
       "  'โดย',\n",
       "  'หา',\n",
       "  'เล่น',\n",
       "  'ทั้ง',\n",
       "  'เอง',\n",
       "  'ว่าจะ',\n",
       "  'น่า',\n",
       "  'รถ',\n",
       "  'จริงๆ',\n",
       "  'ตอน',\n",
       "  'เจอ',\n",
       "  'เดือน',\n",
       "  'คิด',\n",
       "  'เธอ',\n",
       "  'ลง',\n",
       "  'ทำไม',\n",
       "  'เยอะ',\n",
       "  'ค่า',\n",
       "  'ไม่รู้',\n",
       "  'สามารถ',\n",
       "  'ส่ง',\n",
       "  'คุย',\n",
       "  'รู้',\n",
       "  'กลับ',\n",
       "  '4',\n",
       "  'เริ่ม',\n",
       "  'ละ',\n",
       "  'แม่',\n",
       "  'รู้สึก',\n",
       "  'สำหรับ',\n",
       "  'งาน',\n",
       "  'น้อง',\n",
       "  'ใส่',\n",
       "  'เรียน',\n",
       "  'ท่าน',\n",
       "  'เครื่อง',\n",
       "  'ตัวเอง',\n",
       "  'นึง',\n",
       "  'ถูก',\n",
       "  'คิดว่า',\n",
       "  'ลอง',\n",
       "  'ร้าน',\n",
       "  'รับ',\n",
       "  'รบกวน',\n",
       "  'ออก',\n",
       "  'ส่วน',\n",
       "  'คง',\n",
       "  'จึง',\n",
       "  'เงิน',\n",
       "  'เปิด',\n",
       "  'วันที่',\n",
       "  'หมด',\n",
       "  'ไทย',\n",
       "  'ทราบ',\n",
       "  'ควร',\n",
       "  'พอดี',\n",
       "  'ที่จะ',\n",
       "  'กิน',\n",
       "  'พวก',\n",
       "  'รอ',\n",
       "  'ติด',\n",
       "  'รัก',\n",
       "  'ไม่เคย',\n",
       "  'โดน',\n",
       "  'ทุก',\n",
       "  'วันนี้',\n",
       "  'หลัง',\n",
       "  'หนึ่ง',\n",
       "  'อ่ะ',\n",
       "  'แล้วก็',\n",
       "  'ทำงาน',\n",
       "  'แรก',\n",
       "  'เลือก',\n",
       "  'เดิน',\n",
       "  '+',\n",
       "  'โทร',\n",
       "  'ไม่ใช่',\n",
       "  '*',\n",
       "  'ผ่าน',\n",
       "  'ปกติ',\n",
       "  'กระทู้',\n",
       "  'อ่าน',\n",
       "  'ก็ได้',\n",
       "  'ห้อง',\n",
       "  'ไม่ค่อย',\n",
       "  'ช่วง',\n",
       "  'พูด',\n",
       "  'เกิด',\n",
       "  'เด็ก',\n",
       "  'เปลี่ยน',\n",
       "  'ออกมา',\n",
       "  'สอบถาม',\n",
       "  'ตอบ',\n",
       "  'สอง',\n",
       "  'ตลอด',\n",
       "  'ที',\n",
       "  'กลับมา',\n",
       "  'กลัว',\n",
       "  'อะ',\n",
       "  'ลูก',\n",
       "  'รูป',\n",
       "  '!',\n",
       "  'วิธี',\n",
       "  'กำลัง',\n",
       "  'นั่ง',\n",
       "  'นาน',\n",
       "  'เข้ามา',\n",
       "  'จบ',\n",
       "  'ความ',\n",
       "  'ไง',\n",
       "  'ตั้งแต่',\n",
       "  'น่าจะ',\n",
       "  'หนัง',\n",
       "  '“',\n",
       "  'อาจจะ',\n",
       "  'ชื่อ',\n",
       "  'ไม่ต้อง',\n",
       "  'ที่มี',\n",
       "  'อย่างไร',\n",
       "  '”',\n",
       "  'สวย',\n",
       "  'ผู้',\n",
       "  'ฟรี',\n",
       "  '%',\n",
       "  'ครั้ง',\n",
       "  'คับ',\n",
       "  '..',\n",
       "  'ภาพ',\n",
       "  'บริษัท',\n",
       "  'บาง',\n",
       "  'แจ้ง',\n",
       "  'อย่า',\n",
       "  'ผู้หญิง',\n",
       "  'เพลง',\n",
       "  'สุด',\n",
       "  'พ่อ',\n",
       "  'บน',\n",
       "  'อายุ',\n",
       "  'ใหญ่',\n",
       "  'เกี่ยวกับ',\n",
       "  'จริง',\n",
       "  'เดิม',\n",
       "  'รอบ',\n",
       "  'เพิ่ม',\n",
       "  'จ่าย',\n",
       "  'ต่าง ๆ',\n",
       "  'พร้อม',\n",
       "  'สิ่ง',\n",
       "  'เล',\n",
       "  'ข้อมูล',\n",
       "  'สี',\n",
       "  'ฟัง',\n",
       "  'อื่น',\n",
       "  'เหมือนกัน',\n",
       "  'ขนาด',\n",
       "  '#',\n",
       "  'ทุกคน',\n",
       "  'สาย',\n",
       "  'รุ่น',\n",
       "  'เกิน',\n",
       "  'เก่า',\n",
       "  'หรือเปล่า',\n",
       "  'ปล.',\n",
       "  'หาก',\n",
       "  'ฉัน',\n",
       "  'ทีม',\n",
       "  'มากกว่า',\n",
       "  'คบ',\n",
       "  'สนใจ',\n",
       "  'หนู',\n",
       "  'เข้าไป',\n",
       "  'นอน',\n",
       "  'น่ะ',\n",
       "  'ผู้ชาย',\n",
       "  'ตรง',\n",
       "  'ดีกว่า',\n",
       "  'ได้รับ',\n",
       "  'เช่น',\n",
       "  'อยู่ใน',\n",
       "  'ของเรา',\n",
       "  'กลุ่ม',\n",
       "  'สวัสดี',\n",
       "  'เกม',\n",
       "  'เปล่า',\n",
       "  'ช่อง',\n",
       "  'เสีย',\n",
       "  'ระหว่าง',\n",
       "  'ยิ่ง',\n",
       "  'คำ',\n",
       "  'ผิด',\n",
       "  'นิ้ว',\n",
       "  'กะ',\n",
       "  'พยายาม',\n",
       "  'คืน',\n",
       "  'สูง',\n",
       "  'จัด',\n",
       "  'อยากได้',\n",
       "  'เท่าไหร่',\n",
       "  ']',\n",
       "  'สัก',\n",
       "  'คนอื่น',\n",
       "  'พา',\n",
       "  'อยากรู้',\n",
       "  'ชีวิต',\n",
       "  'แอบ',\n",
       "  'สินค้า',\n",
       "  'เพิ่ง',\n",
       "  '[',\n",
       "  'นาง',\n",
       "  'ยาว',\n",
       "  'กด',\n",
       "  'สงสัย',\n",
       "  's',\n",
       "  'ลูกค้า',\n",
       "  'ปัญหา',\n",
       "  'ตั้ง',\n",
       "  'มอง',\n",
       "  'ล่ะ',\n",
       "  'ม.',\n",
       "  'เดียว',\n",
       "  'ใบ',\n",
       "  'น้ำ',\n",
       "  'บัตร',\n",
       "  'เหลือ',\n",
       "  'ที่สุด',\n",
       "  'น้อย',\n",
       "  'สุดท้าย',\n",
       "  'เบอร์',\n",
       "  'ที่ไหน',\n",
       "  'มว่า',\n",
       "  'เลิก',\n",
       "  'ด้าน',\n",
       "  'ใกล้',\n",
       "  'น.',\n",
       "  'ติดต่อ',\n",
       "  'อัน',\n",
       "  'คุยกับ',\n",
       "  'เข้าใจ',\n",
       "  'หรือไม่',\n",
       "  'สาขา',\n",
       "  'ถา',\n",
       "  'นำ',\n",
       "  'ใช่',\n",
       "  'มีปัญหา',\n",
       "  'คำแนะนำ',\n",
       "  'เสียง',\n",
       "  'มาจาก',\n",
       "  'the',\n",
       "  'หมอ',\n",
       "  'ตอนนั้น',\n",
       "  \"'\",\n",
       "  'ยนะ',\n",
       "  'ไร',\n",
       "  'พัก',\n",
       "  'แห่ง',\n",
       "  'ทุกอย่าง',\n",
       "  'ต่อไป',\n",
       "  'หาย',\n",
       "  'ระบบ',\n",
       "  'ล้าน',\n",
       "  'หลังจาก',\n",
       "  'สร้าง',\n",
       "  'เท่านั้น',\n",
       "  'ตา',\n",
       "  'เดินทาง',\n",
       "  'เนื่องจาก',\n",
       "  'ญี่ปุ่น',\n",
       "  'ทั้งหมด',\n",
       "  'ล่วงหน้า',\n",
       "  'คนเดียว',\n",
       "  'จะต้อง',\n",
       "  'อา',\n",
       "  'เก็บ',\n",
       "  'ต้องการ',\n",
       "  'ปิด',\n",
       "  'เต็ม',\n",
       "  'เที่ยว',\n",
       "  'ประเทศ',\n",
       "  't',\n",
       "  'ไปเที่ยว',\n",
       "  'จขกท',\n",
       "  'อาจ',\n",
       "  'เกือบ',\n",
       "  'กล้อง',\n",
       "  'ใจ',\n",
       "  'เสร็จ',\n",
       "  'เรียก',\n",
       "  'รู้จัก',\n",
       "  'อาหาร',\n",
       "  'วิ่ง',\n",
       "  'ลด',\n",
       "  'จ้า',\n",
       "  'ชุด',\n",
       "  'a',\n",
       "  'ที่บ้าน',\n",
       "  'ลงทะเบียน',\n",
       "  'อี',\n",
       "  'กระเป๋า',\n",
       "  'เพียง',\n",
       "  'อื่น ๆ',\n",
       "  'เขียน',\n",
       "  'อันนี้',\n",
       "  'พนักงาน',\n",
       "  'โลก',\n",
       "  'ทุกวัน',\n",
       "  'มือ',\n",
       "  'ไปหา',\n",
       "  'ง่าย',\n",
       "  'ยา',\n",
       "  'งง',\n",
       "  '**',\n",
       "  'ต่าง',\n",
       "  'สอบ',\n",
       "  'ซัก',\n",
       "  'เรื่อย ๆ',\n",
       "  'หัว',\n",
       "  'แก',\n",
       "  'ลืม',\n",
       "  'เล่า',\n",
       "  'จุด',\n",
       "  'อยู่แล้ว',\n",
       "  'คู่',\n",
       "  'ถ่าย',\n",
       "  'เมือง',\n",
       "  'แทน',\n",
       "  'ประกัน',\n",
       "  'เสื้อ',\n",
       "  '@',\n",
       "  'รวม',\n",
       "  'ครอบครัว',\n",
       "  'แนว',\n",
       "  'ตอนแรก',\n",
       "  'น่ารัก',\n",
       "  'ละคร',\n",
       "  'นาย',\n",
       "  'หุ้น',\n",
       "  'ไปดู',\n",
       "  'ที่ผ่านมา',\n",
       "  'ซะ',\n",
       "  'ไม่ทราบ',\n",
       "  'g',\n",
       "  'ชม',\n",
       "  'กี่',\n",
       "  'ยาก',\n",
       "  'ส่วนตัว',\n",
       "  'สมัคร',\n",
       "  'วัด',\n",
       "  'ปัจจุบัน',\n",
       "  'พระ',\n",
       "  'เหมือนเดิม',\n",
       "  'ชั้น',\n",
       "  'นาที',\n",
       "  'แถว',\n",
       "  'แก้',\n",
       "  'วจะ',\n",
       "  'ฝ่าย',\n",
       "  'อาทิตย์',\n",
       "  'ตาย',\n",
       "  'ใด',\n",
       "  'ทัก',\n",
       "  'แชร์',\n",
       "  'เดียวกัน',\n",
       "  'กู',\n",
       "  'ออกไป',\n",
       "  'คัน',\n",
       "  'มีอะไร',\n",
       "  'สั่ง',\n",
       "  'ดิฉัน',\n",
       "  'สาว',\n",
       "  'ตัด',\n",
       "  'รีวิว',\n",
       "  'ฝาก',\n",
       "  'ปล่อย',\n",
       "  'ทาน',\n",
       "  'x',\n",
       "  'แถม',\n",
       "  'แน่นอน',\n",
       "  'ศูนย์',\n",
       "  'บริการ',\n",
       "  'เช็ค',\n",
       "  'เน็ต',\n",
       "  'ใช้งาน',\n",
       "  'จัดส่ง',\n",
       "  'ตอนที่',\n",
       "  'สรุป',\n",
       "  'หรอก',\n",
       "  'หนัก',\n",
       "  'ตัดสินใจ',\n",
       "  'หลัก',\n",
       "  'โปร',\n",
       "  'ไม่สามารถ',\n",
       "  'รึเปล่า',\n",
       "  'บางที',\n",
       "  'ยมา',\n",
       "  'ด้วยกัน',\n",
       "  'จอง',\n",
       "  'มือถือ',\n",
       "  'อาการ',\n",
       "  'หายไป',\n",
       "  'โทรศัพท์',\n",
       "  'ไลน์',\n",
       "  'จอ',\n",
       "  'คำตอบ',\n",
       "  'ก็คือ',\n",
       "  '\\u200b',\n",
       "  'วเรา',\n",
       "  'ครบ',\n",
       "  'รายการ',\n",
       "  '_',\n",
       "  '>',\n",
       "  'ระดับ',\n",
       "  'เจ้า',\n",
       "  'ทำได้',\n",
       "  'ขา',\n",
       "  '=',\n",
       "  'มือสอง',\n",
       "  'สิ',\n",
       "  'นัด',\n",
       "  'ที่พัก',\n",
       "  'ส่วนใหญ่',\n",
       "  'เร็ว',\n",
       "  'แดง',\n",
       "  'โอเค',\n",
       "  'ณ',\n",
       "  '!!',\n",
       "  'ผู้รู้',\n",
       "  'ข่าว',\n",
       "  'คำถาม',\n",
       "  'รีบ',\n",
       "  'มากมาย',\n",
       "  'เย็น',\n",
       "  'ฯ',\n",
       "  'ดีมาก',\n",
       "  'โรงเรียน',\n",
       "  'ค่อนข้าง',\n",
       "  'ยังมี',\n",
       "  'กำลังจะ',\n",
       "  'ดัง',\n",
       "  'ดูแล',\n",
       "  'บางคน',\n",
       "  'แต่ละ',\n",
       "  'ภายใน',\n",
       "  'ออกจาก',\n",
       "  'เกิดขึ้น',\n",
       "  'ไม่ชอบ',\n",
       "  'ขับ',\n",
       "  'แรง',\n",
       "  'เว็บ',\n",
       "  'ที่นี่',\n",
       "  'คอม',\n",
       "  'สาม',\n",
       "  'ความรัก',\n",
       "  'ด่า',\n",
       "  'ไฟ',\n",
       "  'กล่อง',\n",
       "  'โอน',\n",
       "  'พ่อแม่',\n",
       "  'บ่อย',\n",
       "  'ที่มา',\n",
       "  'เนี่ย',\n",
       "  'ไม่ดี',\n",
       "  'ธนาคาร',\n",
       "  'ทิ้ง',\n",
       "  'โรงแรม',\n",
       "  'ยี่ห้อ',\n",
       "  'ข้อ',\n",
       "  'เพิ่มเติม',\n",
       "  'รายละเอียด',\n",
       "  'ความรู้สึก',\n",
       "  'มีโอกาส',\n",
       "  'เช้า',\n",
       "  'ย้าย',\n",
       "  'เอ',\n",
       "  'จำนวน',\n",
       "  'เห็นว่า',\n",
       "  'ครั้งแรก',\n",
       "  'ช่าง',\n",
       "  'ถือว่า',\n",
       "  'ค่าย',\n",
       "  'ais',\n",
       "  'ๆๆ',\n",
       "  'พบ',\n",
       "  'ได้ยิน',\n",
       "  'มาถึง',\n",
       "  'นัก',\n",
       "  'line',\n",
       "  '??',\n",
       "  'จับ',\n",
       "  'ร่วม',\n",
       "  'อีกครั้ง',\n",
       "  'ดนี้',\n",
       "  'com',\n",
       "  'แพง',\n",
       "  'ปรับ',\n",
       "  'นั่น',\n",
       "  'size',\n",
       "  'สอน',\n",
       "  'ทา',\n",
       "  'ล่าสุด',\n",
       "  'เจ้าของ',\n",
       "  'หรือว่า',\n",
       "  'ยอด',\n",
       "  'กลายเป็น',\n",
       "  'ชวน',\n",
       "  'นา',\n",
       "  'ภาค',\n",
       "  'แสน',\n",
       "  'พัน',\n",
       "  'หยุด',\n",
       "  'สภาพ',\n",
       "  'จีน',\n",
       "  'โอ',\n",
       "  'm',\n",
       "  'ครู',\n",
       "  'สมาชิก',\n",
       "  'มีอยู่',\n",
       "  'เล็ก',\n",
       "  '--',\n",
       "  'ที่แล้ว',\n",
       "  'ตก',\n",
       "  'มากขึ้น',\n",
       "  'ทั้งๆ',\n",
       "  'ควรจะ',\n",
       "  'ขอให้',\n",
       "  'สะดวก',\n",
       "  'ผล',\n",
       "  'พื้นที่',\n",
       "  'ข้าง',\n",
       "  'เกาหลี',\n",
       "  'เลี้ยง',\n",
       "  'แก้ไข',\n",
       "  'ที่ใช้',\n",
       "  'กรณี',\n",
       "  'ไอ',\n",
       "  'สามี',\n",
       "  'รักษา',\n",
       "  'ร้องไห้',\n",
       "  'ชั่วโมง',\n",
       "  'ได้ที่',\n",
       "  'ถนน',\n",
       "  'เจ้าหน้าที่',\n",
       "  'ข้อความ',\n",
       "  'ปรากฏ',\n",
       "  'ถ่ายรูป',\n",
       "  'ชาย',\n",
       "  'เก่ง',\n",
       "  'บอล',\n",
       "  'ไม่มีใคร',\n",
       "  'ดัน',\n",
       "  'สนุก',\n",
       "  'วมัน',\n",
       "  'พึ่ง',\n",
       "  'กรุงเทพ',\n",
       "  'เจ็บ',\n",
       "  'จำ',\n",
       "  'ประสบการณ์',\n",
       "  'ทะเลาะ',\n",
       "  'อย่างเดียว',\n",
       "  'คลิป',\n",
       "  '^^',\n",
       "  'เอกสาร',\n",
       "  'เครียด',\n",
       "  'ตลาด',\n",
       "  'ใช่ไหม',\n",
       "  'ไมล์',\n",
       "  'ให้ได้',\n",
       "  'หัวข้อ',\n",
       "  'ใค',\n",
       "  '&',\n",
       "  'หนังสือ',\n",
       "  'จัง',\n",
       "  'หมา',\n",
       "  'เงินเดือน',\n",
       "  'เฟส',\n",
       "  'ช่วงนี้',\n",
       "  'แน่',\n",
       "  'ต้น',\n",
       "  'รหัสสินค้า',\n",
       "  'เรียบร้อย',\n",
       "  'เน้น',\n",
       "  'บัญชี',\n",
       "  'เนื้อ',\n",
       "  'เกาะ',\n",
       "  'แสดง',\n",
       "  'ใช้ได้',\n",
       "  'รมี',\n",
       "  'แปลก',\n",
       "  'ประชาชน',\n",
       "  'ขอโทษ',\n",
       "  'งบ',\n",
       "  'อก',\n",
       "  'เติม',\n",
       "  'ยืน',\n",
       "  'iphone',\n",
       "  'ช้า',\n",
       "  'รัฐบาล',\n",
       "  'ไม่แน่ใจ',\n",
       "  'ปก',\n",
       "  'ให้กับ',\n",
       "  'พระเอก',\n",
       "  'กา',\n",
       "  'ชิ้น',\n",
       "  'ธุรกิจ',\n",
       "  'ผิว',\n",
       "  'ท้อง',\n",
       "  'เท่า',\n",
       "  'หลังจากนั้น',\n",
       "  'ตรวจ',\n",
       "  'เตรียม',\n",
       "  'คิดถึง',\n",
       "  'ที่อยู่',\n",
       "  'นอกจาก',\n",
       "  'โมง',\n",
       "  'แถ',\n",
       "  'ผ่านไป',\n",
       "  'โปรแกรม',\n",
       "  'หญิง',\n",
       "  'ไม่กล้า',\n",
       "  'แต่ง',\n",
       "  'แหละ',\n",
       "  'ประเทศไทย',\n",
       "  'ชาว',\n",
       "  'ทรู',\n",
       "  'ยู',\n",
       "  'มาก่อน',\n",
       "  'ไห',\n",
       "  'ก',\n",
       "  'บท',\n",
       "  'เหตุการณ์',\n",
       "  'เล่ม',\n",
       "  'ฉาก',\n",
       "  'กลับบ้าน',\n",
       "  'ทุกๆ',\n",
       "  'ลา',\n",
       "  'เล็กๆ',\n",
       "  'ดำ',\n",
       "  'i',\n",
       "  'ไปถึง',\n",
       "  'ร้อง',\n",
       "  'แก่',\n",
       "  'เหนื่อย',\n",
       "  'อร่อย',\n",
       "  'คนไทย',\n",
       "  'โอกาส',\n",
       "  'ทันที',\n",
       "  'ตี',\n",
       "  'ย.',\n",
       "  'แทบ',\n",
       "  'รายได้',\n",
       "  'ทริป',\n",
       "  'แมว',\n",
       "  'คอย',\n",
       "  'เผื่อ',\n",
       "  'ยื่น',\n",
       "  'หวัง',\n",
       "  'จากนั้น',\n",
       "  'หน่อ',\n",
       "  'อารมณ์',\n",
       "  'มีความสุข',\n",
       "  'b',\n",
       "  'มหาลัย',\n",
       "  'ครึ่ง',\n",
       "  'วาง',\n",
       "  'ไกล',\n",
       "  'ซ่อม',\n",
       "  'ภาษา',\n",
       "  'ต่อมา',\n",
       "  'ตั้งใจ',\n",
       "  'เฉยๆ',\n",
       "  'เป็นเรื่อง',\n",
       "  'สักพัก',\n",
       "  'ประตู',\n",
       "  'สิว',\n",
       "  'ครั้งนี้',\n",
       "  'ขาว',\n",
       "  'โครงการ',\n",
       "  'ลบ',\n",
       "  'เรียกว่า',\n",
       "  'เกินไป',\n",
       "  'ความคิด',\n",
       "  'ป้า',\n",
       "  'ขึ้นไป',\n",
       "  'สา',\n",
       "  'true',\n",
       "  'เองก็',\n",
       "  'ทน',\n",
       "  'เท่าไร',\n",
       "  'ตกลง',\n",
       "  'สัญญาณ',\n",
       "  'มส่ง',\n",
       "  'ที่ว่า',\n",
       "  'ได้มา',\n",
       "  'ทอง',\n",
       "  'แย่',\n",
       "  'ป',\n",
       "  'ฝั่ง',\n",
       "  'ติดตาม',\n",
       "  'ผ้า',\n",
       "  'ทีนี้',\n",
       "  'cr',\n",
       "  'พิเศษ',\n",
       "  'เหตุผล',\n",
       "  'ไม่ยอม',\n",
       "  'สู้',\n",
       "  'น้า',\n",
       "  'เจอกัน',\n",
       "  'เอาไว้',\n",
       "  'ใช้เวลา',\n",
       "  'ยอม',\n",
       "  'เค',\n",
       "  'วที่',\n",
       "  'เสียใจ',\n",
       "  'ดีขึ้น',\n",
       "  'ต่างประเทศ',\n",
       "  'วันนั้น',\n",
       "  'ขาด',\n",
       "  'แพ้',\n",
       "  'เพื่อให้',\n",
       "  'แอร์',\n",
       "  'ปรึกษา',\n",
       "  'ขออภัย',\n",
       "  'หน้าตา',\n",
       "  'ยอมรับ',\n",
       "  'นิดหน่อย',\n",
       "  'อย่างนี้',\n",
       "  'ตั๋ว',\n",
       "  'สังคม',\n",
       "  'คนละ',\n",
       "  'นางเอก',\n",
       "  'บางครั้ง',\n",
       "  'เบื่อ',\n",
       "  'ห่าง',\n",
       "  'ชาติ',\n",
       "  'เสริม',\n",
       "  'ของเขา',\n",
       "  'd',\n",
       "  'พันทิป',\n",
       "  'อิน',\n",
       "  'ทุกท่าน',\n",
       "  'หมื่น',\n",
       "  'กรอบ',\n",
       "  'ยบอก',\n",
       "  'ฝัน',\n",
       "  'อุปกรณ์',\n",
       "  'เส้น',\n",
       "  'ขับรถ',\n",
       "  'ธรรมดา',\n",
       "  'เดีย',\n",
       "  'ครับผม',\n",
       "  'ไม่เข้าใจ',\n",
       "  'of',\n",
       "  'ที่ทำงาน',\n",
       "  'gb',\n",
       "  'เพราะว่า',\n",
       "  'เฉพาะ',\n",
       "  'dtac',\n",
       "  'facebook',\n",
       "  'ยิ้ม',\n",
       "  'ง่ายๆ',\n",
       "  'ค่อย',\n",
       "  'ขนา',\n",
       "  'ที่ดี',\n",
       "  'มั้ง',\n",
       "  'พิมพ์',\n",
       "  'ตำแหน่ง',\n",
       "  'เดี๋ยว',\n",
       "  'จีบ',\n",
       "  'มิ',\n",
       "  'ซิม',\n",
       "  'ง.',\n",
       "  'นับ',\n",
       "  'รถยนต์',\n",
       "  'พร้อ',\n",
       "  'ตื่น',\n",
       "  'แวะ',\n",
       "  'บริเวณ',\n",
       "  'ประเด็น',\n",
       "  'ดาวน์',\n",
       "  'แยก',\n",
       "  'ยกเลิก',\n",
       "  'สมัย',\n",
       "  'เชื่อ',\n",
       "  'จนถึง',\n",
       "  'ทีวี',\n",
       "  'ครีม',\n",
       "  'มาหา',\n",
       "  'คา',\n",
       "  'อีกที',\n",
       "  'ห้าม',\n",
       "  'ทรง',\n",
       "  'คุ้ม',\n",
       "  'แน่ๆ',\n",
       "  'การเดินทาง',\n",
       "  'ยก',\n",
       "  'ตำรวจ',\n",
       "  'โดยเฉพาะ',\n",
       "  'โหลด',\n",
       "  'สัปดาห์',\n",
       "  'ทั่วไป',\n",
       "  'ใส',\n",
       "  'โต',\n",
       "  'ปาก',\n",
       "  'ไม่เจอ',\n",
       "  'ไอ้',\n",
       "  'กลาง',\n",
       "  'แล้วแต่',\n",
       "  'สัญญา',\n",
       "  'เป็นอะไร',\n",
       "  'เรื่องราว',\n",
       "  'in',\n",
       "  'หนี',\n",
       "  'ค่าใช้จ่าย',\n",
       "  'เป็นแบบ',\n",
       "  'ราย',\n",
       "  'ตรวจสอบ',\n",
       "  'คณะ',\n",
       "  'ประเภท',\n",
       "  'ต่างกัน',\n",
       "  'ทั้งสอง',\n",
       "  'สนามบิน',\n",
       "  'มีเรื่อง',\n",
       "  'อธิบาย',\n",
       "  'ไม่มีอะไร',\n",
       "  'เทียบ',\n",
       "  'คะแนน',\n",
       "  'ดังนั้น',\n",
       "  'ทั้งที่',\n",
       "  'จังหวัด',\n",
       "  'เสมอ',\n",
       "  'เว',\n",
       "  'บ่อยๆ',\n",
       "  'ตั้งกระทู้',\n",
       "  'ดังกล่าว',\n",
       "  'ตรงไหน',\n",
       "  'กำหนด',\n",
       "  'ข้าว',\n",
       "  'ประกาศ',\n",
       "  'ชา',\n",
       "  '–',\n",
       "  'แตก',\n",
       "  'เพจ',\n",
       "  'รา',\n",
       "  'ออนไลน์',\n",
       "  'สถานี',\n",
       "  'ได้ดี',\n",
       "  'วง',\n",
       "  'อันดับ',\n",
       "  'กฎหมาย',\n",
       "  'ปั่น',\n",
       "  'ฮา',\n",
       "  'บิน',\n",
       "  'อ.',\n",
       "  'อเมริกา',\n",
       "  'ยืนยัน',\n",
       "  'ถือ',\n",
       "  'รู้เรื่อง',\n",
       "  'ศึกษา',\n",
       "  'ตรงนี้',\n",
       "  'ไม่ไหว',\n",
       "  'ประกอบ',\n",
       "  'ตามที่',\n",
       "  'ไม่เห็น',\n",
       "  'ภาษาอังกฤษ',\n",
       "  'การใช้',\n",
       "  'ไม่ทัน',\n",
       "  'เมื่อวาน',\n",
       "  'ทหาร',\n",
       "  'ความเห็น',\n",
       "  ...]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocab(itos)\n",
    "vocab.__getstate__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ม่าย', 'เอา', 'เปง', 'ไง', 'บ้าง', 'น่ารัก', 'จุงเบย']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='ม่ายเอาเปงไงบ้างน่ารักจุงเบย'\n",
    "pyThai_tt = ThaiTokenizer()\n",
    "a = pyThai_tt.tokenizer(text)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ไม่', 'เอา', 'เปง', 'ไง', 'บ้าง', 'น่ารัก', 'จังเลย']]\n",
      "[[24, 78, 12028, 241, 67, 464, 3080]]\n"
     ]
    }
   ],
   "source": [
    "tt = Tokenizer(tok_func = ThaiTokenizer, lang = 'th', pre_rules = pre_rules_th, post_rules=post_rules_th, n_cpus=1)\n",
    "test_sample = tt._process_all_1([text[:1000]])\n",
    "print(test_sample)\n",
    "test_sample = [vocab.numericalize(seq) for seq in test_sample]\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.numericalize([\"asdw9eiqpwoied\"]) #UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizer\n",
    "\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"./all-data-bytebpe-20000\", max_len=512)\n",
    "# tokenizer.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSeniorProjectTokenizer(object):\n",
    "    def __init__(self, TOK_PATH = Path('./senior_proj_itos'), BOS='xxbos', EOS='xxeos', FLD = 'xxfld', UNK='xxunk', PAD='xxpad',\n",
    "                 TK_REP='xxrep', TK_WREP='xxwrep', TK_NUM='xxnum', TK_LAUGH='xxlaugh', n_cpus=1,\n",
    "                ):\n",
    "        from senior_project_util import ThaiTokenizer, pre_rules_th, post_rules_th\n",
    "        from fastai.text.transform import BaseTokenizer, Tokenizer, Vocab\n",
    "        from fastai.text.data import TokenizeProcessor, NumericalizeProcessor\n",
    "\n",
    "        with open(TOK_PATH/\"bert_itos_80k_cleaned.pkl\", 'rb') as f:\n",
    "            itos = pickle.load(f)\n",
    "            \n",
    "        self.vocab = Vocab(itos)\n",
    "        self.tokenizer = Tokenizer(tok_func = ThaiTokenizer, lang = 'th', \n",
    "                                   pre_rules = pre_rules_th, post_rules=post_rules_th, n_cpus=n_cpus)\n",
    "        \n",
    "        self.cls_token_id = self.vocab.stoi[BOS]\n",
    "        self.sep_token_id = self.vocab.stoi[EOS]\n",
    "        self.pad_token_id = self.vocab.stoi[PAD]\n",
    "        \n",
    "        self.mask_token = FLD  #SINCE THIS ONE IS NOT USED, and INSIDE SPECIAL TOKEN....\n",
    "        self._pad_token = PAD\n",
    "        \n",
    "#         tokenizer_processor = TokenizeProcessor(tokenizer=tt, chunksize=300000, mark_fields=False)\n",
    "#         numbericalize_processor = NumericalizeProcessor(vocab=vocab)\n",
    "        \n",
    "    def num_special_tokens_to_add(self, pair=False):\n",
    "        return 2\n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer._process_all_1([text])[0]\n",
    "#         return self.tokenizer.process_all([text])[0]\n",
    "    \n",
    "    def convert_tokens_to_ids(self, token_list):\n",
    "        #From https://huggingface.co/transformers/_modules/transformers/tokenization_utils_fast.html#PreTrainedTokenizerFast.convert_tokens_to_ids\n",
    "        if token_list is None:\n",
    "            return None\n",
    "\n",
    "        if isinstance(token_list, str):\n",
    "            return self.vocab.numericalize([token_list])[0]\n",
    "        \n",
    "        return self.vocab.numericalize(token_list)\n",
    "    \n",
    "    def build_inputs_with_special_tokens(self, token_list):\n",
    "        # From https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_bert.py#L235\n",
    "        return [self.cls_token_id] + token_list + [self.sep_token_id]\n",
    "    \n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0, token_ids_1 = None, already_has_special_tokens = False\n",
    "    ):\n",
    "        # From https://huggingface.co/transformers/_modules/transformers/tokenization_utils.html#PreTrainedTokenizer.get_special_tokens_mask\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` method.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0: list of ids (must not contain special tokens)\n",
    "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
    "                for sequence pairs\n",
    "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
    "                special tokens for the model\n",
    "\n",
    "        Returns:\n",
    "            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "        return [0] * ((len(token_ids_1) if token_ids_1 else 0) + len(token_ids_0))\n",
    "    \n",
    "    def __len__(self):\n",
    "        #https://huggingface.co/transformers/_modules/transformers/tokenization_utils_fast.html#PreTrainedTokenizerFast.__len__\n",
    "        return len(self.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "CustomSeniorProjectTokenizer\n",
      "['ใคร', 'เคย', 'มี', 'แฟน', 'ที่', 'กิน', 'อาหาร', 'ไม่ถูกปาก', 'กัน', 'แล้ว', 'รู้สึก', 'เสีย', 'ความสุข', 'ไป', 'อย่าง', 'นึง', 'บ้าง', 'มั้ย', 'ครับ', ' ', 'ก่อนอื่น', 'ผม', 'ต้อง', 'บอก', 'ก่อน', 'เลย', 'ว่า', 'คนเรา', 'จะ', 'เลือก', 'กิน', 'อาหาร', 'แบบ', 'ไหน', 'ชอบ', 'แบบ', 'ไหน', 'เป็นเรื่อง', 'ของ', 'ความชอบ', 'ส่วนตัว', 'นะ', 'ครับ', 'ทุกคน', 'มี', 'สิทธิ', 'ใน', 'การเลือก', 'ของที่ชอบ', 'และ', 'ไม่ชอบ', 'อยู่แล้ว', ' ', 'แต่', 'ผม', 'รู้สึก', 'ว่า', 'ตอนนี้', 'ผม', 'กำลัง', 'ประสบปัญหา', 'ที่', 'ดูเหมือน', 'จะ', 'เล็ก', 'แต่', 'กลายเป็น', 'ว่า', 'มัน', 'ค่อนข้าง', 'ใหญ่', ' ', 'ผม', 'คบ', 'กับ', 'แฟน', 'มา', 'xxnum', ' ', 'ปี', 'แล้ว', 'ครับ', ' ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'กิน', 'อาหาร', 'ญี่ปุ่น', 'และ', 'ปลาดิบ', 'แต่', 'แฟน', 'ผม', 'ไม่', 'กิน', 'ปลาดิบ', 'เลย', ' ', 'ผม', 'อยากกิน', 'บุ', 'ฟเฟ่', 'เนื้อ', 'แต่', 'แฟน', 'ผม', 'ก็', 'ไม่', 'กิน', 'เนื้อ', ' ', 'เรา', 'เลย', 'ไม่ได้', 'เข้า', 'ทาน', 'ร้าน', 'บุ', 'ฟเฟ่', 'เนื้อ', 'และ', 'บุ', 'ฟเฟ่', 'อาหาร', 'ญี่ปุ่น', 'กัน', 'เพราะ', 'รู้สึก', 'ลัว', 'แฟน', 'ผม', 'ทาน', 'ไม่', 'คุ้ม', ' ', 'และ', 'เรื่องใหญ่', 'เลย', 'คือ', 'ผม', 'เป็น', 'คน', 'ชอบ', 'ทานอาหาร', 'รสจัด', 'และ', 'รส', 'เผ็ด', 'มาก', ' ', 'แต่', 'แฟน', 'ผม', 'ทาน', 'เผ็ด', 'ไม่ได้', 'เลยเวลา', 'เรา', 'ไป', 'กิน', 'ส้มตำ', 'กัน', 'ก็', 'จะ', 'สั่ง', ' ', 'ส้มตำ', 'ไม่', 'ใส่', 'พริก', ' ', 'ต้ม', 'แซ่บ', 'ไม่', 'ใส่', 'พริก', ' ', 'ลาบ', 'ไม่', 'ใส่', 'พริก', ' ', 'ร้าน', 'กับข้าว', 'อื่น ๆ', 'ก็', 'เช่นกัน', 'แฟน', 'ผม', 'จะ', 'ไม่ชอบ', 'กิน', 'ผัก', 'ไม่ค่อย', 'สั่ง', 'กับข้าว', 'ที่', 'เป็น', 'ผัก', 'แล้ว', 'ผม', 'ชอบ', 'ผักบุ้ง', 'ทอด', 'กรอบ', ' ', 'เห็ด', 'หอม', 'สด', 'ทอด', 'มาก', ' ', 'แต่', 'ก็', 'ไม่ได้', 'สั่ง', 'เพราะว่า', 'เธอ', 'ไม่', 'กิน', 'ถึง', 'เค้า', 'จะ', 'บอก', 'ให้', 'สั่ง', 'เลย', 'ๆ', 'ก็', 'เถอะ', 'แต่', 'ผม', 'ก็', 'ยัง', 'เกรงใจ', 'เธอ', 'อยู่ดี', 'อ่ะ', 'ครับ', ' ', 'ผม', 'รู้สึก', 'กิน', 'อาหาร', 'ไม่', 'มีความสุข', 'เลย', 'ชีวิต', 'ผม', 'ขาด', 'รส', 'เผ็ด', 'ไป', 'เหมือน', 'จะ', 'ขาดใจ', 'เหมือน', 'มัน', 'ทำให้', 'ขาด', 'ความสุข', 'ไป', 'อย่าง', 'นึง', 'เลย', 'อ่ะ', 'ครับ', ' ', 'ยิ่ง', 'ถ้า', 'เรา', 'แต่งงาน', 'กัน', 'แล้ว', 'ผม', 'ก็', 'อาจ', 'จะต้อง', 'มีปัญหา', 'เรื่อง', 'นี้', 'มากขึ้น', ' ', 'พอ', 'ผม', 'เห็น', 'คู่', 'ที่', 'ชอบ', 'ทานอาหาร', 'เหมือน', 'ๆ', 'กัน', 'เห็น', 'เค้า', 'กิน', 'อาหาร', 'กัน', 'อย่างมีความสุข', 'แล้ว', 'ผม', 'รู้สึก', 'อิจฉา', 'มาก', 'ๆ', 'เลย', ' ', 'มี', 'ใคร', 'เคย', 'มีปัญหา', 'แบบผม', 'มั้ย', 'ครับ', 'แล้', 'วจะ', 'แก้ปัญหา', 'นี้', 'ยังไง', 'ดี', 'ครับ']\n",
      "[74, 102, 16, 80, 10, 189, 420, 26577, 39, 26, 160, 317, 1807, 14, 128, 169, 67, 124, 15, 9, 2090, 25, 43, 77, 111, 22, 20, 1838, 13, 204, 189, 420, 58, 75, 86, 58, 75, 801, 31, 4208, 476, 66, 15, 288, 16, 1064, 30, 2784, 41364, 27, 576, 452, 9, 19, 25, 160, 20, 89, 25, 235, 5816, 10, 1614, 13, 648, 19, 634, 20, 37, 566, 270, 9, 25, 299, 33, 80, 17, 7, 9, 82, 26, 15, 9, 25, 21, 32, 86, 189, 420, 398, 27, 10631, 19, 80, 25, 24, 189, 10631, 22, 9, 25, 3038, 3270, 0, 713, 19, 80, 25, 11, 24, 189, 713, 9, 12, 22, 55, 116, 505, 173, 3270, 0, 713, 27, 3270, 0, 420, 398, 39, 52, 160, 24486, 80, 25, 505, 24, 922, 9, 27, 5716, 22, 42, 25, 21, 32, 86, 3502, 17080, 27, 2206, 4384, 40, 9, 19, 80, 25, 505, 4384, 55, 9411, 12, 14, 189, 5963, 39, 11, 13, 498, 9, 5963, 24, 164, 3399, 9, 2587, 3727, 24, 164, 3399, 9, 11352, 24, 164, 3399, 9, 173, 4525, 431, 11, 1044, 80, 25, 13, 576, 189, 1864, 216, 498, 4525, 10, 21, 1864, 26, 25, 86, 14336, 2020, 872, 9, 4279, 1733, 1430, 2020, 40, 9, 19, 11, 55, 498, 885, 145, 24, 189, 83, 56, 13, 77, 28, 498, 22, 29, 11, 1019, 19, 25, 11, 59, 3131, 145, 1501, 200, 15, 9, 25, 160, 189, 420, 24, 790, 22, 335, 25, 847, 2206, 4384, 14, 120, 13, 14014, 120, 37, 123, 847, 1807, 14, 128, 169, 22, 200, 15, 9, 319, 51, 12, 1090, 39, 26, 25, 11, 413, 402, 375, 62, 38, 652, 9, 69, 25, 107, 453, 10, 86, 3502, 120, 29, 39, 107, 56, 189, 420, 39, 8309, 26, 25, 160, 3531, 40, 29, 22, 9, 16, 74, 102, 375, 3136, 124, 15, 112, 486, 1884, 38, 93, 61, 15]\n",
      "[2, 74, 102, 16, 80, 10, 189, 420, 26577, 39, 26, 160, 317, 1807, 14, 128, 169, 67, 124, 15, 9, 2090, 25, 43, 77, 111, 22, 20, 1838, 13, 204, 189, 420, 58, 75, 86, 58, 75, 801, 31, 4208, 476, 66, 15, 288, 16, 1064, 30, 2784, 41364, 27, 576, 452, 9, 19, 25, 160, 20, 89, 25, 235, 5816, 10, 1614, 13, 648, 19, 634, 20, 37, 566, 270, 9, 25, 299, 33, 80, 17, 7, 9, 82, 26, 15, 9, 25, 21, 32, 86, 189, 420, 398, 27, 10631, 19, 80, 25, 24, 189, 10631, 22, 9, 25, 3038, 3270, 0, 713, 19, 80, 25, 11, 24, 189, 713, 9, 12, 22, 55, 116, 505, 173, 3270, 0, 713, 27, 3270, 0, 420, 398, 39, 52, 160, 24486, 80, 25, 505, 24, 922, 9, 27, 5716, 22, 42, 25, 21, 32, 86, 3502, 17080, 27, 2206, 4384, 40, 9, 19, 80, 25, 505, 4384, 55, 9411, 12, 14, 189, 5963, 39, 11, 13, 498, 9, 5963, 24, 164, 3399, 9, 2587, 3727, 24, 164, 3399, 9, 11352, 24, 164, 3399, 9, 173, 4525, 431, 11, 1044, 80, 25, 13, 576, 189, 1864, 216, 498, 4525, 10, 21, 1864, 26, 25, 86, 14336, 2020, 872, 9, 4279, 1733, 1430, 2020, 40, 9, 19, 11, 55, 498, 885, 145, 24, 189, 83, 56, 13, 77, 28, 498, 22, 29, 11, 1019, 19, 25, 11, 59, 3131, 145, 1501, 200, 15, 9, 25, 160, 189, 420, 24, 790, 22, 335, 25, 847, 2206, 4384, 14, 120, 13, 14014, 120, 37, 123, 847, 1807, 14, 128, 169, 22, 200, 15, 9, 319, 51, 12, 1090, 39, 26, 25, 11, 413, 402, 375, 62, 38, 652, 9, 69, 25, 107, 453, 10, 86, 3502, 120, 29, 39, 107, 56, 189, 420, 39, 8309, 26, 25, 160, 3531, 40, 29, 22, 9, 16, 74, 102, 375, 3136, 124, 15, 112, 486, 1884, 38, 93, 61, 15, 3]\n"
     ]
    }
   ],
   "source": [
    "text = \"ใครเคยมีแฟนที่กินอาหารไม่ถูกปากกันแล้วรู้สึกเสียความสุขไปอย่างนึงบ้างมั้ยครับ  ก่อนอื่นผมต้องบอกก่อนเลยว่าคนเราจะเลือกกินอาหารแบบไหนชอบแบบไหนเป็นเรื่องของความชอบส่วนตัวนะครับทุกคนมีสิทธิในการเลือกของที่ชอบและไม่ชอบอยู่แล้ว แต่ผมรู้สึกว่าตอนนี้ผมกำลังประสบปัญหาที่ดูเหมือนจะเล็กแต่กลายเป็นว่ามันค่อนข้างใหญ่ ผมคบกับแฟนมา6ปีแล้วครับ ผมเป็นคนชอบกินอาหารญี่ปุ่นและปลาดิบแต่แฟนผมไม่กินปลาดิบเลย ผมอยากกินบุฟเฟ่เนื้อแต่แฟนผมก็ไม่กินเนื้อ เราเลยไม่ได้เข้าทานร้านบุฟเฟ่เนื้อและบุฟเฟ่อาหารญี่ปุ่นกันเพราะรู้สึกลัวแฟนผมทานไม่คุ้ม และเรื่องใหญ่เลยคือผมเป็นคนชอบทานอาหารรสจัดและรสเผ็ดมาก แต่แฟนผมทานเผ็ดไม่ได้เลยเวลาเราไปกินส้มตำกันก็จะสั่ง ส้มตำไม่ใส่พริก ต้มแซ่บไม่ใส่พริก ลาบไม่ใส่พริก ร้านกับข้าวอื่นๆก็เช่นกันแฟนผมจะไม่ชอบกินผักไม่ค่อยสั่งกับข้าวที่เป็นผักแล้วผมชอบผักบุ้งทอดกรอบ เห็ดหอมสดทอดมาก แต่ก็ไม่ได้สั่งเพราะว่าเธอไม่กินถึงเค้าจะบอกให้สั่งเลยๆก็เถอะแต่ผมก็ยังเกรงใจเธออยู่ดีอ่ะครับ ผมรู้สึกกินอาหารไม่มีความสุขเลยชีวิตผมขาดรสเผ็ดไปเหมือนจะขาดใจเหมือนมันทำให้ขาดความสุขไปอย่างนึงเลยอ่ะครับ ยิ่งถ้าเราแต่งงานกันแล้วผมก็อาจจะต้องมีปัญหาเรื่องนี้มากขึ้น พอผมเห็นคู่ที่ชอบทานอาหารเหมือนๆกันเห็นเค้ากินอาหารกันอย่างมีความสุขแล้วผมรู้สึกอิจฉามากๆเลย มีใครเคยมีปัญหาแบบผมมั้ยครับแล้วจะแก้ปัญหานี้ยังไงดีครับ\"\n",
    "tokenizer = CustomSeniorProjectTokenizer()\n",
    "print(tokenizer.num_special_tokens_to_add(pair=False))\n",
    "print(tokenizer.__class__.__name__)\n",
    "value = tokenizer.tokenize(text)\n",
    "print(value)\n",
    "value = tokenizer.convert_tokens_to_ids(value)\n",
    "print(value)\n",
    "value = tokenizer.build_inputs_with_special_tokens(value)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing tokenizer wrapper based on [@theblackcat102 #259](https://github.com/huggingface/tokenizers/issues/259#issuecomment-625905930)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building our dataset\n",
    "\n",
    "Build it with `from torch.utils.data.dataset import Dataset` just like [TextDataset](https://github.com/huggingface/transformers/blob/448c467256332e4be8c122a159b482c1ef039b98/src/transformers/data/datasets/language_modeling.py) and [LineByLineTextDataset](https://github.com/huggingface/transformers/blob/448c467256332e4be8c122a159b482c1ef039b98/src/transformers/data/datasets/language_modeling.py#L78)\n",
    "\n",
    "Note: Training with multiple files is currently not supported [issue/3445](https://github.com/huggingface/transformers/issues/3445)\n",
    "\n",
    "padding documentation [link](https://github.com/huggingface/tokenizers/blob/master/bindings/python/tokenizers/implementations/base_tokenizer.py#L52)\n",
    "\n",
    "Potential Improvements\n",
    "- การทำให้ Dataset นั้น dynamically tokenize + dynamically open file : ตอนนี้เวลาทำ Dataset จาก torch.utils.data.dataset จะทำการ tokenize เลยตอนอยู่ใน constructor  , กำลังคิดว่าถ้าเกิดว่า Data ใหญ่มากๆ อาจจะไม่เหมาะสมกับการทำแบบนี้  เพราะว่า Ram จะต้องมีขนาดเท่าๆกับ data ที่เราใส่เข้าไป  ซึ่งเป็นไปได้ยากหาก Data มีขนาดใหญ่มากๆ   ผมได้ทำการ Search ดูแล้วก็พบว่าจาก Discussion Forum ของ Pytorch: https://discuss.pytorch.org/t/how-to-use-a-huge-line-corpus-text-with-dataset-dataloader/30872 \n",
    "Option1: ใช้ pd.Dataframe ในการเปิด File แบบ small chunks of data https://discuss.pytorch.org/t/data-processing-as-a-batch-way/14154/4?u=ptrblck\n",
    "Option2: ใช้ byte Offsets จากไฟล์ใหญ่ๆเพื่อที่จะ lookup .seek(): https://github.com/pytorch/text/issues/130#issuecomment-510412877\n",
    "More Examples: https://github.com/pytorch/text/blob/master/torchtext/datasets/unsupervised_learning.py , https://github.com/pytorch/text/blob/a5880a3da7928dd7dd529507eec943a307204de7/examples/text_classification/iterable_train.py#L169-L214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "class TextDatasetParallel(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"         \n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, sample_path: [], block_size: int, overwrite_cache=False,\n",
    "                num_processes=8, cached_directory = \"/workdir/Code/bma_transformer_model/data/cached_data\"):\n",
    "        # assert os.path.isfile(file_path)\n",
    "        # For Loop MultiFile\n",
    "        self.examples = []\n",
    "        self.sample_path = sample_path\n",
    "#         print(f\"THIS IS SAMPLE PATH {sample_path}\")\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Set block size to be the blocksize-special tokens\n",
    "        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n",
    "        \n",
    "        self.overwrite_cache = overwrite_cache\n",
    "        self.cached_directory = cached_directory\n",
    "        if not os.path.exists(cached_directory):\n",
    "            os.makedirs(cached_directory)\n",
    "        \n",
    "        # Multiprocess for getting examples\n",
    "        with Pool(processes=num_processes) as p:\n",
    "            self.examples = list(tqdm.tqdm(p.imap(self.load_data_tokenized, self.sample_path), total=len(self.sample_path)))\n",
    "#         with Pool(max_workers=num_processes) as p:\n",
    "#             self.examples = list(tqdm.tqdm(p.map(self.load_data_tokenized, self.sample_path), total=len(self.sample_path)))\n",
    "#         for path in tqdm.tqdm(self.sample_path):\n",
    "#             self.examples.append(self.load_data_tokenized(path))\n",
    "        \n",
    "        \n",
    "        # Convert from 3d list to 2d \n",
    "        # self.examples from [[[3], [4]], [[5], [6]], [[7], [8]]] => [[3], [4], [5], [6], [7], [8]]\n",
    "        self.examples = [each_batch for each_file in self.examples for each_batch in each_file]\n",
    "        \n",
    "\n",
    "    def load_data_tokenized(self, file_path):\n",
    "#         print(f\"I AM DOING {file_path}\")\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            self.cached_directory, f\"cached_lm_{tokenizer.__class__.__name__}_{str(self.block_size)}_{filename}\",\n",
    "        )\n",
    "\n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "            if os.path.exists(cached_features_file) and not self.overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    temp_examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "#                 print(f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start)\n",
    "            else:\n",
    "                temp_examples = []\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "#                 print(\"I finished reading \", file_path)\n",
    "                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "#                 print(\"I finished tokenizing \", file_path)\n",
    "                for i in range(0, len(tokenized_text) - self.block_size + 1, self.block_size):  # Truncate in block of block_size\n",
    "                    temp_examples.append(\n",
    "                        tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + self.block_size])\n",
    "                    )\n",
    "#                     if i%20 == 0:\n",
    "#                         print(\"I finished special tok \", file_path)\n",
    "#                 print(\"I finished every tokenizing \", file_path)\n",
    "                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n",
    "                # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
    "                # can change this behavior by adding (model specific) padding.\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(temp_examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "        return temp_examples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(ALL_FILES[0], encoding=\"utf-8\") as f:\n",
    "#     text = f.read()\n",
    "# len(text)/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer from Pretrained copied from SimpleTransformers [link](https://github.com/ThilinaRajapakse/simpletransformers/blob/master/simpletransformers/language_modeling/language_modeling_model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1409/1409 [01:29<00:00, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 20.8 s, total: 1min 32s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# logging.basicConfig(level=logging.WARN)\n",
    "tokenizer = CustomSeniorProjectTokenizer()\n",
    "dataset = TextDatasetParallel(tokenizer, \n",
    "                              sample_path=list(map(str, ALL_FILES)), \n",
    "#                               sample_path=list(map(str, GURU_CRAWLER_FILES)), \n",
    "                              block_size=512, \n",
    "                              cached_directory= \"/workdir/cached_data_senior\",\n",
    "                              overwrite_cache=False, # make sure this is false when you have cache!!\n",
    "                              num_processes=60,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cached_directory= \"/workdir/cached_data\"\n",
    "# def load_data_tokenized(file_path):\n",
    "#     directory, filename = os.path.split(file_path)\n",
    "#     cached_features_file = os.path.join(\n",
    "#         cached_directory, f\"cached_lm_something_{str(123)}_{filename}\",\n",
    "#     )\n",
    "#     temp_examples = []\n",
    "#     with open(file_path, encoding=\"utf-8\") as f:\n",
    "#         text = f.read()\n",
    "#     print(\"I finished reading \", file_path)\n",
    "#     tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[:1000]))\n",
    "#     print(\"I finished tokenizing \", tokenized_text[:100])\n",
    "#     return f\"{file_path}+123\"\n",
    "# # Multiprocess for getting examples\n",
    "# with Pool(processes=2) as p:\n",
    "#     examples = list(tqdm.tqdm(p.imap(load_data_tokenized, list(map(str, GURU_CRAWLER_FILES))), total=len(list(map(str, GURU_CRAWLER_FILES)))))\n",
    "# print(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1537686"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2, 28125,     9, 14966,     9,  5799,     9,    36,   247,  4240,\n",
       "           20,     0,     9,    21,     0,    10,    21, 10444,    30,  4957,\n",
       "        15773,     9,   347,   310, 37854,  5532,     9,  2720,   714,     7,\n",
       "            9,   714,    97,   327,   310,  1621, 17698,   270,     9,     7,\n",
       "            9,  1621,    50,  1566,  7302,  1003,     9,  2504,     9,  1894,\n",
       "         9041,     9,  1127,  1041, 48540,     9,    27,     0,  5708,    31,\n",
       "          409,    42, 22118,   177,   455,   249,  4636,  1175,    42,  3385,\n",
       "         1296,  9041,     9,   950,  4875,    31, 21850, 22118,  5799,    16,\n",
       "        17646,   366,    33,  1210,   642,  1003, 26270,     9,  1210, 20640,\n",
       "        18700, 26200,     9,    27,  1210,  1561,   280,  1160,  2031,   119,\n",
       "        50601,     9,  1720, 16622, 14964,  2166,  2885, 27984,     9,     0,\n",
       "        26200,     9,  3200,    27,  5886, 28756,     9,    27,  1972, 26270,\n",
       "            9, 14966,     9,  5799,   347,   310,  2311,  9955,   386,   585,\n",
       "           27,   364,    33, 46589,     9,   123,  5431,   326,   188, 41113,\n",
       "           50, 10290,    27, 26156,     9,    19,    11,   123,    16,   135,\n",
       "        20130,    10, 16645,    27, 53916,  5090,  1044,     9,  5799,    16,\n",
       "         5544,   117,     9,     7,     9, 27407,     9,    34,     7,     9,\n",
       "            0,    35,     9,    27,    16,  4636,   117,     9,     7,     9,\n",
       "          391,    32,     9,   902,    21,   409,   249,  4636,  1175,    21,\n",
       "          978,    10,     9,     7,     9,    30,  1481,     9,    27,    21,\n",
       "          409,   249,  4636,  1175,    21,   978,    10,     9,     7,     9,\n",
       "           31,   435,     9,  1911,     9,   555,     9,    82,  1620,     9,\n",
       "            7,     9,   567,   753,  5799,    68,   117,     9,     7,     9,\n",
       "          391,    32,  1634,   310,   845,     9,   460,    26,  2089,   312,\n",
       "            0,    10,   270,   356,   312,   199,    31,   435,     9,  8566,\n",
       "          119, 23188,    27,  3219,   674,    28,   107,  7858, 10444,    30,\n",
       "        63549,  1174,   312,   203,    10,   238, 27433,    30, 10444,   386,\n",
       "           38,    42, 41037,  1172, 13166,   932,     9,  1677,    44, 41037,\n",
       "         1395,  2031,   932, 62297,  3012, 32706,   238,  3763,     9,    16,\n",
       "         2914,  2939,    33,  7327,     9, 23995,  2790,     9,    27,  9044,\n",
       "            9,   787,    11,   219, 44190, 14964,    99,    17,   126,   386,\n",
       "         3535,  4701,    31,  1413,   494,   765,    39,     9,  6421,    36,\n",
       "        31571,     9, 14966,     9,  4858,  1263,   237, 24286,    17,  3791,\n",
       "        13713,     9,   823, 16809,    10,   714,  2673, 17269,  2673,  1045,\n",
       "            9,    34,   364,    33,   714,  1041,  3948, 27206,     9,    30,\n",
       "           82,  1620,     9,     7,     9,    34,  6242,     9,     7,    35,\n",
       "            9,   902,    21,  4219,    31,  1206,   386,  8678,    27,  2106,\n",
       "           31,  4352,     9,    30,    82,  1620,     9,     7,     9,    34,\n",
       "         6242,     9,     7,    35,     9, 36505, 32576,   247,     9, 34296,\n",
       "            9,  1425, 40932,     9,  2103,     9,  2925,   440,  1425,  2616,\n",
       "         2031,     9,    18,  4465,   714,  1041,  3948,  1519,    27,   280,\n",
       "         3756,  2588,    20,    60, 10444,  1468,  2812,     0,     9,    36,\n",
       "            9,    60, 11993,     0, 75056,     0,  9509,   113, 18871,  6415,\n",
       "         5948,  1468,  2812,  2907,   386, 11942,   494, 32082,     9,    34,\n",
       "          798, 11993,     0, 75056, 17566,    18,   634, 11811,     0,   399,\n",
       "           35,     9,    30,    82,  1620,     9,     7,     9,    34,  6242,\n",
       "            9,     7,    35,     9,   899, 13234,     9,  1425, 40932,     9,\n",
       "         2103,     9,     0,    82,    18,  7165,  9637,  4352,   386,   203,\n",
       "          268,   714,  1561,  3505,  5799,  2667,  4875,    31, 14284,  4352,\n",
       "         1814,   237,    64,     7,     9,    82,  4188, 14448,  9692, 46265,\n",
       "          634,  2490,   520,    31,   253,  1109, 10444,    30,   217,  3762,\n",
       "            9,     3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxbos',\n",
       " 'ประเทศฟิลิปปินส์',\n",
       " ' ',\n",
       " '\\n',\n",
       " ' ',\n",
       " 'ฟิลิปปินส์',\n",
       " ' ',\n",
       " 'หรือ',\n",
       " 'ชื่อ',\n",
       " 'ทางการ',\n",
       " 'ว่า',\n",
       " 'xxunk',\n",
       " ' ',\n",
       " 'เป็น',\n",
       " 'xxunk',\n",
       " 'ที่',\n",
       " 'เป็น',\n",
       " 'หมู่เกาะ',\n",
       " 'ใน',\n",
       " 'ภูมิภาค',\n",
       " 'เอเชียตะวันออกเฉียงใต้',\n",
       " ' ',\n",
       " 'ตั้ง',\n",
       " 'อยู่ใน',\n",
       " 'มหาสมุทรแปซิฟิก',\n",
       " 'ตะวันตก',\n",
       " ' ',\n",
       " 'ประกอบด้วย',\n",
       " 'เกาะ',\n",
       " 'xxnum',\n",
       " ' ',\n",
       " 'เกาะ',\n",
       " 'ซึ่ง',\n",
       " 'จัด',\n",
       " 'อยู่ใน',\n",
       " 'เขต',\n",
       " 'ภูมิศาสตร์',\n",
       " 'ใหญ่',\n",
       " ' ',\n",
       " 'xxnum',\n",
       " ' ',\n",
       " 'เขต',\n",
       " 'จาก',\n",
       " 'เหนือ',\n",
       " 'จรด',\n",
       " 'ไต้',\n",
       " ' ',\n",
       " 'ได้แก่',\n",
       " ' ',\n",
       " 'ลู',\n",
       " 'ซอน',\n",
       " ' ',\n",
       " 'วิ',\n",
       " 'ซา',\n",
       " 'ยัส',\n",
       " ' ',\n",
       " 'และ',\n",
       " 'xxunk',\n",
       " 'เมืองหลวง',\n",
       " 'ของ',\n",
       " 'ประเทศ',\n",
       " 'คือ',\n",
       " 'มะนิลา',\n",
       " 'ส่วน',\n",
       " 'เมือง',\n",
       " 'ที่มี',\n",
       " 'ประชากร',\n",
       " 'มากที่สุด',\n",
       " 'คือ',\n",
       " 'นคร',\n",
       " 'เก',\n",
       " 'ซอน',\n",
       " ' ',\n",
       " 'ทั้งสอง',\n",
       " 'เป็นส่วนหนึ่ง',\n",
       " 'ของ',\n",
       " 'เมโทร',\n",
       " 'มะนิลา',\n",
       " 'ฟิลิปปินส์',\n",
       " 'มี',\n",
       " 'อาณาเขต',\n",
       " 'ติดต่อ',\n",
       " 'กับ',\n",
       " 'ทะเล',\n",
       " 'จีน',\n",
       " 'ไต้',\n",
       " 'ทางทิศตะวันตก',\n",
       " ' ',\n",
       " 'ทะเล',\n",
       " 'ฟิลิป',\n",
       " 'ปิน',\n",
       " 'ทางทิศตะวันออก',\n",
       " ' ',\n",
       " 'และ',\n",
       " 'ทะเล',\n",
       " 'เซ',\n",
       " 'เล',\n",
       " 'บี',\n",
       " 'ส',\n",
       " 'ทาง',\n",
       " 'ทิศตะวันตกเฉียงใต้',\n",
       " ' ',\n",
       " 'โดยมี',\n",
       " 'พรมแดน',\n",
       " 'ทางทะเล',\n",
       " 'ร่วมกับ',\n",
       " 'ไต้หวัน',\n",
       " 'ทางทิศเหนือ',\n",
       " ' ',\n",
       " 'xxunk',\n",
       " 'ทางทิศตะวันออก',\n",
       " ' ',\n",
       " 'มาเลเซีย',\n",
       " 'และ',\n",
       " 'อินโดนีเซีย',\n",
       " 'ทางทิศใต้',\n",
       " ' ',\n",
       " 'และ',\n",
       " 'เวียดนาม',\n",
       " 'ทางทิศตะวันตก',\n",
       " ' ',\n",
       " '\\n',\n",
       " ' ',\n",
       " 'ฟิลิปปินส์',\n",
       " 'ตั้ง',\n",
       " 'อยู่ใน',\n",
       " 'แถบ',\n",
       " 'วงแหวน',\n",
       " 'แห่ง',\n",
       " 'ไฟ',\n",
       " 'และ',\n",
       " 'ใกล้',\n",
       " 'กับ',\n",
       " 'เส้นศูนย์สูตร',\n",
       " ' ',\n",
       " 'ทำให้',\n",
       " 'มีแนวโน้ม',\n",
       " 'สูง',\n",
       " 'ที่จะ',\n",
       " 'ประสบภัย',\n",
       " 'จาก',\n",
       " 'แผ่นดินไหว',\n",
       " 'และ',\n",
       " 'ไต้ฝุ่น',\n",
       " ' ',\n",
       " 'แต่',\n",
       " 'ก็',\n",
       " 'ทำให้',\n",
       " 'มี',\n",
       " 'ทั้ง',\n",
       " 'ทรัพยากรธรรมชาติ',\n",
       " 'ที่',\n",
       " 'อุดมสมบูรณ์',\n",
       " 'และ',\n",
       " 'ความหลากหลายทางชีวภาพ',\n",
       " 'อย่างยิ่ง',\n",
       " 'เช่นกัน',\n",
       " ' ',\n",
       " 'ฟิลิปปินส์',\n",
       " 'มี',\n",
       " 'เนื้อที่',\n",
       " 'ประมาณ',\n",
       " ' ',\n",
       " 'xxnum',\n",
       " ' ',\n",
       " 'ตารางกิโลเมตร',\n",
       " ' ',\n",
       " '(',\n",
       " 'xxnum',\n",
       " ' ',\n",
       " 'xxunk',\n",
       " ')',\n",
       " ' ',\n",
       " 'และ',\n",
       " 'มี',\n",
       " 'ประชากร',\n",
       " 'ประมาณ',\n",
       " ' ',\n",
       " 'xxnum',\n",
       " ' ',\n",
       " 'ล้าน',\n",
       " 'คน',\n",
       " ' ',\n",
       " 'นับ',\n",
       " 'เป็น',\n",
       " 'ประเทศ',\n",
       " 'ที่มี',\n",
       " 'ประชากร',\n",
       " 'มากที่สุด',\n",
       " 'เป็น',\n",
       " 'อันดับ',\n",
       " 'ที่',\n",
       " ' ',\n",
       " 'xxnum',\n",
       " ' ',\n",
       " 'ใน',\n",
       " 'เอเชีย',\n",
       " ' ',\n",
       " 'และ',\n",
       " 'เป็น',\n",
       " 'ประเทศ',\n",
       " 'ที่มี',\n",
       " 'ประชากร',\n",
       " 'มากที่สุด',\n",
       " 'เป็น',\n",
       " 'อันดับ',\n",
       " 'ที่',\n",
       " ' ',\n",
       " 'xxnum',\n",
       " ' ',\n",
       " 'ของ',\n",
       " 'โลก',\n",
       " ' ',\n",
       " 'นอกจากนี้',\n",
       " ' ',\n",
       " 'ณ',\n",
       " ' ',\n",
       " 'ปี',\n",
       " 'พ.ศ.',\n",
       " ' ',\n",
       " 'xxnum',\n",
       " ' ',\n",
       " 'ยังมี',\n",
       " 'ชาว',\n",
       " 'ฟิลิปปินส์',\n",
       " 'อีก',\n",
       " 'ประมาณ',\n",
       " ' ',\n",
       " 'xxnum',\n",
       " ' ',\n",
       " 'ล้าน',\n",
       " 'คน',\n",
       " 'อาศัย',\n",
       " 'อยู่ใน',\n",
       " 'ต่างประเทศ',\n",
       " ' ',\n",
       " 'รวม',\n",
       " 'แล้ว',\n",
       " 'ถือเป็น',\n",
       " 'กลุ่ม',\n",
       " 'xxunk',\n",
       " 'ที่',\n",
       " 'ใหญ่',\n",
       " 'ที่สุด',\n",
       " 'กลุ่ม',\n",
       " 'หนึ่ง',\n",
       " 'ของ',\n",
       " 'โลก',\n",
       " ' ',\n",
       " 'ความหลากหลาย',\n",
       " 'ทาง',\n",
       " 'ชาติพันธุ์',\n",
       " 'และ',\n",
       " 'วัฒนธรรม',\n",
       " 'ปรากฏ',\n",
       " 'ให้',\n",
       " 'เห็น',\n",
       " 'ตลอดทั้ง',\n",
       " 'หมู่เกาะ',\n",
       " 'ใน',\n",
       " 'ยุคก่อนประวัติศาสตร์',\n",
       " 'มนุษย์',\n",
       " 'กลุ่ม',\n",
       " 'แรก',\n",
       " 'ที่',\n",
       " 'เข้ามา',\n",
       " 'ตั้งรกราก',\n",
       " 'ใน',\n",
       " 'หมู่เกาะ',\n",
       " 'แห่ง',\n",
       " 'นี้',\n",
       " 'คือ',\n",
       " 'กลุ่มชน',\n",
       " 'นิ',\n",
       " 'กรี',\n",
       " 'โต',\n",
       " ' ',\n",
       " 'ตามมา',\n",
       " 'ด้วย',\n",
       " 'กลุ่มชน',\n",
       " 'ออ',\n",
       " 'ส',\n",
       " 'โต',\n",
       " 'รนี',\n",
       " 'เซียน',\n",
       " 'ที่อพยพ',\n",
       " 'เข้ามา',\n",
       " 'อย่างต่อเนื่อง',\n",
       " ' ',\n",
       " 'มี',\n",
       " 'การติดต่อ',\n",
       " 'แลกเปลี่ยน',\n",
       " 'กับ',\n",
       " 'ชาวจีน',\n",
       " ' ',\n",
       " 'มลายู',\n",
       " 'อินเดีย',\n",
       " ' ',\n",
       " 'และ',\n",
       " 'อาหรับ',\n",
       " ' ',\n",
       " 'จากนั้น',\n",
       " 'ก็',\n",
       " 'เกิด',\n",
       " 'นครรัฐ',\n",
       " 'ทางทะเล',\n",
       " 'ขึ้น',\n",
       " 'มา',\n",
       " 'หลาย',\n",
       " 'แห่ง',\n",
       " 'ภายใต้',\n",
       " 'การปกครอง',\n",
       " 'ของ',\n",
       " 'ดา',\n",
       " 'กู',\n",
       " 'ลา',\n",
       " 'กัน',\n",
       " ' ',\n",
       " 'ราชา',\n",
       " 'หรือ',\n",
       " 'สุลต่าน',\n",
       " ' ',\n",
       " '\\n',\n",
       " ' ',\n",
       " 'เฟอร์',\n",
       " 'ดิ',\n",
       " 'นาน',\n",
       " 'ด์',\n",
       " 'มา',\n",
       " 'เจล',\n",
       " 'ลัน',\n",
       " ' ',\n",
       " 'ได้มา',\n",
       " 'ขึ้นฝั่ง',\n",
       " 'ที่',\n",
       " 'เกาะ',\n",
       " 'โฮ',\n",
       " 'โมน',\n",
       " 'โฮ',\n",
       " 'น',\n",
       " ' ',\n",
       " '(',\n",
       " 'ใกล้',\n",
       " 'กับ',\n",
       " 'เกาะ',\n",
       " 'ซา',\n",
       " 'มาร',\n",
       " '์)',\n",
       " ' ',\n",
       " 'ใน',\n",
       " 'ปี',\n",
       " 'พ.ศ.',\n",
       " ' ',\n",
       " 'xxnum',\n",
       " ' ',\n",
       " '(',\n",
       " 'ค.ศ.',\n",
       " ' ',\n",
       " 'xxnum',\n",
       " ')',\n",
       " ' ',\n",
       " 'นับ',\n",
       " 'เป็น',\n",
       " 'จุดเริ่มต้น',\n",
       " 'ของ',\n",
       " 'ยุค',\n",
       " 'แห่ง',\n",
       " 'อิทธิพล',\n",
       " 'และ',\n",
       " 'อำนาจ',\n",
       " 'ของ',\n",
       " 'สเปน',\n",
       " ' ',\n",
       " 'ใน',\n",
       " 'ปี',\n",
       " 'พ.ศ.',\n",
       " ' ',\n",
       " 'xxnum',\n",
       " ' ',\n",
       " '(',\n",
       " 'ค.ศ.',\n",
       " ' ',\n",
       " 'xxnum',\n",
       " ')',\n",
       " ' ',\n",
       " 'นักสำรวจ',\n",
       " 'ชาวสเปน',\n",
       " 'ชื่อ',\n",
       " ' ',\n",
       " 'รุย',\n",
       " ' ',\n",
       " 'โล',\n",
       " 'เปซ',\n",
       " ' ',\n",
       " 'เด',\n",
       " ' ',\n",
       " 'บิ',\n",
       " 'ยา',\n",
       " 'โล',\n",
       " 'โบ',\n",
       " 'ส',\n",
       " ' ',\n",
       " 'ได้',\n",
       " 'ตั้งชื่อ',\n",
       " 'เกาะ',\n",
       " 'ซา',\n",
       " 'มาร',\n",
       " '์',\n",
       " 'และ',\n",
       " 'เล',\n",
       " 'เต',\n",
       " 'รวมกัน',\n",
       " 'ว่า',\n",
       " '\"',\n",
       " 'หมู่เกาะ',\n",
       " 'เฟ',\n",
       " 'ลี',\n",
       " 'xxunk',\n",
       " ' ',\n",
       " 'หรือ',\n",
       " ' ',\n",
       " '\"',\n",
       " 'อิส',\n",
       " 'xxunk',\n",
       " 'ลิปิ',\n",
       " 'xxunk',\n",
       " '  ',\n",
       " 'เพื่อ',\n",
       " 'เป็นเกียรติ',\n",
       " 'แด่',\n",
       " 'เจ้าชาย',\n",
       " 'เฟ',\n",
       " 'ลี',\n",
       " 'เป',\n",
       " 'แห่ง',\n",
       " 'อัส',\n",
       " 'กู',\n",
       " 'เรียส',\n",
       " ' ',\n",
       " '(',\n",
       " 'ต่อมา',\n",
       " 'อิส',\n",
       " 'xxunk',\n",
       " 'ลิปิ',\n",
       " 'นัส',\n",
       " 'ได้',\n",
       " 'กลายเป็น',\n",
       " 'ชื่อเรียก',\n",
       " 'xxunk',\n",
       " 'ทั้งหมด',\n",
       " ')',\n",
       " ' ',\n",
       " 'ใน',\n",
       " 'ปี',\n",
       " 'พ.ศ.',\n",
       " ' ',\n",
       " 'xxnum',\n",
       " ' ',\n",
       " '(',\n",
       " 'ค.ศ.',\n",
       " ' ',\n",
       " 'xxnum',\n",
       " ')',\n",
       " ' ',\n",
       " 'มิ',\n",
       " 'เกล',\n",
       " ' ',\n",
       " 'โล',\n",
       " 'เปซ',\n",
       " ' ',\n",
       " 'เด',\n",
       " ' ',\n",
       " 'xxunk',\n",
       " 'ปี',\n",
       " 'ได้',\n",
       " 'จัดตั้ง',\n",
       " 'นิคม',\n",
       " 'สเปน',\n",
       " 'แห่ง',\n",
       " 'แรก',\n",
       " 'บน',\n",
       " 'เกาะ',\n",
       " 'เซ',\n",
       " 'บู',\n",
       " 'ฟิลิปปินส์',\n",
       " 'กลาย',\n",
       " 'เป็นส่วนหนึ่ง',\n",
       " 'ของ',\n",
       " 'จักรวรรดิ',\n",
       " 'สเปน',\n",
       " 'เป็นเวลา',\n",
       " 'นาน',\n",
       " 'กว่า',\n",
       " 'xxnum',\n",
       " ' ',\n",
       " 'ปี',\n",
       " 'ส่งผลให้',\n",
       " 'ศาสนาคริสต์',\n",
       " 'นิกาย',\n",
       " 'โรมันคาทอลิก',\n",
       " 'กลายเป็น',\n",
       " 'ศาสนา',\n",
       " 'หลัก',\n",
       " 'ของ',\n",
       " 'ผู้',\n",
       " 'คนใน',\n",
       " 'หมู่เกาะ',\n",
       " 'ใน',\n",
       " 'ช่วง',\n",
       " 'เวลานี้',\n",
       " ' ',\n",
       " 'xxeos']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: itos[x], dataset.__getitem__(0).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(text[:1000])\n",
    "# print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[:1000])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For text[:100000]__  \n",
    "\n",
    "Rust implementation\n",
    ">\n",
    "\n",
    "Python\n",
    ">CPU times: user 900 ms, sys: 4 ms, total: 904 ms\n",
    "Wall time: 903 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[:100000]))\n",
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For text[:1000000]__  \n",
    "\n",
    "Rust implementation\n",
    ">\n",
    "\n",
    "Python\n",
    ">CPU times: user 7.27 s, sys: 40 ms, total: 7.31 s\n",
    "Wall time: 7.31 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[:1000000]))\n",
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For text[:3000000]__  \n",
    "\n",
    "Rust implementation\n",
    ">CPU times: user 6.38 s, sys: 328 ms, total: 6.7 s  \n",
    "Wall time: 5.18 s\n",
    "\n",
    "Python\n",
    ">CPU times: user 15.5 s, sys: 72 ms, total: 15.6 s  \n",
    "Wall time: 15.6 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[:3000000]))\n",
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For text[:8000000]__  \n",
    "\n",
    "Rust implementation\n",
    ">\n",
    "\n",
    "Python\n",
    ">CPU times: user 36.1 s, sys: 340 ms, total: 36.4 s  \n",
    "Wall time: 36.4 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[:8000000]))\n",
    "# None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i_batch, sample_batched in enumerate(dataloader):\n",
    "#     print(i_batch, sample_batched)\n",
    "#     oumodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = CharBPETokenizer(vocab_file='vocab.json',merges_file ='merges.txt' )\n",
    "# no_accent_strip = BertNormalizer(strip_accents=False)\n",
    "# tokenizer._tokenizer.normalizer = no_accent_strip\n",
    "# tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "#     (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "#     (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "# )\n",
    "\n",
    "# input_ids = torch.tensor(tokenizer.encode(u\"สวัสดีครับ ผมชื่อไนท์ ตอนนี้ก็เป็นเวลาที่ผมต้องไปโรงเรียนแล้ว  นี่คือการเว้นวรรคสองทีครับ  จะได้ออกเป็นสอง Spaces\").ids).unsqueeze(0)\n",
    "# print(input_ids)\n",
    "# outputs = model(input_ids, labels=input_ids)\n",
    "# print(outputs)\n",
    "# loss, prediction_scores = outputs[:2]\n",
    "# print(loss, prediction_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.__getitem__(1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from transformers import TextDataset, LineByLineTextDataset\n",
    "\n",
    "# # dataset = LineByLineTextDataset(\n",
    "# #     tokenizer=pretrain_tokenizer,\n",
    "# #     file_path=\"../data/text/AA/wiki_01\",\n",
    "# #     block_size=128,\n",
    "# # )\n",
    "\n",
    "# dataset = TextDataset(\n",
    "#     tokenizer=pretrain_tokenizer,\n",
    "#     file_path=\"../data/text/AA/wiki_01\",\n",
    "#     block_size=128,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_doc = list(Path(\"../data/text/AA/\").glob(\"wiki*\"))[0].read_text(encoding=\"utf-8\").splitlines()\n",
    "# tokenizer = Tokenizer.from_file(\"./thwiki-sentencepiecebpe.tokenizer.json\")\n",
    "# tokenizer.encode_batch(one_doc[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_doc = list(Path(\"../data/text/AA/\").glob(\"wiki*\"))[0].read_text(encoding=\"utf-8\").splitlines()\n",
    "# tokenizer = RobertaTokenizerFast(vocab_file='vocab.json',merges_file ='merges.txt', max_len=512)\n",
    "# tokenizer.batch_encode_plus(one_doc[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(tokenizer.encode_batch(one_doc[:8])[5].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_doc[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfomers Trainer [link](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L133)\n",
    "\n",
    "```python\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer is a simple but feature-complete training and eval loop for PyTorch,\n",
    "    optimized for Transformers.\n",
    "    Args:\n",
    "        prediction_loss_only:\n",
    "            (Optional) in evaluation and prediction, only return the loss\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        args: TrainingArguments,\n",
    "        data_collator: Optional[DataCollator] = None,\n",
    "        train_dataset: Optional[Dataset] = None,\n",
    "        eval_dataset: Optional[Dataset] = None,\n",
    "        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n",
    "        prediction_loss_only=False,\n",
    "        tb_writer: Optional[\"SummaryWriter\"] = None,\n",
    "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = None,\n",
    "```\n",
    "\n",
    "[TrainingArguments](https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py#L33) is referenced here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./OriginalBert\",\n",
    "    overwrite_output_dir=False,  #\"Use this to continue training if output_dir points to a checkpoint directory.\"\n",
    "    \n",
    "    fp16=True,\n",
    "    fp16_opt_level='O1',\n",
    "    \n",
    "    \n",
    "    do_train=True, #Whether to run training.\n",
    "#     do_eval=True, #Whether to run eval on the dev set.\n",
    "#     do_predict=True, # Whether to run predictions on the test set.\n",
    "    \n",
    "    num_train_epochs=200, # Total number of training epochs to perform.\n",
    "    \n",
    "    \n",
    "    per_device_train_batch_size=6, # Batch size per GPU/TPU core/CPU for training.\n",
    "#     per_device_eval_batch_size=256, # Batch size per GPU/TPU core/CPU for evaluation.\n",
    "    \n",
    "    learning_rate=5e-5,  #The initial learning rate for Adam.\n",
    "    weight_decay=0.0,\n",
    "    max_grad_norm=1.0,\n",
    "    adam_epsilon=1e-8, #Epsilon for Adam optimizer.\n",
    "    \n",
    "    #Logging\n",
    "#     logging_dir='', default_logdir -> return os.path.join(\"runs\", current_time + \"_\" + socket.gethostname())\n",
    "    logging_first_step= True,\n",
    "    logging_steps = 500,\n",
    "    \n",
    "    save_steps=10_000,  #Save checkpoint every X updates steps.\n",
    "    save_total_limit=2, #\"Limit the total amount of checkpoints. Deletes the older checkpoints in the output_dir. Default is unlimited checkpoints\n",
    "    \n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "#     eval_dataset=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : This is why the GPU 1 is having all the load, and this is how it can be mitigated, and how to migrate to distributed parallel https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError('/opt/conda/lib/python3.7/site-packages/amp_C.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN6caffe28TypeMeta21_typeMetaDataInstanceISt7complexIdEEEPKNS_6detail12TypeMetaDataEv')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb787ad340a4092ba9915a30e655e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=200.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f1907b50974772bb6d0eee836e2255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=32036.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./OriginalBert_Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids([tokenizer.mask_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
